{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    Notebook used to develop regressor that uses the entire xgboost_meta_data to predict the AUC\n",
    "\n",
    "    Everything was moved to fullDataRegression.py and turned into functions\n",
    "\"\"\"\n",
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from IPython.core.display_functions import display\n",
    "# from python.project_utils import get_dataset_to_task\n",
    "#\n",
    "# avg_perf = pd.read_csv('./data/average_performance.csv')\n",
    "# meta_train = pd.read_csv('./data/features.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94, 14) (687101, 14)\n"
     ]
    }
   ],
   "source": [
    "# meta_train['MaxNominalAttDistinctValues'].hist(bins=100)\n",
    "# print(meta_train.shape, avg_perf.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# joint_and_averaged = avg_perf.merge(meta_train, how='left', on='data_id')\n",
    "# joint_and_averaged\n",
    "# joint_and_averaged.isna().sum()\n",
    "# joint_and_averaged['MaxNominalAttDistinctValues'] = joint_and_averaged['MaxNominalAttDistinctValues'].fillna(5)\n",
    "# joint_and_averaged.drop(['index', 'name', 'version', 'status'], axis=1, inplace=True)\n",
    "# joint_and_averaged['task_id'] = joint_and_averaged['data_id'].map(get_dataset_to_task())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "        data_id  num_round       eta     gamma      lambda          alpha  \\\n0             3          1  0.000097  0.002120    1.074280     156.026000   \n1             3          1  0.000200  0.014477    0.001188       0.971001   \n2             3          1  0.001281  0.000142    0.000156       0.012087   \n3             3          1  0.001911  0.021976    0.009044       0.002017   \n4             3          1  0.002022  0.000058    0.258957       1.867090   \n...         ...        ...       ...       ...         ...            ...   \n687096    41169        834  0.000271  1.687760    4.597980       9.997210   \n687097    41169        834  0.909963  1.115170   18.010900   21499.300000   \n687098    41169        863  0.998202  0.000071  260.983000     355.355000   \n687099    41169        893  0.359722  0.103639    0.007329  639500.000000   \n687100    41169       1044  0.008165  0.000038    0.001994     144.932000   \n\n        subsample  max_depth  min_child_weight  colsample_bytree  ...  \\\n0        0.202192          2          16.66880          0.442926  ...   \n1        0.186941          7           1.54714          0.644196  ...   \n2        0.367161         12           1.02729          0.223957  ...   \n3        0.876026         14           1.95115          0.355134  ...   \n4        0.354213          4           2.44719          0.239795  ...   \n...           ...        ...               ...               ...  ...   \n687096   0.113565         13           1.59306          0.023307  ...   \n687097   0.709129          9          31.55750          0.197853  ...   \n687098   0.327998          8           1.30746          0.679743  ...   \n687099   0.218695          2           4.49711          0.826498  ...   \n687100   0.139421         12          59.63500          0.034331  ...   \n\n        MaxNominalAttDistinctValues  MinorityClassSize  NumberOfClasses  \\\n0                               3.0               1527                2   \n1                               3.0               1527                2   \n2                               3.0               1527                2   \n3                               3.0               1527                2   \n4                               3.0               1527                2   \n...                             ...                ...              ...   \n687096                        100.0                111              100   \n687097                        100.0                111              100   \n687098                        100.0                111              100   \n687099                        100.0                111              100   \n687100                        100.0                111              100   \n\n        NumberOfFeatures  NumberOfInstances  \\\n0                     37               3196   \n1                     37               3196   \n2                     37               3196   \n3                     37               3196   \n4                     37               3196   \n...                  ...                ...   \n687096                28              65196   \n687097                28              65196   \n687098                28              65196   \n687099                28              65196   \n687100                28              65196   \n\n        NumberOfInstancesWithMissingValues  NumberOfMissingValues  \\\n0                                        0                      0   \n1                                        0                      0   \n2                                        0                      0   \n3                                        0                      0   \n4                                        0                      0   \n...                                    ...                    ...   \n687096                                   0                      0   \n687097                                   0                      0   \n687098                                   0                      0   \n687099                                   0                      0   \n687100                                   0                      0   \n\n        NumberOfNumericFeatures  NumberOfSymbolicFeatures  task_id  \n0                             0                        37        3  \n1                             0                        37        3  \n2                             0                        37        3  \n3                             0                        37        3  \n4                             0                        37        3  \n...                         ...                       ...      ...  \n687096                       27                         1   168329  \n687097                       27                         1   168329  \n687098                       27                         1   168329  \n687099                       27                         1   168329  \n687100                       27                         1   168329  \n\n[687101 rows x 24 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data_id</th>\n      <th>num_round</th>\n      <th>eta</th>\n      <th>gamma</th>\n      <th>lambda</th>\n      <th>alpha</th>\n      <th>subsample</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>colsample_bytree</th>\n      <th>...</th>\n      <th>MaxNominalAttDistinctValues</th>\n      <th>MinorityClassSize</th>\n      <th>NumberOfClasses</th>\n      <th>NumberOfFeatures</th>\n      <th>NumberOfInstances</th>\n      <th>NumberOfInstancesWithMissingValues</th>\n      <th>NumberOfMissingValues</th>\n      <th>NumberOfNumericFeatures</th>\n      <th>NumberOfSymbolicFeatures</th>\n      <th>task_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.000097</td>\n      <td>0.002120</td>\n      <td>1.074280</td>\n      <td>156.026000</td>\n      <td>0.202192</td>\n      <td>2</td>\n      <td>16.66880</td>\n      <td>0.442926</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1527</td>\n      <td>2</td>\n      <td>37</td>\n      <td>3196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.000200</td>\n      <td>0.014477</td>\n      <td>0.001188</td>\n      <td>0.971001</td>\n      <td>0.186941</td>\n      <td>7</td>\n      <td>1.54714</td>\n      <td>0.644196</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1527</td>\n      <td>2</td>\n      <td>37</td>\n      <td>3196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.001281</td>\n      <td>0.000142</td>\n      <td>0.000156</td>\n      <td>0.012087</td>\n      <td>0.367161</td>\n      <td>12</td>\n      <td>1.02729</td>\n      <td>0.223957</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1527</td>\n      <td>2</td>\n      <td>37</td>\n      <td>3196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.001911</td>\n      <td>0.021976</td>\n      <td>0.009044</td>\n      <td>0.002017</td>\n      <td>0.876026</td>\n      <td>14</td>\n      <td>1.95115</td>\n      <td>0.355134</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1527</td>\n      <td>2</td>\n      <td>37</td>\n      <td>3196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.002022</td>\n      <td>0.000058</td>\n      <td>0.258957</td>\n      <td>1.867090</td>\n      <td>0.354213</td>\n      <td>4</td>\n      <td>2.44719</td>\n      <td>0.239795</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1527</td>\n      <td>2</td>\n      <td>37</td>\n      <td>3196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>37</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>687096</th>\n      <td>41169</td>\n      <td>834</td>\n      <td>0.000271</td>\n      <td>1.687760</td>\n      <td>4.597980</td>\n      <td>9.997210</td>\n      <td>0.113565</td>\n      <td>13</td>\n      <td>1.59306</td>\n      <td>0.023307</td>\n      <td>...</td>\n      <td>100.0</td>\n      <td>111</td>\n      <td>100</td>\n      <td>28</td>\n      <td>65196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>1</td>\n      <td>168329</td>\n    </tr>\n    <tr>\n      <th>687097</th>\n      <td>41169</td>\n      <td>834</td>\n      <td>0.909963</td>\n      <td>1.115170</td>\n      <td>18.010900</td>\n      <td>21499.300000</td>\n      <td>0.709129</td>\n      <td>9</td>\n      <td>31.55750</td>\n      <td>0.197853</td>\n      <td>...</td>\n      <td>100.0</td>\n      <td>111</td>\n      <td>100</td>\n      <td>28</td>\n      <td>65196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>1</td>\n      <td>168329</td>\n    </tr>\n    <tr>\n      <th>687098</th>\n      <td>41169</td>\n      <td>863</td>\n      <td>0.998202</td>\n      <td>0.000071</td>\n      <td>260.983000</td>\n      <td>355.355000</td>\n      <td>0.327998</td>\n      <td>8</td>\n      <td>1.30746</td>\n      <td>0.679743</td>\n      <td>...</td>\n      <td>100.0</td>\n      <td>111</td>\n      <td>100</td>\n      <td>28</td>\n      <td>65196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>1</td>\n      <td>168329</td>\n    </tr>\n    <tr>\n      <th>687099</th>\n      <td>41169</td>\n      <td>893</td>\n      <td>0.359722</td>\n      <td>0.103639</td>\n      <td>0.007329</td>\n      <td>639500.000000</td>\n      <td>0.218695</td>\n      <td>2</td>\n      <td>4.49711</td>\n      <td>0.826498</td>\n      <td>...</td>\n      <td>100.0</td>\n      <td>111</td>\n      <td>100</td>\n      <td>28</td>\n      <td>65196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>1</td>\n      <td>168329</td>\n    </tr>\n    <tr>\n      <th>687100</th>\n      <td>41169</td>\n      <td>1044</td>\n      <td>0.008165</td>\n      <td>0.000038</td>\n      <td>0.001994</td>\n      <td>144.932000</td>\n      <td>0.139421</td>\n      <td>12</td>\n      <td>59.63500</td>\n      <td>0.034331</td>\n      <td>...</td>\n      <td>100.0</td>\n      <td>111</td>\n      <td>100</td>\n      <td>28</td>\n      <td>65196</td>\n      <td>0</td>\n      <td>0</td>\n      <td>27</td>\n      <td>1</td>\n      <td>168329</td>\n    </tr>\n  </tbody>\n</table>\n<p>687101 rows × 24 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# display(joint_and_averaged)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# joint_and_averaged.to_csv('./data/joint_and_averaged_clean.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "        Unnamed: 0  data_id  num_round       eta     gamma        lambda  \\\n502600      502600    40536        146  0.263461  0.182106  1.347390e+00   \n164602      164602      151        311  0.126755  0.000008  7.160270e+01   \n128115      128115       46         61  0.213556  1.120790  1.081520e-03   \n138975      138975       50        390  0.892973  1.968120  1.160000e-06   \n382846      382846     1487        812  0.833905  0.394070  1.878460e+00   \n...            ...      ...        ...       ...       ...           ...   \n298147      298147     1068       1711  0.170173  0.043988  9.021430e-03   \n429918      429918     4134        102  0.006323  0.000731  9.810570e+01   \n158052      158052       60        828  0.004691  0.014469  1.771000e+01   \n283444      283444     1067         72  0.009739  0.001445  1.760650e+02   \n629565      629565    41146          5  0.073945  0.070057  2.201000e+08   \n\n             alpha  subsample  max_depth  min_child_weight  ...  \\\n502600    0.006482   0.780254         12           8.09592  ...   \n164602    0.001683   0.904014         11           2.01811  ...   \n128115    0.003688   0.518079          4          21.59270  ...   \n138975    0.534515   0.716349          1          20.28740  ...   \n382846   12.635700   0.939675          2          62.06850  ...   \n...            ...        ...        ...               ...  ...   \n298147    0.000004   0.304678          6          28.07650  ...   \n429918   17.321200   0.599755         15           9.16689  ...   \n158052  120.198000   0.793861          9           4.06040  ...   \n283444    0.596482   0.496458          9          50.31140  ...   \n629565    0.010069   0.792287          5           3.36013  ...   \n\n        MaxNominalAttDistinctValues  MinorityClassSize  NumberOfClasses  \\\n502600                          5.0               1380                2   \n164602                          7.0              19237                2   \n128115                          6.0                767                3   \n138975                          3.0                332                2   \n382846                          2.0                160                2   \n...                             ...                ...              ...   \n298147                          2.0                 77                2   \n429918                          2.0               1717                2   \n158052                          3.0               1653                3   \n283444                          2.0                326                2   \n629565                          2.0               2562                2   \n\n        NumberOfFeatures  NumberOfInstances  \\\n502600               121               8378   \n164602                 9              45312   \n128115                61               3190   \n138975                10                958   \n382846                73               2534   \n...                  ...                ...   \n298147                22               1109   \n429918              1777               3751   \n158052                41               5000   \n283444                22               2109   \n629565                21               5124   \n\n        NumberOfInstancesWithMissingValues  NumberOfMissingValues  \\\n502600                                7330                  18372   \n164602                                   0                      0   \n128115                                   0                      0   \n138975                                   0                      0   \n382846                                   0                      0   \n...                                    ...                    ...   \n298147                                   0                      0   \n429918                                   0                      0   \n158052                                   0                      0   \n283444                                   0                      0   \n629565                                   0                      0   \n\n        NumberOfNumericFeatures  NumberOfSymbolicFeatures  task_id  \n502600                       59                        62   146607  \n164602                        7                         2      219  \n128115                        0                        61       45  \n138975                        0                        10       49  \n382846                       72                         1     9978  \n...                         ...                       ...      ...  \n298147                       21                         1     3918  \n429918                     1776                         1     9910  \n158052                       40                         1       58  \n283444                       21                         1     3917  \n629565                       20                         1   168761  \n\n[103065 rows x 25 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>data_id</th>\n      <th>num_round</th>\n      <th>eta</th>\n      <th>gamma</th>\n      <th>lambda</th>\n      <th>alpha</th>\n      <th>subsample</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>...</th>\n      <th>MaxNominalAttDistinctValues</th>\n      <th>MinorityClassSize</th>\n      <th>NumberOfClasses</th>\n      <th>NumberOfFeatures</th>\n      <th>NumberOfInstances</th>\n      <th>NumberOfInstancesWithMissingValues</th>\n      <th>NumberOfMissingValues</th>\n      <th>NumberOfNumericFeatures</th>\n      <th>NumberOfSymbolicFeatures</th>\n      <th>task_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>502600</th>\n      <td>502600</td>\n      <td>40536</td>\n      <td>146</td>\n      <td>0.263461</td>\n      <td>0.182106</td>\n      <td>1.347390e+00</td>\n      <td>0.006482</td>\n      <td>0.780254</td>\n      <td>12</td>\n      <td>8.09592</td>\n      <td>...</td>\n      <td>5.0</td>\n      <td>1380</td>\n      <td>2</td>\n      <td>121</td>\n      <td>8378</td>\n      <td>7330</td>\n      <td>18372</td>\n      <td>59</td>\n      <td>62</td>\n      <td>146607</td>\n    </tr>\n    <tr>\n      <th>164602</th>\n      <td>164602</td>\n      <td>151</td>\n      <td>311</td>\n      <td>0.126755</td>\n      <td>0.000008</td>\n      <td>7.160270e+01</td>\n      <td>0.001683</td>\n      <td>0.904014</td>\n      <td>11</td>\n      <td>2.01811</td>\n      <td>...</td>\n      <td>7.0</td>\n      <td>19237</td>\n      <td>2</td>\n      <td>9</td>\n      <td>45312</td>\n      <td>0</td>\n      <td>0</td>\n      <td>7</td>\n      <td>2</td>\n      <td>219</td>\n    </tr>\n    <tr>\n      <th>128115</th>\n      <td>128115</td>\n      <td>46</td>\n      <td>61</td>\n      <td>0.213556</td>\n      <td>1.120790</td>\n      <td>1.081520e-03</td>\n      <td>0.003688</td>\n      <td>0.518079</td>\n      <td>4</td>\n      <td>21.59270</td>\n      <td>...</td>\n      <td>6.0</td>\n      <td>767</td>\n      <td>3</td>\n      <td>61</td>\n      <td>3190</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>61</td>\n      <td>45</td>\n    </tr>\n    <tr>\n      <th>138975</th>\n      <td>138975</td>\n      <td>50</td>\n      <td>390</td>\n      <td>0.892973</td>\n      <td>1.968120</td>\n      <td>1.160000e-06</td>\n      <td>0.534515</td>\n      <td>0.716349</td>\n      <td>1</td>\n      <td>20.28740</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>332</td>\n      <td>2</td>\n      <td>10</td>\n      <td>958</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>10</td>\n      <td>49</td>\n    </tr>\n    <tr>\n      <th>382846</th>\n      <td>382846</td>\n      <td>1487</td>\n      <td>812</td>\n      <td>0.833905</td>\n      <td>0.394070</td>\n      <td>1.878460e+00</td>\n      <td>12.635700</td>\n      <td>0.939675</td>\n      <td>2</td>\n      <td>62.06850</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>160</td>\n      <td>2</td>\n      <td>73</td>\n      <td>2534</td>\n      <td>0</td>\n      <td>0</td>\n      <td>72</td>\n      <td>1</td>\n      <td>9978</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>298147</th>\n      <td>298147</td>\n      <td>1068</td>\n      <td>1711</td>\n      <td>0.170173</td>\n      <td>0.043988</td>\n      <td>9.021430e-03</td>\n      <td>0.000004</td>\n      <td>0.304678</td>\n      <td>6</td>\n      <td>28.07650</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>77</td>\n      <td>2</td>\n      <td>22</td>\n      <td>1109</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21</td>\n      <td>1</td>\n      <td>3918</td>\n    </tr>\n    <tr>\n      <th>429918</th>\n      <td>429918</td>\n      <td>4134</td>\n      <td>102</td>\n      <td>0.006323</td>\n      <td>0.000731</td>\n      <td>9.810570e+01</td>\n      <td>17.321200</td>\n      <td>0.599755</td>\n      <td>15</td>\n      <td>9.16689</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>1717</td>\n      <td>2</td>\n      <td>1777</td>\n      <td>3751</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1776</td>\n      <td>1</td>\n      <td>9910</td>\n    </tr>\n    <tr>\n      <th>158052</th>\n      <td>158052</td>\n      <td>60</td>\n      <td>828</td>\n      <td>0.004691</td>\n      <td>0.014469</td>\n      <td>1.771000e+01</td>\n      <td>120.198000</td>\n      <td>0.793861</td>\n      <td>9</td>\n      <td>4.06040</td>\n      <td>...</td>\n      <td>3.0</td>\n      <td>1653</td>\n      <td>3</td>\n      <td>41</td>\n      <td>5000</td>\n      <td>0</td>\n      <td>0</td>\n      <td>40</td>\n      <td>1</td>\n      <td>58</td>\n    </tr>\n    <tr>\n      <th>283444</th>\n      <td>283444</td>\n      <td>1067</td>\n      <td>72</td>\n      <td>0.009739</td>\n      <td>0.001445</td>\n      <td>1.760650e+02</td>\n      <td>0.596482</td>\n      <td>0.496458</td>\n      <td>9</td>\n      <td>50.31140</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>326</td>\n      <td>2</td>\n      <td>22</td>\n      <td>2109</td>\n      <td>0</td>\n      <td>0</td>\n      <td>21</td>\n      <td>1</td>\n      <td>3917</td>\n    </tr>\n    <tr>\n      <th>629565</th>\n      <td>629565</td>\n      <td>41146</td>\n      <td>5</td>\n      <td>0.073945</td>\n      <td>0.070057</td>\n      <td>2.201000e+08</td>\n      <td>0.010069</td>\n      <td>0.792287</td>\n      <td>5</td>\n      <td>3.36013</td>\n      <td>...</td>\n      <td>2.0</td>\n      <td>2562</td>\n      <td>2</td>\n      <td>21</td>\n      <td>5124</td>\n      <td>0</td>\n      <td>0</td>\n      <td>20</td>\n      <td>1</td>\n      <td>168761</td>\n    </tr>\n  </tbody>\n</table>\n<p>103065 rows × 25 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from xgboost import XGBRegressor\n",
    "from skopt import BayesSearchCV\n",
    "from sklearn.model_selection import RepeatedKFold\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "from skopt.space import Real, Integer\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from IPython.core.display_functions import display\n",
    "from python.project_utils import get_dataset_to_task, training_meta_features, hyperparameters_data, test_ids\n",
    "from datetime import datetime\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "full_data = pandas.read_csv('./data/joint_and_averaged_clean.csv')\n",
    "\"\"\"Data will be structured as\n",
    "X_train: dataset_metafeatures, XGBoost hyperparameters\n",
    "y_train: avg_auc\n",
    "\"\"\"\n",
    "sampled_data = full_data.sample(frac=0.15)\n",
    "display(sampled_data)\n",
    "\n",
    "def bayes_callback(res):\n",
    "    print(\"Next iteration, Time:\",datetime.now().strftime(\"%H:%M:%S\"))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting outer evaluation  1 for Bayesian optimisation \n",
      " Time: 14:08:13\n",
      "Next iteration, Time: 14:09:13\n",
      "Next iteration, Time: 14:10:08\n",
      "Next iteration, Time: 14:10:38\n",
      "Next iteration, Time: 14:10:44\n",
      "Next iteration, Time: 14:11:17\n",
      "Next iteration, Time: 14:11:45\n",
      "Next iteration, Time: 14:12:08\n",
      "Next iteration, Time: 14:14:49\n",
      "Next iteration, Time: 14:15:42\n",
      "Next iteration, Time: 14:20:05\n",
      "Next iteration, Time: 14:25:00\n",
      "Next iteration, Time: 14:28:20\n",
      "Next iteration, Time: 14:28:56\n",
      "Next iteration, Time: 14:32:10\n",
      "Next iteration, Time: 14:32:24\n",
      "Next iteration, Time: 14:39:00\n",
      "Next iteration, Time: 14:39:16\n",
      "Next iteration, Time: 14:41:29\n",
      "Next iteration, Time: 14:46:35\n",
      "Next iteration, Time: 14:48:00\n",
      "Best score in this iteration: 0.9297696655137203\n",
      "Best regression parameters in this iteration: OrderedDict([('colsample_bytree', 0.9), ('eta', 0.10088669342732684), ('learning_rate', 0.031810056486861714), ('max_depth', 15), ('n_estimators', 500), ('subsample', 1.0)])\n",
      "Starting outer evaluation  2 for Bayesian optimisation \n",
      " Time: 14:50:14\n",
      "Next iteration, Time: 14:52:32\n",
      "Next iteration, Time: 14:53:43\n",
      "Next iteration, Time: 14:54:01\n",
      "Next iteration, Time: 14:54:29\n",
      "Next iteration, Time: 14:55:18\n",
      "Next iteration, Time: 14:55:34\n",
      "Next iteration, Time: 14:55:45\n",
      "Next iteration, Time: 14:56:01\n",
      "Next iteration, Time: 14:56:34\n",
      "Next iteration, Time: 14:57:03\n",
      "Next iteration, Time: 15:01:12\n",
      "Next iteration, Time: 15:06:50\n",
      "Next iteration, Time: 15:07:04\n",
      "Next iteration, Time: 15:07:10\n",
      "Next iteration, Time: 15:09:32\n",
      "Next iteration, Time: 15:11:13\n",
      "Next iteration, Time: 15:12:02\n",
      "Next iteration, Time: 15:18:38\n",
      "Next iteration, Time: 15:22:07\n",
      "Next iteration, Time: 15:24:23\n",
      "Best score in this iteration: 0.9333228705877413\n",
      "Best regression parameters in this iteration: OrderedDict([('colsample_bytree', 0.9), ('eta', 0.0), ('learning_rate', 0.017764780206487096), ('max_depth', 15), ('n_estimators', 500), ('subsample', 0.7538036564590659)])\n",
      "Starting outer evaluation  3 for Bayesian optimisation \n",
      " Time: 15:27:05\n",
      "Next iteration, Time: 15:27:38\n",
      "Next iteration, Time: 15:29:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py:702: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next iteration, Time: 15:30:10\n",
      "Next iteration, Time: 15:30:22\n",
      "Next iteration, Time: 15:33:35\n",
      "Next iteration, Time: 15:34:02\n",
      "Next iteration, Time: 15:36:05\n",
      "Next iteration, Time: 15:36:29\n",
      "Next iteration, Time: 15:39:30\n",
      "Next iteration, Time: 15:40:36\n",
      "Next iteration, Time: 15:41:14\n",
      "Next iteration, Time: 15:47:24\n",
      "Next iteration, Time: 15:48:57\n",
      "Next iteration, Time: 15:50:36\n",
      "Next iteration, Time: 15:57:17\n",
      "Next iteration, Time: 16:00:58\n",
      "Next iteration, Time: 16:01:45\n",
      "Next iteration, Time: 16:07:44\n",
      "Next iteration, Time: 16:09:43\n",
      "Next iteration, Time: 16:11:47\n",
      "Best score in this iteration: 0.9294472488905287\n",
      "Best regression parameters in this iteration: OrderedDict([('colsample_bytree', 0.9), ('eta', 1.0), ('learning_rate', 0.011757634493083981), ('max_depth', 15), ('n_estimators', 500), ('subsample', 1.0)])\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "NOTE - Order of features - first dataset features, then hyperparameters\n",
    "\"\"\"\n",
    "\n",
    "#Defining search space for XGBoost regressor\n",
    "search_space = {}\n",
    "search_space['eta'] = Real(0,1,'uniform')\n",
    "search_space['n_estimators'] = Integer(50,500)\n",
    "search_space['max_depth'] = Integer(3,15)\n",
    "search_space['learning_rate'] = Real(10e-4,0.1,'log-uniform')\n",
    "search_space['colsample_bytree'] = Real(0.3,0.9,'uniform')\n",
    "search_space['subsample'] = Real(0.3,1,'uniform')\n",
    "\n",
    "#Selecting relevant features for regressor\n",
    "training_features = training_meta_features + hyperparameters_data\n",
    "X = sampled_data[training_features].to_numpy()\n",
    "y = sampled_data['avg_auc'].to_numpy()\n",
    "\n",
    "#We split the data 2:1 to get 10% of the entire dataset in each training\n",
    "cv_outer = RepeatedKFold(n_splits=3, n_repeats=1, random_state=1)\n",
    "#Lists for nested sampling results\n",
    "outer_score = []\n",
    "regressors = []\n",
    "search_results = []\n",
    "\n",
    "for counter, (train_index,test_index) in enumerate(cv_outer.split(X,y)):\n",
    "    print(\"Starting outer evaluation \",counter+1,\"for Bayesian optimisation \\n Time:\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "\n",
    "    #selecting relevant data for fold\n",
    "    X_train, X_test = X[train_index,:], X[test_index,:]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "    \"\"\"\n",
    "    Inner loop uses 5-fold cross validation to evaluate regressor configurations\n",
    "    Metric doesn't need to be specified - inherited by XGBoost (standard R2 score)\n",
    "    \"\"\"\n",
    "    cv = RepeatedKFold(n_splits=5, n_repeats=1, random_state=1)\n",
    "    search = BayesSearchCV(estimator= XGBRegressor(), search_spaces=search_space, n_jobs=-1,\\\n",
    "                           cv=cv, n_iter=20) # n_iter=150\n",
    "    # perform the search and save results\n",
    "    curr_result = search.fit(X_train, y_train, callback = bayes_callback)\n",
    "\n",
    "    search_results.append(curr_result)\n",
    "    best_candidate = curr_result.best_estimator_\n",
    "    regressors.append(best_candidate)\n",
    "    # report the best result\n",
    "    print(\"Best score in this iteration:\",search.best_score_)\n",
    "    print(\"Best regression parameters in this iteration:\",search.best_params_)\n",
    "\n",
    "    y_pred = best_candidate.predict(X_test)\n",
    "    score = r2_score(y_test, y_pred)\n",
    "    outer_score.append(score)\n",
    "\n",
    "    #Save results\n",
    "    with open('fulldata_xgb_search_results.pkl', 'wb') as f:\n",
    "        pickle.dump(search_results, f)\n",
    "    with open('full_data_xgboost_regressors.pkl', 'wb') as f:\n",
    "        pickle.dump(regressors, f)\n",
    "    with open('full_data_xgboost_scores.pkl', 'wb') as f:\n",
    "        pickle.dump(outer_score, f)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.936097452681282, 0.9387437709312048, 0.9353902074991705]\n",
      "[XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.9, enable_categorical=False,\n",
      "             eta=0.10088669342732684, gamma=0, gpu_id=-1, importance_type=None,\n",
      "             interaction_constraints='', learning_rate=0.031810056486861714,\n",
      "             max_delta_step=0, max_depth=15, min_child_weight=1, missing=nan,\n",
      "             monotone_constraints='()', n_estimators=500, n_jobs=4,\n",
      "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
      "             reg_lambda=1, scale_pos_weight=1, subsample=1.0,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.9, enable_categorical=False,\n",
      "             eta=0.0, gamma=0, gpu_id=-1, importance_type=None,\n",
      "             interaction_constraints='', learning_rate=0.017764780206487096,\n",
      "             max_delta_step=0, max_depth=15, min_child_weight=1, missing=nan,\n",
      "             monotone_constraints='()', n_estimators=500, n_jobs=4,\n",
      "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
      "             reg_lambda=1, scale_pos_weight=1, subsample=0.7538036564590659,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None), XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.9, enable_categorical=False,\n",
      "             eta=1.0, gamma=0, gpu_id=-1, importance_type=None,\n",
      "             interaction_constraints='', learning_rate=0.011757634493083981,\n",
      "             max_delta_step=0, max_depth=15, min_child_weight=1, missing=nan,\n",
      "             monotone_constraints='()', n_estimators=500, n_jobs=4,\n",
      "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
      "             reg_lambda=1, scale_pos_weight=1, subsample=1.0,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)]\n",
      "[BayesSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=5, random_state=1),\n",
      "              estimator=XGBRegressor(base_score=None, booster=None,\n",
      "                                     colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     enable_categorical=False, gamma=None,\n",
      "                                     gpu_id=None, importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_delta_step=None,\n",
      "                                     max_depth=None, min_child...\n",
      "                             'eta': Real(low=0, high=1, prior='uniform', transform='normalize'),\n",
      "                             'learning_rate': Real(low=0.001, high=0.1, prior='log-uniform', transform='normalize'),\n",
      "                             'max_depth': Integer(low=3, high=15, prior='uniform', transform='normalize'),\n",
      "                             'n_estimators': Integer(low=50, high=500, prior='uniform', transform='normalize'),\n",
      "                             'subsample': Real(low=0.3, high=1, prior='uniform', transform='normalize')}), BayesSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=5, random_state=1),\n",
      "              estimator=XGBRegressor(base_score=None, booster=None,\n",
      "                                     colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     enable_categorical=False, gamma=None,\n",
      "                                     gpu_id=None, importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_delta_step=None,\n",
      "                                     max_depth=None, min_child...\n",
      "                             'eta': Real(low=0, high=1, prior='uniform', transform='normalize'),\n",
      "                             'learning_rate': Real(low=0.001, high=0.1, prior='log-uniform', transform='normalize'),\n",
      "                             'max_depth': Integer(low=3, high=15, prior='uniform', transform='normalize'),\n",
      "                             'n_estimators': Integer(low=50, high=500, prior='uniform', transform='normalize'),\n",
      "                             'subsample': Real(low=0.3, high=1, prior='uniform', transform='normalize')}), BayesSearchCV(cv=RepeatedKFold(n_repeats=1, n_splits=5, random_state=1),\n",
      "              estimator=XGBRegressor(base_score=None, booster=None,\n",
      "                                     colsample_bylevel=None,\n",
      "                                     colsample_bynode=None,\n",
      "                                     colsample_bytree=None,\n",
      "                                     enable_categorical=False, gamma=None,\n",
      "                                     gpu_id=None, importance_type=None,\n",
      "                                     interaction_constraints=None,\n",
      "                                     learning_rate=None, max_delta_step=None,\n",
      "                                     max_depth=None, min_child...\n",
      "                             'eta': Real(low=0, high=1, prior='uniform', transform='normalize'),\n",
      "                             'learning_rate': Real(low=0.001, high=0.1, prior='log-uniform', transform='normalize'),\n",
      "                             'max_depth': Integer(low=3, high=15, prior='uniform', transform='normalize'),\n",
      "                             'n_estimators': Integer(low=50, high=500, prior='uniform', transform='normalize'),\n",
      "                             'subsample': Real(low=0.3, high=1, prior='uniform', transform='normalize')})]\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "with open('full_data_xgboost_scores.pkl', 'rb') as f:\n",
    "    scores = pickle.load(f)\n",
    "with open('full_data_xgboost_regressors.pkl', 'rb') as f:\n",
    "    regressors = pickle.load(f)\n",
    "#with open('fulldata_xgb_search_results.pkl', 'rb') as f:\n",
    "#    searches = pickle.load(f)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "index_max = np.argmax(scores)\n",
    "best_score = scores[index_max]\n",
    "best_regressor = regressors[index_max]\n",
    "#best_search = searches[index_max]\n",
    "#results_dict = best_search.cv_results_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.9, enable_categorical=False,\n",
      "             eta=0.0, gamma=0, gpu_id=-1, importance_type=None,\n",
      "             interaction_constraints='', learning_rate=0.017764780206487096,\n",
      "             max_delta_step=0, max_depth=15, min_child_weight=1, missing=nan,\n",
      "             monotone_constraints='()', n_estimators=500, n_jobs=4,\n",
      "             num_parallel_tree=1, predictor='auto', random_state=0, reg_alpha=0,\n",
      "             reg_lambda=1, scale_pos_weight=1, subsample=0.7538036564590659,\n",
      "             tree_method='exact', validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "print(best_regressor)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                                      name  status  MajorityClassSize  \\\n0                           mfeat-karhunen  active              200.0   \n1                            mfeat-zernike  active              200.0   \n2                                 credit-g  active              700.0   \n3                                 satimage  active             1531.0   \n4                               eucalyptus  active              214.0   \n5                         monks-problems-2  active              395.0   \n6                                      mc1  active             9398.0   \n7                                      kc2  active              415.0   \n8                               micro-mass  active               60.0   \n9                                  phoneme  active             3818.0   \n10                                    ilpd  active              416.0   \n11                        CreditCardSubset  active            14217.0   \n12                          cylinder-bands  active              312.0   \n13                                     har  active             1944.0   \n14                                 shuttle  active            45586.0   \n15  jungle_chess_2pcs_raw_endgame_complete  active            23062.0   \n16                 Internet-Advertisements  active             2820.0   \n17                                    kick  active            64007.0   \n\n    MaxNominalAttDistinctValues  MinorityClassSize  NumberOfClasses  \\\n0                          10.0              200.0             10.0   \n1                          10.0              200.0             10.0   \n2                          10.0              300.0              2.0   \n3                           6.0              625.0              6.0   \n4                          27.0              105.0              5.0   \n5                           4.0              206.0              2.0   \n6                           2.0               68.0              2.0   \n7                           2.0              107.0              2.0   \n8                          20.0               11.0             20.0   \n9                           2.0             1586.0              2.0   \n10                          2.0              167.0              2.0   \n11                          2.0               23.0              2.0   \n12                         71.0              228.0              2.0   \n13                          6.0             1406.0              6.0   \n14                          7.0               10.0              7.0   \n15                          3.0             4335.0              3.0   \n16                          2.0              459.0              2.0   \n17                          NaN             8976.0              2.0   \n\n    NumberOfFeatures  NumberOfInstances  NumberOfInstancesWithMissingValues  \\\n0               65.0             2000.0                                 0.0   \n1               48.0             2000.0                                 0.0   \n2               21.0             1000.0                                 0.0   \n3               37.0             6430.0                                 0.0   \n4               20.0              736.0                                95.0   \n5                7.0              601.0                                 0.0   \n6               39.0             9466.0                                 0.0   \n7               22.0              522.0                                 0.0   \n8             1301.0              571.0                                 0.0   \n9                6.0             5404.0                                 0.0   \n10              11.0              583.0                                 0.0   \n11              31.0            14240.0                                 0.0   \n12              40.0              540.0                               263.0   \n13             562.0            10299.0                                 0.0   \n14              10.0            58000.0                                 0.0   \n15               7.0            44819.0                                 0.0   \n16            1559.0             3279.0                                 0.0   \n17              33.0            72983.0                             69709.0   \n\n    NumberOfMissingValues  NumberOfNumericFeatures  NumberOfSymbolicFeatures  \\\n0                     0.0                     64.0                       1.0   \n1                     0.0                     47.0                       1.0   \n2                     0.0                      7.0                      14.0   \n3                     0.0                     36.0                       1.0   \n4                   448.0                     14.0                       6.0   \n5                     0.0                      0.0                       7.0   \n6                     0.0                     38.0                       1.0   \n7                     0.0                     21.0                       1.0   \n8                     0.0                   1300.0                       1.0   \n9                     0.0                      5.0                       1.0   \n10                    0.0                      9.0                       2.0   \n11                    0.0                     30.0                       1.0   \n12                  999.0                     18.0                      22.0   \n13                    0.0                    561.0                       1.0   \n14                    0.0                      9.0                       1.0   \n15                    0.0                      6.0                       1.0   \n16                    0.0                      3.0                    1556.0   \n17               149271.0                     14.0                      19.0   \n\n    data_id   task_id  \n0      16.0      16.0  \n1      22.0      22.0  \n2      31.0      31.0  \n3     182.0    2074.0  \n4     188.0    2079.0  \n5     334.0    3493.0  \n6    1056.0    3907.0  \n7    1063.0    3913.0  \n8    1515.0    9950.0  \n9    1489.0    9952.0  \n10   1480.0    9971.0  \n11   4154.0   10106.0  \n12   6332.0   14954.0  \n13   1478.0   14970.0  \n14  40685.0  146212.0  \n15  41027.0  167119.0  \n16  40978.0  167125.0  \n17  41162.0  168336.0  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>status</th>\n      <th>MajorityClassSize</th>\n      <th>MaxNominalAttDistinctValues</th>\n      <th>MinorityClassSize</th>\n      <th>NumberOfClasses</th>\n      <th>NumberOfFeatures</th>\n      <th>NumberOfInstances</th>\n      <th>NumberOfInstancesWithMissingValues</th>\n      <th>NumberOfMissingValues</th>\n      <th>NumberOfNumericFeatures</th>\n      <th>NumberOfSymbolicFeatures</th>\n      <th>data_id</th>\n      <th>task_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>mfeat-karhunen</td>\n      <td>active</td>\n      <td>200.0</td>\n      <td>10.0</td>\n      <td>200.0</td>\n      <td>10.0</td>\n      <td>65.0</td>\n      <td>2000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>64.0</td>\n      <td>1.0</td>\n      <td>16.0</td>\n      <td>16.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>mfeat-zernike</td>\n      <td>active</td>\n      <td>200.0</td>\n      <td>10.0</td>\n      <td>200.0</td>\n      <td>10.0</td>\n      <td>48.0</td>\n      <td>2000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>47.0</td>\n      <td>1.0</td>\n      <td>22.0</td>\n      <td>22.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>credit-g</td>\n      <td>active</td>\n      <td>700.0</td>\n      <td>10.0</td>\n      <td>300.0</td>\n      <td>2.0</td>\n      <td>21.0</td>\n      <td>1000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>14.0</td>\n      <td>31.0</td>\n      <td>31.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>satimage</td>\n      <td>active</td>\n      <td>1531.0</td>\n      <td>6.0</td>\n      <td>625.0</td>\n      <td>6.0</td>\n      <td>37.0</td>\n      <td>6430.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>36.0</td>\n      <td>1.0</td>\n      <td>182.0</td>\n      <td>2074.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>eucalyptus</td>\n      <td>active</td>\n      <td>214.0</td>\n      <td>27.0</td>\n      <td>105.0</td>\n      <td>5.0</td>\n      <td>20.0</td>\n      <td>736.0</td>\n      <td>95.0</td>\n      <td>448.0</td>\n      <td>14.0</td>\n      <td>6.0</td>\n      <td>188.0</td>\n      <td>2079.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>monks-problems-2</td>\n      <td>active</td>\n      <td>395.0</td>\n      <td>4.0</td>\n      <td>206.0</td>\n      <td>2.0</td>\n      <td>7.0</td>\n      <td>601.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>7.0</td>\n      <td>334.0</td>\n      <td>3493.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>mc1</td>\n      <td>active</td>\n      <td>9398.0</td>\n      <td>2.0</td>\n      <td>68.0</td>\n      <td>2.0</td>\n      <td>39.0</td>\n      <td>9466.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>38.0</td>\n      <td>1.0</td>\n      <td>1056.0</td>\n      <td>3907.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>kc2</td>\n      <td>active</td>\n      <td>415.0</td>\n      <td>2.0</td>\n      <td>107.0</td>\n      <td>2.0</td>\n      <td>22.0</td>\n      <td>522.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>21.0</td>\n      <td>1.0</td>\n      <td>1063.0</td>\n      <td>3913.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>micro-mass</td>\n      <td>active</td>\n      <td>60.0</td>\n      <td>20.0</td>\n      <td>11.0</td>\n      <td>20.0</td>\n      <td>1301.0</td>\n      <td>571.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>1300.0</td>\n      <td>1.0</td>\n      <td>1515.0</td>\n      <td>9950.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>phoneme</td>\n      <td>active</td>\n      <td>3818.0</td>\n      <td>2.0</td>\n      <td>1586.0</td>\n      <td>2.0</td>\n      <td>6.0</td>\n      <td>5404.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>5.0</td>\n      <td>1.0</td>\n      <td>1489.0</td>\n      <td>9952.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>ilpd</td>\n      <td>active</td>\n      <td>416.0</td>\n      <td>2.0</td>\n      <td>167.0</td>\n      <td>2.0</td>\n      <td>11.0</td>\n      <td>583.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>2.0</td>\n      <td>1480.0</td>\n      <td>9971.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>CreditCardSubset</td>\n      <td>active</td>\n      <td>14217.0</td>\n      <td>2.0</td>\n      <td>23.0</td>\n      <td>2.0</td>\n      <td>31.0</td>\n      <td>14240.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>30.0</td>\n      <td>1.0</td>\n      <td>4154.0</td>\n      <td>10106.0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>cylinder-bands</td>\n      <td>active</td>\n      <td>312.0</td>\n      <td>71.0</td>\n      <td>228.0</td>\n      <td>2.0</td>\n      <td>40.0</td>\n      <td>540.0</td>\n      <td>263.0</td>\n      <td>999.0</td>\n      <td>18.0</td>\n      <td>22.0</td>\n      <td>6332.0</td>\n      <td>14954.0</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>har</td>\n      <td>active</td>\n      <td>1944.0</td>\n      <td>6.0</td>\n      <td>1406.0</td>\n      <td>6.0</td>\n      <td>562.0</td>\n      <td>10299.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>561.0</td>\n      <td>1.0</td>\n      <td>1478.0</td>\n      <td>14970.0</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>shuttle</td>\n      <td>active</td>\n      <td>45586.0</td>\n      <td>7.0</td>\n      <td>10.0</td>\n      <td>7.0</td>\n      <td>10.0</td>\n      <td>58000.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>9.0</td>\n      <td>1.0</td>\n      <td>40685.0</td>\n      <td>146212.0</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>jungle_chess_2pcs_raw_endgame_complete</td>\n      <td>active</td>\n      <td>23062.0</td>\n      <td>3.0</td>\n      <td>4335.0</td>\n      <td>3.0</td>\n      <td>7.0</td>\n      <td>44819.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>6.0</td>\n      <td>1.0</td>\n      <td>41027.0</td>\n      <td>167119.0</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>Internet-Advertisements</td>\n      <td>active</td>\n      <td>2820.0</td>\n      <td>2.0</td>\n      <td>459.0</td>\n      <td>2.0</td>\n      <td>1559.0</td>\n      <td>3279.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>3.0</td>\n      <td>1556.0</td>\n      <td>40978.0</td>\n      <td>167125.0</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>kick</td>\n      <td>active</td>\n      <td>64007.0</td>\n      <td>NaN</td>\n      <td>8976.0</td>\n      <td>2.0</td>\n      <td>33.0</td>\n      <td>72983.0</td>\n      <td>69709.0</td>\n      <td>149271.0</td>\n      <td>14.0</td>\n      <td>19.0</td>\n      <td>41162.0</td>\n      <td>168336.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_features = pandas.read_csv('./data/test_features.csv')\n",
    "display(test_features)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random configurations\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([[2.01700000e+03, 6.48260293e-01, 2.87181733e+00, ...,\n        5.45600079e+01, 8.26753370e-01, 9.05286405e-01],\n       [2.70000000e+01, 5.64719654e-01, 2.20165311e+00, ...,\n        4.92511797e+01, 9.88846795e-01, 9.61373654e-01],\n       [2.60000000e+01, 6.79016689e-01, 2.35399848e+00, ...,\n        2.05648939e+01, 4.18157504e-01, 8.68885101e-01],\n       ...,\n       [3.60000000e+01, 6.94121098e-01, 2.36656301e-01, ...,\n        5.21018298e+01, 1.97559061e-01, 4.22806723e-01],\n       [2.40000000e+01, 4.48900958e-01, 1.55533893e+00, ...,\n        2.66084441e+01, 7.96993159e-01, 1.52354894e-01],\n       [1.92600000e+03, 8.36793962e-02, 3.28433033e+00, ...,\n        2.66272690e+01, 7.83051902e-01, 2.73339816e-01]])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from python.taskRegression import sample_points\n",
    "\n",
    "\"\"\"\n",
    "For each of the test tasks we will predict the best configuration by sampling random configurations and observing the expected AUC\n",
    "\"\"\"\n",
    "print(\"Generating random configurations\")\n",
    "sample_configs = sample_points(n_points=200000)[hyperparameters_data].to_numpy()\n",
    "display(sample_configs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selecting the best configuration for each task:\n",
      "Task: 16 time 18:47:26\n",
      "Task: 22 time 18:47:30\n",
      "Task: 31 time 18:47:34\n",
      "Task: 2074 time 18:47:37\n",
      "Task: 2079 time 18:47:40\n",
      "Task: 3493 time 18:47:44\n",
      "Task: 3907 time 18:47:47\n",
      "Task: 3913 time 18:47:51\n",
      "Task: 9950 time 18:47:54\n",
      "Task: 9952 time 18:47:57\n",
      "Task: 9971 time 18:48:01\n",
      "Task: 10106 time 18:48:04\n",
      "Task: 14954 time 18:48:07\n",
      "Task: 14970 time 18:48:12\n",
      "Task: 146212 time 18:48:15\n",
      "Task: 167119 time 18:48:19\n",
      "Task: 167125 time 18:48:22\n",
      "Task: 168336 time 18:48:25\n",
      "Evaluating task with id:  16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:48:29] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.9945\n",
      "Evaluating task with id:  22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:48:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.9688333333333332\n",
      "Evaluating task with id:  31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:49:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.8099999999999999\n",
      "Evaluating task with id:  2074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:49:11] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.9897344902722427\n",
      "Evaluating task with id:  2079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:49:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.8879966941674334\n",
      "Evaluating task with id:  3493\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:49:54] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.5\n",
      "Evaluating task with id:  3907\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:49:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.901595744680851\n",
      "Evaluating task with id:  3913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:49:55] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.8982683982683982\n",
      "Evaluating task with id:  9950\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:49:58] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.9918304697252067\n",
      "Evaluating task with id:  9952\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.9538015739734599\n",
      "Evaluating task with id:  9971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:04] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.696078431372549\n",
      "Evaluating task with id:  10106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.99929676511955\n",
      "Evaluating task with id:  14954\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.8849929873772792\n",
      "Evaluating task with id:  14970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18:50:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.9998984304064713\n",
      "Evaluating task with id:  146212\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:09:40] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.9999896514485928\n",
      "Evaluating task with id:  167119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:10:17] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.9763923587704704\n",
      "Evaluating task with id:  167125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:11:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.9657338883749614\n",
      "Evaluating task with id:  168336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19:13:52] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.1/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "New auc 0.7526029479664404\n"
     ]
    },
    {
     "data": {
      "text/plain": "    task_id  base_auc   base_time   new_auc     new_time  delta_auc  \\\n0        16  0.997222   31.225133  0.994500    14.023385  -0.002722   \n1        22  0.973361   25.774230  0.968833    24.805384  -0.004528   \n2        31  0.841905    1.274992  0.810000     2.276000  -0.031905   \n3      2074  0.989416   18.240999  0.989734    40.257960   0.000319   \n4      2079  0.915436    3.369999  0.887997     1.475038  -0.027439   \n5      3493  0.976190    0.539915  0.500000     0.354961  -0.476190   \n6      3907  0.985866    2.770957  0.901596     0.324998  -0.084271   \n7      3913  0.906926    0.559001  0.898268     1.623962  -0.008658   \n8      9950  0.996221   32.786961  0.991830     2.706999  -0.004390   \n9      9952  0.956024    2.991995  0.953802     3.121042  -0.002223   \n10     9971  0.668067    0.624963  0.696078     0.099001   0.028011   \n11    10106  0.999297    9.776999  0.999297    11.642998   0.000000   \n12    14954  0.910238    0.831042  0.884993     0.733303  -0.025245   \n13    14970  0.999844  748.155138  0.999898  1160.218471   0.000055   \n14   146212  0.999998   30.165969  0.999990    36.428032  -0.000009   \n15   167119  0.969022   34.102057  0.976392    69.063016   0.007370   \n16   167125  0.962805   64.667999  0.965734   140.439371   0.002929   \n17   168336  0.735831  170.242889  0.752603   219.147188   0.016772   \n\n    better_auc  better_time  \n0        False         True  \n1        False         True  \n2        False        False  \n3         True        False  \n4        False         True  \n5        False         True  \n6        False         True  \n7        False        False  \n8        False         True  \n9        False        False  \n10        True         True  \n11       False        False  \n12       False         True  \n13        True        False  \n14       False        False  \n15        True        False  \n16        True        False  \n17        True        False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>task_id</th>\n      <th>base_auc</th>\n      <th>base_time</th>\n      <th>new_auc</th>\n      <th>new_time</th>\n      <th>delta_auc</th>\n      <th>better_auc</th>\n      <th>better_time</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>16</td>\n      <td>0.997222</td>\n      <td>31.225133</td>\n      <td>0.994500</td>\n      <td>14.023385</td>\n      <td>-0.002722</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>22</td>\n      <td>0.973361</td>\n      <td>25.774230</td>\n      <td>0.968833</td>\n      <td>24.805384</td>\n      <td>-0.004528</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>31</td>\n      <td>0.841905</td>\n      <td>1.274992</td>\n      <td>0.810000</td>\n      <td>2.276000</td>\n      <td>-0.031905</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2074</td>\n      <td>0.989416</td>\n      <td>18.240999</td>\n      <td>0.989734</td>\n      <td>40.257960</td>\n      <td>0.000319</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2079</td>\n      <td>0.915436</td>\n      <td>3.369999</td>\n      <td>0.887997</td>\n      <td>1.475038</td>\n      <td>-0.027439</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>3493</td>\n      <td>0.976190</td>\n      <td>0.539915</td>\n      <td>0.500000</td>\n      <td>0.354961</td>\n      <td>-0.476190</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>3907</td>\n      <td>0.985866</td>\n      <td>2.770957</td>\n      <td>0.901596</td>\n      <td>0.324998</td>\n      <td>-0.084271</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>3913</td>\n      <td>0.906926</td>\n      <td>0.559001</td>\n      <td>0.898268</td>\n      <td>1.623962</td>\n      <td>-0.008658</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9950</td>\n      <td>0.996221</td>\n      <td>32.786961</td>\n      <td>0.991830</td>\n      <td>2.706999</td>\n      <td>-0.004390</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>9952</td>\n      <td>0.956024</td>\n      <td>2.991995</td>\n      <td>0.953802</td>\n      <td>3.121042</td>\n      <td>-0.002223</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>9971</td>\n      <td>0.668067</td>\n      <td>0.624963</td>\n      <td>0.696078</td>\n      <td>0.099001</td>\n      <td>0.028011</td>\n      <td>True</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>10106</td>\n      <td>0.999297</td>\n      <td>9.776999</td>\n      <td>0.999297</td>\n      <td>11.642998</td>\n      <td>0.000000</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>14954</td>\n      <td>0.910238</td>\n      <td>0.831042</td>\n      <td>0.884993</td>\n      <td>0.733303</td>\n      <td>-0.025245</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>14970</td>\n      <td>0.999844</td>\n      <td>748.155138</td>\n      <td>0.999898</td>\n      <td>1160.218471</td>\n      <td>0.000055</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>146212</td>\n      <td>0.999998</td>\n      <td>30.165969</td>\n      <td>0.999990</td>\n      <td>36.428032</td>\n      <td>-0.000009</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>15</th>\n      <td>167119</td>\n      <td>0.969022</td>\n      <td>34.102057</td>\n      <td>0.976392</td>\n      <td>69.063016</td>\n      <td>0.007370</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>16</th>\n      <td>167125</td>\n      <td>0.962805</td>\n      <td>64.667999</td>\n      <td>0.965734</td>\n      <td>140.439371</td>\n      <td>0.002929</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>17</th>\n      <td>168336</td>\n      <td>0.735831</td>\n      <td>170.242889</td>\n      <td>0.752603</td>\n      <td>219.147188</td>\n      <td>0.016772</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from python.baseline import XGBoostTest\n",
    "from python.project_utils import make_valid_config, meta_feature_names\n",
    "\n",
    "predicted_config = {}\n",
    "print(\"Selecting the best configuration for each task:\")\n",
    "for test_id in test_ids:\n",
    "    print(\"Task:\",test_id, \"time\", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "    meta_featues = test_features[test_features['task_id']==test_id].head(1)[training_meta_features].to_numpy()\n",
    "    helper_materix = np.tile(meta_featues,(sample_configs.shape[0], 1))\n",
    "    input = np.concatenate((helper_materix,sample_configs),axis=1)\n",
    "    predicted_auc = best_regressor.predict(input)\n",
    "    ind = np.argmax(predicted_auc)\n",
    "    predicted_config[test_id] = sample_configs[ind,:]\n",
    "\n",
    "baseline = pd.read_csv('./data/baseline_performance.csv')\n",
    "comparison = pd.DataFrame()\n",
    "for key,config in predicted_config.items():\n",
    "    \"\"\"For each of the Test tasks (18 in total) we evaluate the performance of its predicted best config for XGBoost\"\"\"\n",
    "    current_task = key\n",
    "    print(\"Evaluating task with id: \", current_task)\n",
    "    seed = 123\n",
    "    \"\"\"We save the performance in a dataframe containing:\n",
    "    1.task_id\n",
    "    2.baseline auc and time\n",
    "    3.new auc and time - for the predicted best configuration\n",
    "    \"\"\"\n",
    "    new_row = {}\n",
    "    new_row['task_id'] = current_task\n",
    "    base_performance = baseline[baseline['task_id'] == current_task].head(1)\n",
    "    new_row['base_auc'] = base_performance['auc'].values[0]\n",
    "    new_row['base_time'] = base_performance['timetrain'].values[0]\n",
    "\n",
    "    \"\"\"Corrections to continuous predictions that might not be possible\"\"\"\n",
    "    new_config = dict(zip(hyperparameters_data, config))\n",
    "    new_config = make_valid_config(new_config)\n",
    "\n",
    "    \"\"\"Evaluating new config performance\"\"\"\n",
    "    objective = XGBoostTest(current_task, meta_feature_names=meta_feature_names, seed=seed)\n",
    "    auc, traintime = objective.evaluate(new_config)\n",
    "\n",
    "    \"\"\"Adding info about new performance \"\"\"\n",
    "    new_row['new_auc'] = auc\n",
    "    print(\"New auc\", auc)\n",
    "    new_row['new_time'] = traintime\n",
    "    new_row['delta_auc'] = new_row['new_auc'] - new_row['base_auc']\n",
    "    new_row['better_auc'] = new_row['new_auc'] > new_row['base_auc']\n",
    "    new_row['better_time'] = new_row['new_time'] < new_row['base_time']\n",
    "    new_row = pd.DataFrame([new_row])\n",
    "    comparison = pd.concat([comparison, new_row], ignore_index=True)\n",
    "display(comparison)\n",
    "comparison.to_csv('./data/fulldata_nested_xgboost_results.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}