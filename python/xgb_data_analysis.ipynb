{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook was used to test some of the functions for taskwise regression and full_data regression\n",
    "Also for some preliminary analysis of the average performance data\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.core.display_functions import display\n",
    "data = pd.read_csv('./data/xgboost_meta_data.csv')\n",
    "# 6 million rows - original\n",
    "print(\"Original metadata \\n\")\n",
    "display(data)\n",
    "# Only one feature with missing values\n",
    "print(\"Number of missing values per feature\\n\")\n",
    "display(data.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#training tasks have between 18.916 and 61.258 evaluated runs\n",
    "print(\"Number of runs available per task\\n\")\n",
    "display(data.groupby(by=['task_id']).count().sort_values(by=['num_round'],ascending = False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we look at the average performance of each (configuration,dataset) pair\n",
    "\"\"\"\n",
    "data = pd.read_csv('./data/average_performance.csv')\n",
    "#training tasks have between 1657 and 10.954 unique configurations evaluated\n",
    "display(data.groupby(by=['data_id']).count().sort_values(by=['num_round'],ascending = False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(687101, 14) (679321, 14)\n"
     ]
    },
    {
     "data": {
      "text/plain": "         index  num_round    eta  gamma  lambda  alpha  subsample  max_depth  \\\ndata_id                                                                        \n312      10954      10954  10954  10954   10954  10954      10954      10954   \n1040     10738      10738  10738  10738   10738  10738      10738      10738   \n1487     10601      10601  10601  10601   10601  10601      10601      10601   \n40900    10471      10471  10471  10471   10471  10471      10471      10471   \n41143    10176      10176  10176  10176   10176  10176      10176      10176   \n...        ...        ...    ...    ...     ...    ...        ...        ...   \n6         1876       1876   1876   1876    1876   1876       1876       1876   \n1457      1553       1553   1553   1553    1553   1553       1553       1553   \n40927     1392       1392   1392   1392    1392   1392       1392       1392   \n41169     1275       1275   1275   1275    1275   1275       1275       1275   \n40923     1155       1155   1155   1155    1155   1155       1155       1155   \n\n         min_child_weight  colsample_bytree  colsample_bylevel  avg_auc  \\\ndata_id                                                                   \n312                 10954             10954              10954    10954   \n1040                10738             10738              10738    10738   \n1487                10601             10601              10601    10601   \n40900               10471             10471              10471    10471   \n41143               10176             10176              10176    10176   \n...                   ...               ...                ...      ...   \n6                    1876              1876               1876     1876   \n1457                 1553              1553               1553     1553   \n40927                1392              1392               1392     1392   \n41169                1275              1275               1275     1275   \n40923                1155              1155               1155     1155   \n\n         avg_time  \ndata_id            \n312         10954  \n1040        10738  \n1487        10601  \n40900       10471  \n41143       10176  \n...           ...  \n6            1876  \n1457         1553  \n40927        1392  \n41169        1275  \n40923        1155  \n\n[94 rows x 13 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>index</th>\n      <th>num_round</th>\n      <th>eta</th>\n      <th>gamma</th>\n      <th>lambda</th>\n      <th>alpha</th>\n      <th>subsample</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>colsample_bytree</th>\n      <th>colsample_bylevel</th>\n      <th>avg_auc</th>\n      <th>avg_time</th>\n    </tr>\n    <tr>\n      <th>data_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>312</th>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n      <td>10954</td>\n    </tr>\n    <tr>\n      <th>1040</th>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n      <td>10738</td>\n    </tr>\n    <tr>\n      <th>1487</th>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n      <td>10601</td>\n    </tr>\n    <tr>\n      <th>40900</th>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n      <td>10471</td>\n    </tr>\n    <tr>\n      <th>41143</th>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n      <td>10176</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n      <td>1876</td>\n    </tr>\n    <tr>\n      <th>1457</th>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n      <td>1553</td>\n    </tr>\n    <tr>\n      <th>40927</th>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n      <td>1392</td>\n    </tr>\n    <tr>\n      <th>41169</th>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n      <td>1275</td>\n    </tr>\n    <tr>\n      <th>40923</th>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n      <td>1155</td>\n    </tr>\n  </tbody>\n</table>\n<p>94 rows Ã— 13 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Limiting the average training time to 10 minutes keeps most of the configurations in the dataset\n",
    "This is later used when filtering the data for taskwise regrression\n",
    "\"\"\"\n",
    "data_timelimit = data[data['avg_time']<10*60]\n",
    "print(data.shape, data_timelimit.shape)\n",
    "#After limiting the avg_time we get between 1155 and 10.954 unique configurations per task\n",
    "display(data_timelimit.groupby(by=['data_id']).count().sort_values(by=['num_round'],ascending = False))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'get_average_performance' from 'python.project_utils' (C:\\Users\\Korisnik\\Desktop\\metalearning_xgboost\\python\\project_utils.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[1;32mIn [34]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[1;34m()\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mproject_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_dataset_to_task,get_average_performance\n\u001B[0;32m      3\u001B[0m task_dict \u001B[38;5;241m=\u001B[39m get_dataset_to_task()\n\u001B[0;32m      4\u001B[0m data_timelimit[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtask_id\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m data_timelimit[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdata_id\u001B[39m\u001B[38;5;124m'\u001B[39m]\u001B[38;5;241m.\u001B[39mmap(task_dict)\n",
      "\u001B[1;31mImportError\u001B[0m: cannot import name 'get_average_performance' from 'python.project_utils' (C:\\Users\\Korisnik\\Desktop\\metalearning_xgboost\\python\\project_utils.py)"
     ]
    }
   ],
   "source": [
    "from python.project_utils import get_dataset_to_task,get_average_performance\n",
    "\n",
    "task_dict = get_dataset_to_task()\n",
    "data_timelimit['task_id'] = data_timelimit['data_id'].map(task_dict)\n",
    "display(data_timelimit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "data": {
      "text/plain": "{3:       data_id  num_round       eta     gamma     lambda       alpha  \\\n 0           3          1  0.000097  0.002120   1.074280  156.026000   \n 1           3          1  0.000200  0.014477   0.001188    0.971001   \n 2           3          1  0.001281  0.000142   0.000156    0.012087   \n 3           3          1  0.001911  0.021976   0.009044    0.002017   \n 4           3          1  0.002022  0.000058   0.258957    1.867090   \n ...       ...        ...       ...       ...        ...         ...   \n 9945        3       5749  0.028359  0.000032   0.018361    0.007791   \n 9946        3       5845  0.131354  0.010094   0.003000    0.439140   \n 9947        3       5865  0.003243  0.000045   0.340472   18.326600   \n 9948        3       5887  0.010789  0.002366  21.107800    0.076290   \n 9949        3       5917  0.081559  4.644440   0.001400    0.253718   \n \n       subsample  max_depth  min_child_weight  colsample_bytree  \\\n 0      0.202192          2         16.668800          0.442926   \n 1      0.186941          7          1.547140          0.644196   \n 2      0.367161         12          1.027290          0.223957   \n 3      0.876026         14          1.951150          0.355134   \n 4      0.354213          4          2.447190          0.239795   \n ...         ...        ...               ...               ...   \n 9945   0.569283         13          2.053570          0.332356   \n 9946   0.403850          1        233.686000          0.214519   \n 9947   0.394949          6          6.671510          0.051653   \n 9948   0.241146         12         61.870400          0.537502   \n 9949   0.998722          1          0.592597          0.802198   \n \n       colsample_bylevel   avg_auc   avg_time  task_id  \n 0              0.435029  0.500000   0.264143        3  \n 1              0.558417  0.884843   0.280833        3  \n 2              0.909821  0.725330   0.240000        3  \n 3              0.076143  0.500000   0.335000        3  \n 4              0.874333  0.866785   0.246500        3  \n ...                 ...       ...        ...      ...  \n 9945           0.825170  0.999335  24.884750        3  \n 9946           0.663293  0.500000   9.541000        3  \n 9947           0.785170  0.978744  10.782600        3  \n 9948           0.805439  0.757829  14.016500        3  \n 9949           0.270201  0.990246   9.615667        3  \n \n [9950 rows x 14 columns],\n 6:        data_id  num_round       eta     gamma      lambda       alpha  \\\n 9950         6          1  0.024234  0.000067    1.000500  768.603000   \n 9951         6          1  0.099818  0.028129    0.001607    0.001102   \n 9952         6          1  0.101048  0.000067    3.301350    0.119235   \n 9953         6          1  0.139589  2.918780    0.326829  196.354000   \n 9954         6          2  0.004823  0.002550    0.016822    5.293030   \n ...        ...        ...       ...       ...         ...         ...   \n 11828        6       3520  0.168119  0.005091    0.001530    4.427620   \n 11829        6       4127  0.669168  0.002332   87.110400    0.000502   \n 11830        6       4180  0.504788  0.000805  416.362000  690.390000   \n 11831        6       4658  0.075037  0.000673    0.007150  601.580000   \n 11833        6       4734  0.003478  0.003172   45.664900  157.418000   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 9950    0.641650          5           4.64789          0.561103   \n 9951    0.821261         14           8.48476          0.245569   \n 9952    0.618137          3         106.53300          0.702793   \n 9953    0.786081          4          40.70170          0.516169   \n 9954    0.661372         15           5.38856          0.952915   \n ...          ...        ...               ...               ...   \n 11828   0.755557          7          12.01180          0.256184   \n 11829   0.102350         15          51.71450          0.323692   \n 11830   0.584915          4           2.18529          0.118481   \n 11831   0.430539          5          10.52920          0.788015   \n 11833   0.287994         12           6.58579          0.674964   \n \n        colsample_bylevel   avg_auc   avg_time  task_id  \n 9950            0.167461  0.500000    0.57750        6  \n 9951            0.451169  0.843565    4.34350        6  \n 9952            0.850884  0.864983    0.74610        6  \n 9953            0.952272  0.854778    0.56010        6  \n 9954            0.840901  0.976389    1.54355        6  \n ...                  ...       ...        ...      ...  \n 11828           0.046136  0.998837  451.21480        6  \n 11829           0.122990  0.946679  398.70570        6  \n 11830           0.510327  0.500000  368.32790        6  \n 11831           0.523482  0.500000  515.37045        6  \n 11833           0.537067  0.782395  532.33990        6  \n \n [1876 rows x 14 columns],\n 11:        data_id  num_round       eta     gamma    lambda       alpha  \\\n 11834       11          1  0.000097  0.002120  1.074280  156.026000   \n 11835       11          1  0.000200  0.014477  0.001188    0.971001   \n 11836       11          1  0.001911  0.021976  0.009044    0.002017   \n 11837       11          1  0.002022  0.000058  0.258957    1.867090   \n 11838       11          1  0.005065  1.015660  0.086148    0.338362   \n ...        ...        ...       ...       ...       ...         ...   \n 19725       11       5646  0.711899  0.000108  2.355870    0.039822   \n 19726       11       5656  0.018053  0.006614  0.009698  685.994000   \n 19727       11       5845  0.131354  0.010094  0.003000    0.439140   \n 19728       11       5865  0.003243  0.000045  0.340472   18.326600   \n 19729       11       5917  0.081559  4.644440  0.001400    0.253718   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 11834   0.202192          2         16.668800          0.442926   \n 11835   0.186941          7          1.547140          0.644196   \n 11836   0.876026         14          1.951150          0.355134   \n 11837   0.354213          4          2.447190          0.239795   \n 11838   0.667977         12         24.070300          0.907258   \n ...          ...        ...               ...               ...   \n 19725   0.218562         11         14.791100          0.133948   \n 19726   0.563012          8          4.999610          0.891314   \n 19727   0.403850          1        233.686000          0.214519   \n 19728   0.394949          6          6.671510          0.051653   \n 19729   0.998722          1          0.592597          0.802198   \n \n        colsample_bylevel   avg_auc   avg_time  task_id  \n 11834           0.435029  0.500000   0.144000       11  \n 11835           0.558417  0.800112   0.176375       11  \n 11836           0.076143  0.829359   0.181000       11  \n 11837           0.874333  0.763578   0.184333       11  \n 11838           0.637908  0.825171   0.139000       11  \n ...                  ...       ...        ...      ...  \n 19725           0.888355  0.929562  18.251667       11  \n 19726           0.876172  0.500000   3.904500       11  \n 19727           0.663293  0.500000   4.086875       11  \n 19728           0.785170  0.931995   4.493643       11  \n 19729           0.270201  0.955520   4.082000       11  \n \n [7896 rows x 14 columns],\n 12:        data_id  num_round       eta     gamma      lambda       alpha  \\\n 19730       12          1  0.000200  0.014477    0.001188    0.971001   \n 19731       12          1  0.001911  0.021976    0.009044    0.002017   \n 19732       12          1  0.002022  0.000058    0.258957    1.867090   \n 19733       12          1  0.005065  1.015660    0.086148    0.338362   \n 19734       12          1  0.005310  0.009738    0.018482    0.008865   \n ...        ...        ...       ...       ...         ...         ...   \n 26203       12       5626  0.003521  0.008594  169.189000    0.181859   \n 26204       12       5646  0.711899  0.000108    2.355870    0.039822   \n 26205       12       5656  0.018053  0.006614    0.009698  685.994000   \n 26206       12       5845  0.131354  0.010094    0.003000    0.439140   \n 26207       12       5865  0.003243  0.000045    0.340472   18.326600   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 19730   0.186941          7           1.54714          0.644196   \n 19731   0.876026         14           1.95115          0.355134   \n 19732   0.354213          4           2.44719          0.239795   \n 19733   0.667977         12          24.07030          0.907258   \n 19734   0.521716          9           3.37821          0.616922   \n ...          ...        ...               ...               ...   \n 26203   0.701476          5          57.84400          0.762448   \n 26204   0.218562         11          14.79110          0.133948   \n 26205   0.563012          8           4.99961          0.891314   \n 26206   0.403850          1         233.68600          0.214519   \n 26207   0.394949          6           6.67151          0.051653   \n \n        colsample_bylevel   avg_auc    avg_time  task_id  \n 19730           0.558417  0.963194    1.632000       12  \n 19731           0.076143  0.965623    8.170500       12  \n 19732           0.874333  0.964902    3.923267       12  \n 19733           0.637908  0.956243    0.614750       12  \n 19734           0.434134  0.972889    0.556500       12  \n ...                  ...       ...         ...      ...  \n 26203           0.675186  0.994747  293.633091       12  \n 26204           0.888355  0.994259   87.076000       12  \n 26205           0.876172  0.500000  267.946250       12  \n 26206           0.663293  0.500000  104.408250       12  \n 26207           0.785170  0.996597   92.887667       12  \n \n [6477 rows x 14 columns],\n 14:        data_id  num_round       eta     gamma    lambda       alpha  \\\n 26208       14          1  0.000200  0.014477  0.001188    0.971001   \n 26209       14          1  0.001911  0.021976  0.009044    0.002017   \n 26210       14          1  0.002022  0.000058  0.258957    1.867090   \n 26211       14          1  0.005065  1.015660  0.086148    0.338362   \n 26212       14          1  0.005310  0.009738  0.018482    0.008865   \n ...        ...        ...       ...       ...       ...         ...   \n 33698       14       5646  0.711899  0.000108  2.355870    0.039822   \n 33699       14       5656  0.018053  0.006614  0.009698  685.994000   \n 33700       14       5845  0.131354  0.010094  0.003000    0.439140   \n 33701       14       5865  0.003243  0.000045  0.340472   18.326600   \n 33702       14       5917  0.081559  4.644440  0.001400    0.253718   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 26208   0.186941          7          1.547140          0.644196   \n 26209   0.876026         14          1.951150          0.355134   \n 26210   0.354213          4          2.447190          0.239795   \n 26211   0.667977         12         24.070300          0.907258   \n 26212   0.521716          9          3.378210          0.616922   \n ...          ...        ...               ...               ...   \n 33698   0.218562         11         14.791100          0.133948   \n 33699   0.563012          8          4.999610          0.891314   \n 33700   0.403850          1        233.686000          0.214519   \n 33701   0.394949          6          6.671510          0.051653   \n 33702   0.998722          1          0.592597          0.802198   \n \n        colsample_bylevel   avg_auc    avg_time  task_id  \n 26208           0.558417  0.888990    0.433500       14  \n 26209           0.076143  0.800944    0.301000       14  \n 26210           0.874333  0.872564    0.311500       14  \n 26211           0.637908  0.925556    0.318500       14  \n 26212           0.434134  0.916132    0.473000       14  \n ...                  ...       ...         ...      ...  \n 33698           0.888355  0.958726   42.099500       14  \n 33699           0.876172  0.500000  124.849200       14  \n 33700           0.663293  0.500000   47.411500       14  \n 33701           0.785170  0.967977   48.865667       14  \n 33702           0.270201  0.975625   69.021000       14  \n \n [7495 rows x 14 columns],\n 15:        data_id  num_round       eta     gamma    lambda       alpha  \\\n 33703       15          1  0.000097  0.002120  1.074280  156.026000   \n 33704       15          1  0.000200  0.014477  0.001188    0.971001   \n 33705       15          1  0.001281  0.000142  0.000156    0.012087   \n 33706       15          1  0.001911  0.021976  0.009044    0.002017   \n 33707       15          1  0.002022  0.000058  0.258957    1.867090   \n ...        ...        ...       ...       ...       ...         ...   \n 42349       15       5646  0.711899  0.000108  2.355870    0.039822   \n 42350       15       5656  0.018053  0.006614  0.009698  685.994000   \n 42351       15       5845  0.131354  0.010094  0.003000    0.439140   \n 42352       15       5865  0.003243  0.000045  0.340472   18.326600   \n 42353       15       5917  0.081559  4.644440  0.001400    0.253718   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 33703   0.202192          2         16.668800          0.442926   \n 33704   0.186941          7          1.547140          0.644196   \n 33705   0.367161         12          1.027290          0.223957   \n 33706   0.876026         14          1.951150          0.355134   \n 33707   0.354213          4          2.447190          0.239795   \n ...          ...        ...               ...               ...   \n 42349   0.218562         11         14.791100          0.133948   \n 42350   0.563012          8          4.999610          0.891314   \n 42351   0.403850          1        233.686000          0.214519   \n 42352   0.394949          6          6.671510          0.051653   \n 42353   0.998722          1          0.592597          0.802198   \n \n        colsample_bylevel   avg_auc  avg_time  task_id  \n 33703           0.435029  0.500000  0.183500       15  \n 33704           0.558417  0.939991  0.186000       15  \n 33705           0.909821  0.938708  0.204333       15  \n 33706           0.076143  0.948855  0.172800       15  \n 33707           0.874333  0.826103  1.290800       15  \n ...                  ...       ...       ...      ...  \n 42349           0.888355  0.954503  3.201333       15  \n 42350           0.876172  0.500000  3.071333       15  \n 42351           0.663293  0.500000  2.799167       15  \n 42352           0.785170  0.987932  5.117385       15  \n 42353           0.270201  1.000000  2.878000       15  \n \n [8651 rows x 14 columns],\n 18:        data_id  num_round       eta     gamma    lambda       alpha  \\\n 42354       18          1  0.000097  0.002120  1.074280  156.026000   \n 42355       18          1  0.000200  0.014477  0.001188    0.971001   \n 42356       18          1  0.001281  0.000142  0.000156    0.012087   \n 42357       18          1  0.001911  0.021976  0.009044    0.002017   \n 42358       18          1  0.002022  0.000058  0.258957    1.867090   \n ...        ...        ...       ...       ...       ...         ...   \n 50881       18       5646  0.711899  0.000108  2.355870    0.039822   \n 50882       18       5656  0.018053  0.006614  0.009698  685.994000   \n 50883       18       5845  0.131354  0.010094  0.003000    0.439140   \n 50884       18       5865  0.003243  0.000045  0.340472   18.326600   \n 50885       18       5917  0.081559  4.644440  0.001400    0.253718   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 42354   0.202192          2         16.668800          0.442926   \n 42355   0.186941          7          1.547140          0.644196   \n 42356   0.367161         12          1.027290          0.223957   \n 42357   0.876026         14          1.951150          0.355134   \n 42358   0.354213          4          2.447190          0.239795   \n ...          ...        ...               ...               ...   \n 50881   0.218562         11         14.791100          0.133948   \n 50882   0.563012          8          4.999610          0.891314   \n 50883   0.403850          1        233.686000          0.214519   \n 50884   0.394949          6          6.671510          0.051653   \n 50885   0.998722          1          0.592597          0.802198   \n \n        colsample_bylevel   avg_auc   avg_time  task_id  \n 42354           0.435029  0.500000   0.175125       18  \n 42355           0.558417  0.904583   0.230000       18  \n 42356           0.909821  0.814799   0.247500       18  \n 42357           0.076143  0.891676   0.210333       18  \n 42358           0.874333  0.864049   0.152500       18  \n ...                  ...       ...        ...      ...  \n 50881           0.888355  0.929161  21.746400       18  \n 50882           0.876172  0.500000  23.539667       18  \n 50883           0.663293  0.500000  23.153600       18  \n 50884           0.785170  0.949491  32.107077       18  \n 50885           0.270201  0.957389  33.326000       18  \n \n [8532 rows x 14 columns],\n 23:        data_id  num_round       eta     gamma     lambda       alpha  \\\n 50886       23          1  0.000097  0.002120   1.074280  156.026000   \n 50887       23          1  0.001281  0.000142   0.000156    0.012087   \n 50888       23          1  0.001911  0.021976   0.009044    0.002017   \n 50889       23          1  0.002022  0.000058   0.258957    1.867090   \n 50890       23          1  0.004626  0.931871   7.967300   22.020300   \n ...        ...        ...       ...       ...        ...         ...   \n 60103       23       5646  0.711899  0.000108   2.355870    0.039822   \n 60104       23       5656  0.018053  0.006614   0.009698  685.994000   \n 60105       23       5865  0.003243  0.000045   0.340472   18.326600   \n 60106       23       5887  0.010789  0.002366  21.107800    0.076290   \n 60107       23       5917  0.081559  4.644440   0.001400    0.253718   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 50886   0.202192          2         16.668800          0.442926   \n 50887   0.367161         12          1.027290          0.223957   \n 50888   0.876026         14          1.951150          0.355134   \n 50889   0.354213          4          2.447190          0.239795   \n 50890   0.101276         12          4.499670          0.679333   \n ...          ...        ...               ...               ...   \n 60103   0.218562         11         14.791100          0.133948   \n 60104   0.563012          8          4.999610          0.891314   \n 60105   0.394949          6          6.671510          0.051653   \n 60106   0.241146         12         61.870400          0.537502   \n 60107   0.998722          1          0.592597          0.802198   \n \n        colsample_bylevel   avg_auc   avg_time  task_id  \n 50886           0.435029  0.500000   0.169667       23  \n 50887           0.909821  0.622410   0.271500       23  \n 50888           0.076143  0.550616   0.185000       23  \n 50889           0.874333  0.582812   0.185250       23  \n 50890           0.109048  0.500000   0.228333       23  \n ...                  ...       ...        ...      ...  \n 60103           0.888355  0.688047  11.425000       23  \n 60104           0.876172  0.500000  10.493500       23  \n 60105           0.785170  0.696309   8.233667       23  \n 60106           0.805439  0.675307   9.997000       23  \n 60107           0.270201  0.727230  18.620500       23  \n \n [9222 rows x 14 columns],\n 24:        data_id  num_round       eta     gamma     lambda       alpha  \\\n 60108       24          1  0.000097  0.002120   1.074280  156.026000   \n 60109       24          1  0.000200  0.014477   0.001188    0.971001   \n 60110       24          1  0.001281  0.000142   0.000156    0.012087   \n 60111       24          1  0.002022  0.000058   0.258957    1.867090   \n 60112       24          1  0.004626  0.931871   7.967300   22.020300   \n ...        ...        ...       ...       ...        ...         ...   \n 69456       24       5656  0.018053  0.006614   0.009698  685.994000   \n 69457       24       5845  0.131354  0.010094   0.003000    0.439140   \n 69458       24       5865  0.003243  0.000045   0.340472   18.326600   \n 69459       24       5887  0.010789  0.002366  21.107800    0.076290   \n 69460       24       5917  0.081559  4.644440   0.001400    0.253718   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 60108   0.202192          2         16.668800          0.442926   \n 60109   0.186941          7          1.547140          0.644196   \n 60110   0.367161         12          1.027290          0.223957   \n 60111   0.354213          4          2.447190          0.239795   \n 60112   0.101276         12          4.499670          0.679333   \n ...          ...        ...               ...               ...   \n 69456   0.563012          8          4.999610          0.891314   \n 69457   0.403850          1        233.686000          0.214519   \n 69458   0.394949          6          6.671510          0.051653   \n 69459   0.241146         12         61.870400          0.537502   \n 69460   0.998722          1          0.592597          0.802198   \n \n        colsample_bylevel   avg_auc   avg_time  task_id  \n 60108           0.435029  0.820800   0.380333       24  \n 60109           0.558417  0.989995   4.101250       24  \n 60110           0.909821  0.984704   0.371667       24  \n 60111           0.874333  0.987722   0.333500       24  \n 60112           0.109048  0.931453   0.377500       24  \n ...                  ...       ...        ...      ...  \n 69456           0.876172  0.993886  59.864000       24  \n 69457           0.663293  0.945935  36.195500       24  \n 69458           0.785170  0.994904  34.142286       24  \n 69459           0.805439  0.990000  47.532500       24  \n 69460           0.270201  0.999970  39.297000       24  \n \n [9353 rows x 14 columns],\n 28:        data_id  num_round       eta     gamma      lambda       alpha  \\\n 69461       28          1  0.000200  0.014477    0.001188    0.971001   \n 69462       28          1  0.001911  0.021976    0.009044    0.002017   \n 69463       28          1  0.002022  0.000058    0.258957    1.867090   \n 69464       28          1  0.005065  1.015660    0.086148    0.338362   \n 69465       28          1  0.005310  0.009738    0.018482    0.008865   \n ...        ...        ...       ...       ...         ...         ...   \n 75601       28       5626  0.003521  0.008594  169.189000    0.181859   \n 75602       28       5646  0.711899  0.000108    2.355870    0.039822   \n 75603       28       5656  0.018053  0.006614    0.009698  685.994000   \n 75604       28       5845  0.131354  0.010094    0.003000    0.439140   \n 75605       28       5865  0.003243  0.000045    0.340472   18.326600   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 69461   0.186941          7           1.54714          0.644196   \n 69462   0.876026         14           1.95115          0.355134   \n 69463   0.354213          4           2.44719          0.239795   \n 69464   0.667977         12          24.07030          0.907258   \n 69465   0.521716          9           3.37821          0.616922   \n ...          ...        ...               ...               ...   \n 75601   0.701476          5          57.84400          0.762448   \n 75602   0.218562         11          14.79110          0.133948   \n 75603   0.563012          8           4.99961          0.891314   \n 75604   0.403850          1         233.68600          0.214519   \n 75605   0.394949          6           6.67151          0.051653   \n \n        colsample_bylevel   avg_auc    avg_time  task_id  \n 69461           0.558417  0.969161    0.416667       28  \n 69462           0.076143  0.844127    0.365200       28  \n 69463           0.874333  0.937906    9.297250       28  \n 69464           0.637908  0.967309    0.430667       28  \n 69465           0.434134  0.973590    0.482667       28  \n ...                  ...       ...         ...      ...  \n 75601           0.675186  0.997480  318.426800       28  \n 75602           0.888355  0.996778  103.554000       28  \n 75603           0.876172  0.500000  209.564250       28  \n 75604           0.663293  0.500000  117.789500       28  \n 75605           0.785170  0.995547  130.998000       28  \n \n [6144 rows x 14 columns],\n 29:        data_id  num_round       eta     gamma    lambda       alpha  \\\n 75606       29          1  0.000097  0.002120  1.074280  156.026000   \n 75607       29          1  0.000200  0.014477  0.001188    0.971001   \n 75608       29          1  0.001281  0.000142  0.000156    0.012087   \n 75609       29          1  0.001911  0.021976  0.009044    0.002017   \n 75610       29          1  0.002022  0.000058  0.258957    1.867090   \n ...        ...        ...       ...       ...       ...         ...   \n 84607       29       5646  0.711899  0.000108  2.355870    0.039822   \n 84608       29       5656  0.018053  0.006614  0.009698  685.994000   \n 84609       29       5845  0.131354  0.010094  0.003000    0.439140   \n 84610       29       5865  0.003243  0.000045  0.340472   18.326600   \n 84611       29       5917  0.081559  4.644440  0.001400    0.253718   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 75606   0.202192          2         16.668800          0.442926   \n 75607   0.186941          7          1.547140          0.644196   \n 75608   0.367161         12          1.027290          0.223957   \n 75609   0.876026         14          1.951150          0.355134   \n 75610   0.354213          4          2.447190          0.239795   \n ...          ...        ...               ...               ...   \n 84607   0.218562         11         14.791100          0.133948   \n 84608   0.563012          8          4.999610          0.891314   \n 84609   0.403850          1        233.686000          0.214519   \n 84610   0.394949          6          6.671510          0.051653   \n 84611   0.998722          1          0.592597          0.802198   \n \n        colsample_bylevel   avg_auc   avg_time  task_id  \n 75606           0.435029  0.500000   0.416333       29  \n 75607           0.558417  0.796419   0.174143       29  \n 75608           0.909821  0.678163   0.192000       29  \n 75609           0.076143  0.492148   0.177500       29  \n 75610           0.874333  0.652141   0.185000       29  \n ...                  ...       ...        ...      ...  \n 84607           0.888355  0.905784   6.456750       29  \n 84608           0.876172  0.500000   4.287000       29  \n 84609           0.663293  0.500000   3.473000       29  \n 84610           0.785170  0.919764   3.338071       29  \n 84611           0.270201  0.928600  11.976667       29  \n \n [9006 rows x 14 columns],\n 32:        data_id  num_round       eta     gamma      lambda       alpha  \\\n 84612       32          1  0.000200  0.014477    0.001188    0.971001   \n 84613       32          1  0.001911  0.021976    0.009044    0.002017   \n 84614       32          1  0.002022  0.000058    0.258957    1.867090   \n 84615       32          1  0.005065  1.015660    0.086148    0.338362   \n 84616       32          1  0.005310  0.009738    0.018482    0.008865   \n ...        ...        ...       ...       ...         ...         ...   \n 90332       32       5626  0.003521  0.008594  169.189000    0.181859   \n 90333       32       5646  0.711899  0.000108    2.355870    0.039822   \n 90334       32       5656  0.018053  0.006614    0.009698  685.994000   \n 90335       32       5845  0.131354  0.010094    0.003000    0.439140   \n 90336       32       5865  0.003243  0.000045    0.340472   18.326600   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 84612   0.186941          7           1.54714          0.644196   \n 84613   0.876026         14           1.95115          0.355134   \n 84614   0.354213          4           2.44719          0.239795   \n 84615   0.667977         12          24.07030          0.907258   \n 84616   0.521716          9           3.37821          0.616922   \n ...          ...        ...               ...               ...   \n 90332   0.701476          5          57.84400          0.762448   \n 90333   0.218562         11          14.79110          0.133948   \n 90334   0.563012          8           4.99961          0.891314   \n 90335   0.403850          1         233.68600          0.214519   \n 90336   0.394949          6           6.67151          0.051653   \n \n        colsample_bylevel   avg_auc    avg_time  task_id  \n 84612           0.558417  0.983763    0.314000       32  \n 84613           0.076143  0.962463    0.328600       32  \n 84614           0.874333  0.920522    0.308500       32  \n 84615           0.637908  0.982066    0.510000       32  \n 84616           0.434134  0.984088    0.366800       32  \n ...                  ...       ...         ...      ...  \n 90332           0.675186  0.998647  308.969667       32  \n 90333           0.888355  0.999008  120.569600       32  \n 90334           0.876172  0.500000  162.627286       32  \n 90335           0.663293  0.980783  125.921167       32  \n 90336           0.785170  0.991207  176.051800       32  \n \n [5725 rows x 14 columns],\n 37:        data_id  num_round       eta     gamma    lambda       alpha  \\\n 90337       37          1  0.000097  0.002120  1.074280  156.026000   \n 90338       37          1  0.000200  0.014477  0.001188    0.971001   \n 90339       37          1  0.001281  0.000142  0.000156    0.012087   \n 90340       37          1  0.001911  0.021976  0.009044    0.002017   \n 90341       37          1  0.002022  0.000058  0.258957    1.867090   \n ...        ...        ...       ...       ...       ...         ...   \n 98857       37       5646  0.711899  0.000108  2.355870    0.039822   \n 98858       37       5656  0.018053  0.006614  0.009698  685.994000   \n 98859       37       5845  0.131354  0.010094  0.003000    0.439140   \n 98860       37       5865  0.003243  0.000045  0.340472   18.326600   \n 98861       37       5917  0.081559  4.644440  0.001400    0.253718   \n \n        subsample  max_depth  min_child_weight  colsample_bytree  \\\n 90337   0.202192          2         16.668800          0.442926   \n 90338   0.186941          7          1.547140          0.644196   \n 90339   0.367161         12          1.027290          0.223957   \n 90340   0.876026         14          1.951150          0.355134   \n 90341   0.354213          4          2.447190          0.239795   \n ...          ...        ...               ...               ...   \n 98857   0.218562         11         14.791100          0.133948   \n 98858   0.563012          8          4.999610          0.891314   \n 98859   0.403850          1        233.686000          0.214519   \n 98860   0.394949          6          6.671510          0.051653   \n 98861   0.998722          1          0.592597          0.802198   \n \n        colsample_bylevel   avg_auc  avg_time  task_id  \n 90337           0.435029  0.500000  0.149500       37  \n 90338           0.558417  0.692682  0.173000       37  \n 90339           0.909821  0.578471  0.157000       37  \n 90340           0.076143  0.683846  0.160000       37  \n 90341           0.874333  0.679255  0.213000       37  \n ...                  ...       ...       ...      ...  \n 98857           0.888355  0.792171  2.732400       37  \n 98858           0.876172  0.500000  2.777500       37  \n 98859           0.663293  0.500000  9.688250       37  \n 98860           0.785170  0.834603  2.981000       37  \n 98861           0.270201  0.829012  2.804667       37  \n \n [8525 rows x 14 columns],\n 3021:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 98862        38          1  0.000097  0.002120   1.074280  156.026000   \n 98863        38          1  0.000200  0.014477   0.001188    0.971001   \n 98864        38          1  0.001281  0.000142   0.000156    0.012087   \n 98865        38          1  0.001911  0.021976   0.009044    0.002017   \n 98866        38          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 108407       38       5656  0.018053  0.006614   0.009698  685.994000   \n 108408       38       5845  0.131354  0.010094   0.003000    0.439140   \n 108409       38       5865  0.003243  0.000045   0.340472   18.326600   \n 108410       38       5887  0.010789  0.002366  21.107800    0.076290   \n 108411       38       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 98862    0.202192          2         16.668800          0.442926   \n 98863    0.186941          7          1.547140          0.644196   \n 98864    0.367161         12          1.027290          0.223957   \n 98865    0.876026         14          1.951150          0.355134   \n 98866    0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 108407   0.563012          8          4.999610          0.891314   \n 108408   0.403850          1        233.686000          0.214519   \n 108409   0.394949          6          6.671510          0.051653   \n 108410   0.241146         12         61.870400          0.537502   \n 108411   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 98862            0.435029  0.662908   0.266400     3021  \n 98863            0.558417  0.774753   0.430667     3021  \n 98864            0.909821  0.695916   0.327000     3021  \n 98865            0.076143  0.591998   0.300667     3021  \n 98866            0.874333  0.771593   0.279250     3021  \n ...                   ...       ...        ...      ...  \n 108407           0.876172  0.908745  16.177200     3021  \n 108408           0.663293  0.500000  11.135500     3021  \n 108409           0.785170  0.958384  11.646429     3021  \n 108410           0.805439  0.865328  12.063000     3021  \n 108411           0.270201  0.964290  10.269000     3021  \n \n [9550 rows x 14 columns],\n 41:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 108412       42          1  0.000200  0.014477    0.001188    0.971001   \n 108413       42          1  0.001911  0.021976    0.009044    0.002017   \n 108414       42          1  0.002022  0.000058    0.258957    1.867090   \n 108415       42          1  0.005065  1.015660    0.086148    0.338362   \n 108416       42          1  0.005310  0.009738    0.018482    0.008865   \n ...         ...        ...       ...       ...         ...         ...   \n 115310       42       5626  0.003521  0.008594  169.189000    0.181859   \n 115311       42       5646  0.711899  0.000108    2.355870    0.039822   \n 115312       42       5656  0.018053  0.006614    0.009698  685.994000   \n 115313       42       5845  0.131354  0.010094    0.003000    0.439140   \n 115314       42       5865  0.003243  0.000045    0.340472   18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 108412   0.186941          7           1.54714          0.644196   \n 108413   0.876026         14           1.95115          0.355134   \n 108414   0.354213          4           2.44719          0.239795   \n 108415   0.667977         12          24.07030          0.907258   \n 108416   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 115310   0.701476          5          57.84400          0.762448   \n 115311   0.218562         11          14.79110          0.133948   \n 115312   0.563012          8           4.99961          0.891314   \n 115313   0.403850          1         233.68600          0.214519   \n 115314   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 108412           0.558417  0.857103   0.236000       41  \n 108413           0.076143  0.806864   0.201667       41  \n 108414           0.874333  0.848653   0.220400       41  \n 108415           0.637908  0.500000   0.248500       41  \n 108416           0.434134  0.911931   0.256500       41  \n ...                   ...       ...        ...      ...  \n 115310           0.675186  0.500000  59.785000       41  \n 115311           0.888355  0.835278  44.752200       41  \n 115312           0.876172  0.500000  77.506000       41  \n 115313           0.663293  0.500000  43.389400       41  \n 115314           0.785170  0.874276  37.827833       41  \n \n [6903 rows x 14 columns],\n 43:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 115315       44          1  0.000097  0.002120   1.074280  156.026000   \n 115316       44          1  0.000200  0.014477   0.001188    0.971001   \n 115317       44          1  0.001046  0.000033   0.001359  372.626000   \n 115318       44          1  0.001281  0.000142   0.000156    0.012087   \n 115319       44          1  0.001911  0.021976   0.009044    0.002017   \n ...         ...        ...       ...       ...        ...         ...   \n 125464       44       5646  0.711899  0.000108   2.355870    0.039822   \n 125465       44       5845  0.131354  0.010094   0.003000    0.439140   \n 125466       44       5865  0.003243  0.000045   0.340472   18.326600   \n 125467       44       5887  0.010789  0.002366  21.107800    0.076290   \n 125468       44       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 115315   0.202192          2         16.668800          0.442926   \n 115316   0.186941          7          1.547140          0.644196   \n 115317   0.591609         15          1.099660          0.641624   \n 115318   0.367161         12          1.027290          0.223957   \n 115319   0.876026         14          1.951150          0.355134   \n ...           ...        ...               ...               ...   \n 125464   0.218562         11         14.791100          0.133948   \n 125465   0.403850          1        233.686000          0.214519   \n 125466   0.394949          6          6.671510          0.051653   \n 125467   0.241146         12         61.870400          0.537502   \n 125468   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 115315           0.435029  0.630476   0.338667       43  \n 115316           0.558417  0.906909   0.311500       43  \n 115317           0.207744  0.827692   0.307200       43  \n 115318           0.909821  0.909939   0.369333       43  \n 115319           0.076143  0.850413   0.524000       43  \n ...                   ...       ...        ...      ...  \n 125464           0.888355  0.952243  16.827500       43  \n 125465           0.663293  0.500000  11.630000       43  \n 125466           0.785170  0.970875  17.767444       43  \n 125467           0.805439  0.956153  16.023000       43  \n 125468           0.270201  0.989132  24.840333       43  \n \n [10154 rows x 14 columns],\n 45:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 125469       46          1  0.000200  0.014477  0.001188    0.971001   \n 125470       46          1  0.001911  0.021976  0.009044    0.002017   \n 125471       46          1  0.002022  0.000058  0.258957    1.867090   \n 125472       46          1  0.005065  1.015660  0.086148    0.338362   \n 125473       46          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 132636       46       5646  0.711899  0.000108  2.355870    0.039822   \n 132637       46       5656  0.018053  0.006614  0.009698  685.994000   \n 132638       46       5845  0.131354  0.010094  0.003000    0.439140   \n 132639       46       5865  0.003243  0.000045  0.340472   18.326600   \n 132640       46       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 125469   0.186941          7          1.547140          0.644196   \n 125470   0.876026         14          1.951150          0.355134   \n 125471   0.354213          4          2.447190          0.239795   \n 125472   0.667977         12         24.070300          0.907258   \n 125473   0.521716          9          3.378210          0.616922   \n ...           ...        ...               ...               ...   \n 132636   0.218562         11         14.791100          0.133948   \n 132637   0.563012          8          4.999610          0.891314   \n 132638   0.403850          1        233.686000          0.214519   \n 132639   0.394949          6          6.671510          0.051653   \n 132640   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 125469           0.558417  0.908679    0.423250       45  \n 125470           0.076143  0.824134    0.393000       45  \n 125471           0.874333  0.800527    0.407429       45  \n 125472           0.637908  0.964021    0.501750       45  \n 125473           0.434134  0.930121    0.435400       45  \n ...                   ...       ...         ...      ...  \n 132636           0.888355  0.978975   63.324167       45  \n 132637           0.876172  0.500000  139.310500       45  \n 132638           0.663293  0.500000   60.211250       45  \n 132639           0.785170  0.988357   61.519813       45  \n 132640           0.270201  0.993189   64.270000       45  \n \n [7171 rows x 14 columns],\n 49:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 132641       50          1  0.000097  0.002120   1.074280  156.026000   \n 132642       50          1  0.000200  0.014477   0.001188    0.971001   \n 132643       50          1  0.001281  0.000142   0.000156    0.012087   \n 132644       50          1  0.001911  0.021976   0.009044    0.002017   \n 132645       50          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 141713       50       5656  0.018053  0.006614   0.009698  685.994000   \n 141714       50       5845  0.131354  0.010094   0.003000    0.439140   \n 141715       50       5865  0.003243  0.000045   0.340472   18.326600   \n 141716       50       5887  0.010789  0.002366  21.107800    0.076290   \n 141717       50       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 132641   0.202192          2         16.668800          0.442926   \n 132642   0.186941          7          1.547140          0.644196   \n 132643   0.367161         12          1.027290          0.223957   \n 132644   0.876026         14          1.951150          0.355134   \n 132645   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 141713   0.563012          8          4.999610          0.891314   \n 141714   0.403850          1        233.686000          0.214519   \n 141715   0.394949          6          6.671510          0.051653   \n 141716   0.241146         12         61.870400          0.537502   \n 141717   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 132641           0.435029  0.500000  0.194857       49  \n 132642           0.558417  0.725349  0.151000       49  \n 132643           0.909821  0.585317  0.218000       49  \n 132644           0.076143  0.597655  0.197667       49  \n 132645           0.874333  0.647481  0.162600       49  \n ...                   ...       ...       ...      ...  \n 141713           0.876172  0.500000  5.645000       49  \n 141714           0.663293  0.500000  3.437000       49  \n 141715           0.785170  0.759536  4.299375       49  \n 141716           0.805439  0.500000  3.560000       49  \n 141717           0.270201  0.777541  3.672000       49  \n \n [9077 rows x 14 columns],\n 53:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 141718       54          1  0.000097  0.002120  1.074280  156.026000   \n 141719       54          1  0.000200  0.014477  0.001188    0.971001   \n 141720       54          1  0.001281  0.000142  0.000156    0.012087   \n 141721       54          1  0.001911  0.021976  0.009044    0.002017   \n 141722       54          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 150853       54       5646  0.711899  0.000108  2.355870    0.039822   \n 150854       54       5656  0.018053  0.006614  0.009698  685.994000   \n 150855       54       5845  0.131354  0.010094  0.003000    0.439140   \n 150856       54       5865  0.003243  0.000045  0.340472   18.326600   \n 150857       54       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 141718   0.202192          2         16.668800          0.442926   \n 141719   0.186941          7          1.547140          0.644196   \n 141720   0.367161         12          1.027290          0.223957   \n 141721   0.876026         14          1.951150          0.355134   \n 141722   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 150853   0.218562         11         14.791100          0.133948   \n 150854   0.563012          8          4.999610          0.891314   \n 150855   0.403850          1        233.686000          0.214519   \n 150856   0.394949          6          6.671510          0.051653   \n 150857   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 141718           0.435029  0.500000   0.171600       53  \n 141719           0.558417  0.812917   0.457000       53  \n 141720           0.909821  0.811462   0.239333       53  \n 141721           0.076143  0.817150   0.170600       53  \n 141722           0.874333  0.782026   0.163000       53  \n ...                   ...       ...        ...      ...  \n 150853           0.888355  0.869565   7.788625       53  \n 150854           0.876172  0.500000   8.171333       53  \n 150855           0.663293  0.500000   6.117571       53  \n 150856           0.785170  0.815105  12.798800       53  \n 150857           0.270201  0.914722   6.738500       53  \n \n [9140 rows x 14 columns],\n 58:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 150858       60          1  0.000097  0.002120  1.074280  156.026000   \n 150859       60          1  0.000200  0.014477  0.001188    0.971001   \n 150860       60          1  0.001281  0.000142  0.000156    0.012087   \n 150861       60          1  0.001911  0.021976  0.009044    0.002017   \n 150862       60          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 159534       60       5646  0.711899  0.000108  2.355870    0.039822   \n 159535       60       5656  0.018053  0.006614  0.009698  685.994000   \n 159536       60       5845  0.131354  0.010094  0.003000    0.439140   \n 159537       60       5865  0.003243  0.000045  0.340472   18.326600   \n 159538       60       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 150858   0.202192          2         16.668800          0.442926   \n 150859   0.186941          7          1.547140          0.644196   \n 150860   0.367161         12          1.027290          0.223957   \n 150861   0.876026         14          1.951150          0.355134   \n 150862   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 159534   0.218562         11         14.791100          0.133948   \n 159535   0.563012          8          4.999610          0.891314   \n 159536   0.403850          1        233.686000          0.214519   \n 159537   0.394949          6          6.671510          0.051653   \n 159538   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 150858           0.435029  0.500000   0.330571       58  \n 150859           0.558417  0.894365   0.314000       58  \n 150860           0.909821  0.808740   0.320333       58  \n 150861           0.076143  0.801197   0.292500       58  \n 150862           0.874333  0.881304   0.297125       58  \n ...                   ...       ...        ...      ...  \n 159534           0.888355  0.948800  27.269429       58  \n 159535           0.876172  0.500000  48.875167       58  \n 159536           0.663293  0.958328  36.710000       58  \n 159537           0.785170  0.962710  33.926667       58  \n 159538           0.270201  0.970178  29.280667       58  \n \n [8681 rows x 14 columns],\n 219:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 159539      151          1  0.000200  0.014477  0.001188    0.971001   \n 159540      151          1  0.001911  0.021976  0.009044    0.002017   \n 159541      151          1  0.002022  0.000058  0.258957    1.867090   \n 159542      151          1  0.005065  1.015660  0.086148    0.338362   \n 159543      151          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 167244      151       5646  0.711899  0.000108  2.355870    0.039822   \n 167245      151       5656  0.018053  0.006614  0.009698  685.994000   \n 167246      151       5845  0.131354  0.010094  0.003000    0.439140   \n 167247      151       5865  0.003243  0.000045  0.340472   18.326600   \n 167248      151       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 159539   0.186941          7          1.547140          0.644196   \n 159540   0.876026         14          1.951150          0.355134   \n 159541   0.354213          4          2.447190          0.239795   \n 159542   0.667977         12         24.070300          0.907258   \n 159543   0.521716          9          3.378210          0.616922   \n ...           ...        ...               ...               ...   \n 167244   0.218562         11         14.791100          0.133948   \n 167245   0.563012          8          4.999610          0.891314   \n 167246   0.403850          1        233.686000          0.214519   \n 167247   0.394949          6          6.671510          0.051653   \n 167248   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 159539           0.558417  0.772801    0.537500      219  \n 159540           0.076143  0.600861    0.536857      219  \n 159541           0.874333  0.690401    0.522364      219  \n 159542           0.637908  0.883228    0.625083      219  \n 159543           0.434134  0.797150    0.569333      219  \n ...                   ...       ...         ...      ...  \n 167244           0.888355  0.943625  100.270615      219  \n 167245           0.876172  0.839882  101.800125      219  \n 167246           0.663293  0.894794   67.053143      219  \n 167247           0.785170  0.864575   92.875429      219  \n 167248           0.270201  0.897535   65.325833      219  \n \n [7710 rows x 14 columns],\n 2073:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 167249      181          1  0.000097  0.002120  1.074280  156.026000   \n 167250      181          1  0.000200  0.014477  0.001188    0.971001   \n 167251      181          1  0.001281  0.000142  0.000156    0.012087   \n 167252      181          1  0.001911  0.021976  0.009044    0.002017   \n 167253      181          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 175122      181       5646  0.711899  0.000108  2.355870    0.039822   \n 175123      181       5656  0.018053  0.006614  0.009698  685.994000   \n 175124      181       5845  0.131354  0.010094  0.003000    0.439140   \n 175125      181       5865  0.003243  0.000045  0.340472   18.326600   \n 175126      181       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 167249   0.202192          2         16.668800          0.442926   \n 167250   0.186941          7          1.547140          0.644196   \n 167251   0.367161         12          1.027290          0.223957   \n 167252   0.876026         14          1.951150          0.355134   \n 167253   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 175122   0.218562         11         14.791100          0.133948   \n 175123   0.563012          8          4.999610          0.891314   \n 175124   0.403850          1        233.686000          0.214519   \n 175125   0.394949          6          6.671510          0.051653   \n 175126   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 167249           0.435029  0.500000   0.209000     2073  \n 167250           0.558417  0.706892   0.153500     2073  \n 167251           0.909821  0.625879   0.206000     2073  \n 167252           0.076143  0.656320   0.305000     2073  \n 167253           0.874333  0.647202   1.392000     2073  \n ...                   ...       ...        ...      ...  \n 175122           0.888355  0.806905  31.482667     2073  \n 175123           0.876172  0.500000  19.120500     2073  \n 175124           0.663293  0.500000  17.682333     2073  \n 175125           0.785170  0.839149  21.840750     2073  \n 175126           0.270201  0.831631  17.497000     2073  \n \n [7878 rows x 14 columns],\n 3481:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 175127      300          1  0.024234  0.000067    1.000500  768.603000   \n 175128      300          1  0.094189  0.000669  183.615000    0.017106   \n 175129      300          1  0.099818  0.028129    0.001607    0.001102   \n 175130      300          1  0.101048  0.000067    3.301350    0.119235   \n 175131      300          1  0.139589  2.918780    0.326829  196.354000   \n ...         ...        ...       ...       ...         ...         ...   \n 177282      300       1920  0.637484  0.000118    3.153670    0.053353   \n 177285      300       1941  0.001571  0.002780    0.004328   19.250500   \n 177293      300       2042  0.010077  5.448790   17.985100    0.003473   \n 177294      300       2042  0.024953  0.028946   69.766500    0.113284   \n 177295      300       2107  0.167591  0.001877  551.975000    8.572160   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 175127   0.641650          5           4.64789          0.561103   \n 175128   0.879396         10           3.90781          0.312301   \n 175129   0.821261         14           8.48476          0.245569   \n 175130   0.618137          3         106.53300          0.702793   \n 175131   0.786081          4          40.70170          0.516169   \n ...           ...        ...               ...               ...   \n 177282   0.118039          6           3.21059          0.172644   \n 177285   0.906573          8         257.58000          0.073961   \n 177293   0.516276         14        4776.42000          0.611708   \n 177294   0.144710         11          42.44260          0.420743   \n 177295   0.130110         16          66.11220          0.113656   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 175127           0.167461  0.500000    3.092222     3481  \n 175128           0.710544  0.969189    7.647050     3481  \n 175129           0.451169  0.963883    4.422667     3481  \n 175130           0.850884  0.858776    8.591474     3481  \n 175131           0.952272  0.500000    5.334649     3481  \n ...                   ...       ...         ...      ...  \n 177282           0.150899  0.996718  520.762737     3481  \n 177285           0.038247  0.500000  494.150684     3481  \n 177293           0.029486  0.500000  573.452889     3481  \n 177294           0.086915  0.552124  573.710885     3481  \n 177295           0.420408  0.500000  592.868842     3481  \n \n [1895 rows x 14 columns],\n 3022:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 177307      307          1  0.000097  0.002120  1.074280  156.026000   \n 177308      307          1  0.000200  0.014477  0.001188    0.971001   \n 177309      307          1  0.001911  0.021976  0.009044    0.002017   \n 177310      307          1  0.002022  0.000058  0.258957    1.867090   \n 177311      307          1  0.005065  1.015660  0.086148    0.338362   \n ...         ...        ...       ...       ...       ...         ...   \n 185488      307       5646  0.711899  0.000108  2.355870    0.039822   \n 185489      307       5656  0.018053  0.006614  0.009698  685.994000   \n 185490      307       5845  0.131354  0.010094  0.003000    0.439140   \n 185491      307       5865  0.003243  0.000045  0.340472   18.326600   \n 185492      307       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 177307   0.202192          2         16.668800          0.442926   \n 177308   0.186941          7          1.547140          0.644196   \n 177309   0.876026         14          1.951150          0.355134   \n 177310   0.354213          4          2.447190          0.239795   \n 177311   0.667977         12         24.070300          0.907258   \n ...           ...        ...               ...               ...   \n 185488   0.218562         11         14.791100          0.133948   \n 185489   0.563012          8          4.999610          0.891314   \n 185490   0.403850          1        233.686000          0.214519   \n 185491   0.394949          6          6.671510          0.051653   \n 185492   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 177307           0.435029  0.500000   0.323286     3022  \n 177308           0.558417  0.795258   0.190750     3022  \n 177309           0.076143  0.722110   0.251333     3022  \n 177310           0.874333  0.736644   0.247250     3022  \n 177311           0.637908  0.775393   0.216000     3022  \n ...                   ...       ...        ...      ...  \n 185488           0.888355  0.923232  15.733778     3022  \n 185489           0.876172  0.500000  27.210500     3022  \n 185490           0.663293  0.500000  17.813333     3022  \n 185491           0.785170  0.835690  16.671182     3022  \n 185492           0.270201  0.960157  19.148000     3022  \n \n [8186 rows x 14 columns],\n 3485:         data_id  num_round       eta      gamma      lambda       alpha  \\\n 185493      312          1  0.000097   0.002120    1.074280  156.026000   \n 185494      312          1  0.000200   0.014477    0.001188    0.971001   \n 185495      312          1  0.001046   0.000033    0.001359  372.626000   \n 185496      312          1  0.001281   0.000142    0.000156    0.012087   \n 185497      312          1  0.001513  29.504900  463.819000   22.707600   \n ...         ...        ...       ...        ...         ...         ...   \n 196442      312       5656  0.018053   0.006614    0.009698  685.994000   \n 196443      312       5749  0.028359   0.000032    0.018361    0.007791   \n 196444      312       5865  0.003243   0.000045    0.340472   18.326600   \n 196445      312       5887  0.010789   0.002366   21.107800    0.076290   \n 196446      312       5917  0.081559   4.644440    0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 185493   0.202192          2         16.668800          0.442926   \n 185494   0.186941          7          1.547140          0.644196   \n 185495   0.591609         15          1.099660          0.641624   \n 185496   0.367161         12          1.027290          0.223957   \n 185497   0.634888          8         15.484400          0.670356   \n ...           ...        ...               ...               ...   \n 196442   0.563012          8          4.999610          0.891314   \n 196443   0.569283         13          2.053570          0.332356   \n 196444   0.394949          6          6.671510          0.051653   \n 196445   0.241146         12         61.870400          0.537502   \n 196446   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 185493           0.435029  0.500000   0.786500     3485  \n 185494           0.558417  0.746265   0.688143     3485  \n 185495           0.207744  0.513419   0.681000     3485  \n 185496           0.909821  0.770767   0.850000     3485  \n 185497           0.689271  0.500000   0.776200     3485  \n ...                   ...       ...        ...      ...  \n 196442           0.876172  0.500000  57.362000     3485  \n 196443           0.825170  0.978781  48.515500     3485  \n 196444           0.785170  0.943366  23.500333     3485  \n 196445           0.805439  0.830922  26.244667     3485  \n 196446           0.270201  0.996618  27.219000     3485  \n \n [10954 rows x 14 columns],\n 3510:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 196447      375          1  0.000200  0.014477    0.001188    0.971001   \n 196448      375          1  0.001911  0.021976    0.009044    0.002017   \n 196449      375          1  0.002022  0.000058    0.258957    1.867090   \n 196450      375          1  0.005065  1.015660    0.086148    0.338362   \n 196451      375          1  0.005310  0.009738    0.018482    0.008865   \n ...         ...        ...       ...       ...         ...         ...   \n 202164      375       5626  0.003521  0.008594  169.189000    0.181859   \n 202165      375       5646  0.711899  0.000108    2.355870    0.039822   \n 202166      375       5656  0.018053  0.006614    0.009698  685.994000   \n 202167      375       5845  0.131354  0.010094    0.003000    0.439140   \n 202168      375       5865  0.003243  0.000045    0.340472   18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 196447   0.186941          7           1.54714          0.644196   \n 196448   0.876026         14           1.95115          0.355134   \n 196449   0.354213          4           2.44719          0.239795   \n 196450   0.667977         12          24.07030          0.907258   \n 196451   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 202164   0.701476          5          57.84400          0.762448   \n 202165   0.218562         11          14.79110          0.133948   \n 202166   0.563012          8           4.99961          0.891314   \n 202167   0.403850          1         233.68600          0.214519   \n 202168   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 196447           0.558417  0.933653    0.324692     3510  \n 196448           0.076143  0.871196    0.340500     3510  \n 196449           0.874333  0.831333    0.309867     3510  \n 196450           0.637908  0.951383    0.354667     3510  \n 196451           0.434134  0.944417    0.353600     3510  \n ...                   ...       ...         ...      ...  \n 202164           0.675186  0.996387  311.197000     3510  \n 202165           0.888355  0.995228  101.446833     3510  \n 202166           0.876172  0.500000  144.133000     3510  \n 202167           0.663293  0.975118  113.515600     3510  \n 202168           0.785170  0.983610  150.009444     3510  \n \n [5722 rows x 14 columns],\n 3512:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 202169      377          1  0.000097  0.002120  1.074280  156.026000   \n 202170      377          1  0.000200  0.014477  0.001188    0.971001   \n 202171      377          1  0.001281  0.000142  0.000156    0.012087   \n 202172      377          1  0.001911  0.021976  0.009044    0.002017   \n 202173      377          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 210514      377       5646  0.711899  0.000108  2.355870    0.039822   \n 210515      377       5656  0.018053  0.006614  0.009698  685.994000   \n 210516      377       5845  0.131354  0.010094  0.003000    0.439140   \n 210517      377       5865  0.003243  0.000045  0.340472   18.326600   \n 210518      377       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 202169   0.202192          2         16.668800          0.442926   \n 202170   0.186941          7          1.547140          0.644196   \n 202171   0.367161         12          1.027290          0.223957   \n 202172   0.876026         14          1.951150          0.355134   \n 202173   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 210514   0.218562         11         14.791100          0.133948   \n 210515   0.563012          8          4.999610          0.891314   \n 210516   0.403850          1        233.686000          0.214519   \n 210517   0.394949          6          6.671510          0.051653   \n 210518   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 202169           0.435029  0.500000   0.267250     3512  \n 202170           0.558417  0.940333   0.235000     3512  \n 202171           0.909821  0.966528   0.276000     3512  \n 202172           0.076143  0.960190   0.214000     3512  \n 202173           0.874333  0.965444   0.239833     3512  \n ...                   ...       ...        ...      ...  \n 210514           0.888355  0.988889   8.987333     3512  \n 210515           0.876172  0.500000  40.667000     3512  \n 210516           0.663293  0.500000  16.948333     3512  \n 210517           0.785170  0.993381  12.589000     3512  \n 210518           0.270201  0.996083  15.431250     3512  \n \n [8350 rows x 14 columns],\n 3549:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 210519      458          1  0.000097  0.002120  1.074280  156.026000   \n 210520      458          1  0.000200  0.014477  0.001188    0.971001   \n 210521      458          1  0.001281  0.000142  0.000156    0.012087   \n 210522      458          1  0.001911  0.021976  0.009044    0.002017   \n 210523      458          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 219277      458       5646  0.711899  0.000108  2.355870    0.039822   \n 219278      458       5656  0.018053  0.006614  0.009698  685.994000   \n 219279      458       5845  0.131354  0.010094  0.003000    0.439140   \n 219280      458       5865  0.003243  0.000045  0.340472   18.326600   \n 219281      458       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 210519   0.202192          2         16.668800          0.442926   \n 210520   0.186941          7          1.547140          0.644196   \n 210521   0.367161         12          1.027290          0.223957   \n 210522   0.876026         14          1.951150          0.355134   \n 210523   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 219277   0.218562         11         14.791100          0.133948   \n 219278   0.563012          8          4.999610          0.891314   \n 219279   0.403850          1        233.686000          0.214519   \n 219280   0.394949          6          6.671510          0.051653   \n 219281   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 210519           0.435029  0.500000   0.254333     3549  \n 210520           0.558417  0.954812   0.327000     3549  \n 210521           0.909821  0.933534   0.259250     3549  \n 210522           0.076143  0.894931   0.233250     3549  \n 210523           0.874333  0.956533   0.266750     3549  \n ...                   ...       ...        ...      ...  \n 219277           0.888355  0.996302   8.472750     3549  \n 219278           0.876172  0.500000  17.390000     3549  \n 219279           0.663293  0.500000   9.607000     3549  \n 219280           0.785170  0.998466  10.993846     3549  \n 219281           0.270201  0.999902  10.417500     3549  \n \n [8763 rows x 14 columns],\n 3560:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 219282      469          1  0.000097  0.002120    1.074280  156.026000   \n 219283      469          1  0.000200  0.014477    0.001188    0.971001   \n 219284      469          1  0.001281  0.000142    0.000156    0.012087   \n 219285      469          1  0.001911  0.021976    0.009044    0.002017   \n 219286      469          1  0.002022  0.000058    0.258957    1.867090   \n ...         ...        ...       ...       ...         ...         ...   \n 227908      469       5626  0.003521  0.008594  169.189000    0.181859   \n 227909      469       5646  0.711899  0.000108    2.355870    0.039822   \n 227910      469       5656  0.018053  0.006614    0.009698  685.994000   \n 227911      469       5845  0.131354  0.010094    0.003000    0.439140   \n 227912      469       5865  0.003243  0.000045    0.340472   18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 219282   0.202192          2          16.66880          0.442926   \n 219283   0.186941          7           1.54714          0.644196   \n 219284   0.367161         12           1.02729          0.223957   \n 219285   0.876026         14           1.95115          0.355134   \n 219286   0.354213          4           2.44719          0.239795   \n ...           ...        ...               ...               ...   \n 227908   0.701476          5          57.84400          0.762448   \n 227909   0.218562         11          14.79110          0.133948   \n 227910   0.563012          8           4.99961          0.891314   \n 227911   0.403850          1         233.68600          0.214519   \n 227912   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 219282           0.435029  0.500000   0.159143     3560  \n 219283           0.558417  0.570832   0.183000     3560  \n 219284           0.909821  0.566612   0.177500     3560  \n 219285           0.076143  0.562058   0.160222     3560  \n 219286           0.874333  0.575562   0.224778     3560  \n ...                   ...       ...        ...      ...  \n 227908           0.675186  0.582447  13.406500     3560  \n 227909           0.888355  0.579336  11.492500     3560  \n 227910           0.876172  0.500000  20.192750     3560  \n 227911           0.663293  0.500000   8.237000     3560  \n 227912           0.785170  0.569628   8.959375     3560  \n \n [8631 rows x 14 columns],\n 3561:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 227913      470          1  0.000097  0.002120  1.074280  156.026000   \n 227914      470          1  0.000200  0.014477  0.001188    0.971001   \n 227915      470          1  0.001281  0.000142  0.000156    0.012087   \n 227916      470          1  0.001911  0.021976  0.009044    0.002017   \n 227917      470          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 236961      470       5646  0.711899  0.000108  2.355870    0.039822   \n 236962      470       5656  0.018053  0.006614  0.009698  685.994000   \n 236963      470       5845  0.131354  0.010094  0.003000    0.439140   \n 236964      470       5865  0.003243  0.000045  0.340472   18.326600   \n 236965      470       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 227913   0.202192          2         16.668800          0.442926   \n 227914   0.186941          7          1.547140          0.644196   \n 227915   0.367161         12          1.027290          0.223957   \n 227916   0.876026         14          1.951150          0.355134   \n 227917   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 236961   0.218562         11         14.791100          0.133948   \n 236962   0.563012          8          4.999610          0.891314   \n 236963   0.403850          1        233.686000          0.214519   \n 236964   0.394949          6          6.671510          0.051653   \n 236965   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 227913           0.435029  0.500000  0.179500     3561  \n 227914           0.558417  0.561095  0.182750     3561  \n 227915           0.909821  0.554842  0.176000     3561  \n 227916           0.076143  0.515810  0.171500     3561  \n 227917           0.874333  0.505679  0.224750     3561  \n ...                   ...       ...       ...      ...  \n 236961           0.888355  0.641792  3.437200     3561  \n 236962           0.876172  0.500000  5.013500     3561  \n 236963           0.663293  0.500000  3.520600     3561  \n 236964           0.785170  0.643712  3.401444     3561  \n 236965           0.270201  0.638794  3.853200     3561  \n \n [9053 rows x 14 columns],\n 3573:         data_id  num_round       eta     gamma        lambda       alpha  \\\n 236966      554          1  0.000200  0.014477      0.001188    0.971001   \n 236967      554          1  0.005065  1.015660      0.086148    0.338362   \n 236968      554          1  0.006275  0.007421      0.047000    5.249080   \n 236969      554          1  0.006948  0.000345  18183.000000    0.034853   \n 236970      554          1  0.010452  0.004530      0.031252    0.018283   \n ...         ...        ...       ...       ...           ...         ...   \n 239417      554        489  0.132535  0.000170      0.023690    1.350810   \n 239421      554        499  0.090315  0.013687      0.068119  967.467000   \n 239427      554        502  0.694250  0.000131   7207.160000   19.949200   \n 239433      554        522  0.001551  0.015698     29.136900  786.250000   \n 239445      554        535  0.375936  0.044922      0.053736    0.098094   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 236966   0.186941          7           1.54714          0.644196   \n 236967   0.667977         12          24.07030          0.907258   \n 236968   0.959774         15           2.04807          0.790151   \n 236969   0.194973          1         115.84000          0.506227   \n 236970   0.583031          7           2.32741          0.574711   \n ...           ...        ...               ...               ...   \n 239417   0.245295         10          77.13290          0.119859   \n 239421   0.173585          6           2.73419          0.028564   \n 239427   0.403335          6           5.35574          0.023301   \n 239433   0.166916         10          14.22930          0.180971   \n 239445   0.313928          8          91.68010          0.027972   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 236966           0.558417  0.969841   34.4491     3573  \n 236967           0.637908  0.978927   43.0723     3573  \n 236968           0.896264  0.987203   52.0011     3573  \n 236969           0.956662  0.806467   19.3343     3573  \n 236970           0.752382  0.975029   33.6630     3573  \n ...                   ...       ...       ...      ...  \n 239417           0.036289  0.996435  556.0438     3573  \n 239421           0.169590  0.500000  514.2357     3573  \n 239427           0.042804  0.986878  542.5005     3573  \n 239433           0.331794  0.612869  597.3894     3573  \n 239445           0.192872  0.997161  592.4611     3573  \n \n [1941 rows x 14 columns],\n 3893:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 239612     1040          1  0.000097  0.002120   1.074280  156.026000   \n 239613     1040          1  0.000200  0.014477   0.001188    0.971001   \n 239614     1040          1  0.001046  0.000033   0.001359  372.626000   \n 239615     1040          1  0.001281  0.000142   0.000156    0.012087   \n 239616     1040          1  0.001911  0.021976   0.009044    0.002017   \n ...         ...        ...       ...       ...        ...         ...   \n 250345     1040       5749  0.028359  0.000032   0.018361    0.007791   \n 250346     1040       5845  0.131354  0.010094   0.003000    0.439140   \n 250347     1040       5865  0.003243  0.000045   0.340472   18.326600   \n 250348     1040       5887  0.010789  0.002366  21.107800    0.076290   \n 250349     1040       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 239612   0.202192          2         16.668800          0.442926   \n 239613   0.186941          7          1.547140          0.644196   \n 239614   0.591609         15          1.099660          0.641624   \n 239615   0.367161         12          1.027290          0.223957   \n 239616   0.876026         14          1.951150          0.355134   \n ...           ...        ...               ...               ...   \n 250345   0.569283         13          2.053570          0.332356   \n 250346   0.403850          1        233.686000          0.214519   \n 250347   0.394949          6          6.671510          0.051653   \n 250348   0.241146         12         61.870400          0.537502   \n 250349   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 239612           0.435029  0.704357    0.790000     3893  \n 239613           0.558417  0.964546    0.762000     3893  \n 239614           0.207744  0.886265    1.135286     3893  \n 239615           0.909821  0.951733    0.722333     3893  \n 239616           0.076143  0.852777    0.953800     3893  \n ...                   ...       ...         ...      ...  \n 250345           0.825170  0.998615  101.117500     3893  \n 250346           0.663293  0.992263   52.084333     3893  \n 250347           0.785170  0.997286   44.682667     3893  \n 250348           0.805439  0.994264   58.834000     3893  \n 250349           0.270201  0.999042   50.296400     3893  \n \n [10738 rows x 14 columns],\n 3902:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 250350     1049          1  0.000097  0.002120   1.074280  156.026000   \n 250351     1049          1  0.002022  0.000058   0.258957    1.867090   \n 250352     1049          1  0.004626  0.931871   7.967300   22.020300   \n 250353     1049          1  0.005065  1.015660   0.086148    0.338362   \n 250354     1049          1  0.005310  0.009738   0.018482    0.008865   \n ...         ...        ...       ...       ...        ...         ...   \n 259729     1049       5656  0.018053  0.006614   0.009698  685.994000   \n 259730     1049       5845  0.131354  0.010094   0.003000    0.439140   \n 259731     1049       5865  0.003243  0.000045   0.340472   18.326600   \n 259732     1049       5887  0.010789  0.002366  21.107800    0.076290   \n 259733     1049       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 250350   0.202192          2         16.668800          0.442926   \n 250351   0.354213          4          2.447190          0.239795   \n 250352   0.101276         12          4.499670          0.679333   \n 250353   0.667977         12         24.070300          0.907258   \n 250354   0.521716          9          3.378210          0.616922   \n ...           ...        ...               ...               ...   \n 259729   0.563012          8          4.999610          0.891314   \n 259730   0.403850          1        233.686000          0.214519   \n 259731   0.394949          6          6.671510          0.051653   \n 259732   0.241146         12         61.870400          0.537502   \n 259733   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 250350           0.435029  0.500000  0.196667     3902  \n 250351           0.874333  0.747938  0.260750     3902  \n 250352           0.109048  0.500000  0.194333     3902  \n 250353           0.637908  0.836372  0.188000     3902  \n 250354           0.434134  0.858077  0.195667     3902  \n ...                   ...       ...       ...      ...  \n 259729           0.876172  0.500000  6.229667     3902  \n 259730           0.663293  0.500000  4.779400     3902  \n 259731           0.785170  0.859003  4.459429     3902  \n 259732           0.805439  0.500000  5.003800     3902  \n 259733           0.270201  0.922152  4.835500     3902  \n \n [9384 rows x 14 columns],\n 3903:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 259734     1050          1  0.000097  0.002120   1.074280  156.026000   \n 259735     1050          1  0.000200  0.014477   0.001188    0.971001   \n 259736     1050          1  0.001281  0.000142   0.000156    0.012087   \n 259737     1050          1  0.001911  0.021976   0.009044    0.002017   \n 259738     1050          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 269497     1050       5656  0.018053  0.006614   0.009698  685.994000   \n 269498     1050       5845  0.131354  0.010094   0.003000    0.439140   \n 269499     1050       5865  0.003243  0.000045   0.340472   18.326600   \n 269500     1050       5887  0.010789  0.002366  21.107800    0.076290   \n 269501     1050       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 259734   0.202192          2         16.668800          0.442926   \n 259735   0.186941          7          1.547140          0.644196   \n 259736   0.367161         12          1.027290          0.223957   \n 259737   0.876026         14          1.951150          0.355134   \n 259738   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 269497   0.563012          8          4.999610          0.891314   \n 269498   0.403850          1        233.686000          0.214519   \n 269499   0.394949          6          6.671510          0.051653   \n 269500   0.241146         12         61.870400          0.537502   \n 269501   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 259734           0.435029  0.500000  0.197000     3903  \n 259735           0.558417  0.637844  0.274000     3903  \n 259736           0.909821  0.702344  0.192500     3903  \n 259737           0.076143  0.722725  0.216000     3903  \n 259738           0.874333  0.666148  0.231000     3903  \n ...                   ...       ...       ...      ...  \n 269497           0.876172  0.500000  6.562333     3903  \n 269498           0.663293  0.500000  5.214000     3903  \n 269499           0.785170  0.771712  4.772400     3903  \n 269500           0.805439  0.500000  5.314333     3903  \n 269501           0.270201  0.831250  5.009500     3903  \n \n [9768 rows x 14 columns],\n 3904:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 269502     1053          1  0.000097  0.002120   1.074280  156.026000   \n 269503     1053          1  0.000200  0.014477   0.001188    0.971001   \n 269504     1053          1  0.001281  0.000142   0.000156    0.012087   \n 269505     1053          1  0.001911  0.021976   0.009044    0.002017   \n 269506     1053          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 279593     1053       5656  0.018053  0.006614   0.009698  685.994000   \n 279594     1053       5845  0.131354  0.010094   0.003000    0.439140   \n 279595     1053       5865  0.003243  0.000045   0.340472   18.326600   \n 279596     1053       5887  0.010789  0.002366  21.107800    0.076290   \n 279597     1053       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 269502   0.202192          2         16.668800          0.442926   \n 269503   0.186941          7          1.547140          0.644196   \n 269504   0.367161         12          1.027290          0.223957   \n 269505   0.876026         14          1.951150          0.355134   \n 269506   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 279593   0.563012          8          4.999610          0.891314   \n 279594   0.403850          1        233.686000          0.214519   \n 279595   0.394949          6          6.671510          0.051653   \n 279596   0.241146         12         61.870400          0.537502   \n 279597   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 269502           0.435029  0.591670   0.297000     3904  \n 269503           0.558417  0.670180   0.294667     3904  \n 269504           0.909821  0.630703   0.349250     3904  \n 269505           0.076143  0.679875   0.460500     3904  \n 269506           0.874333  0.687110   0.339167     3904  \n ...                   ...       ...        ...      ...  \n 279593           0.876172  0.662261  33.118000     3904  \n 279594           0.663293  0.705393  21.147500     3904  \n 279595           0.785170  0.705007  24.230214     3904  \n 279596           0.805439  0.740937  42.551333     3904  \n 279597           0.270201  0.727829  21.965714     3904  \n \n [10096 rows x 14 columns],\n 3917:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 279598     1067          1  0.000097  0.002120   1.074280  156.026000   \n 279599     1067          1  0.000200  0.014477   0.001188    0.971001   \n 279600     1067          1  0.001281  0.000142   0.000156    0.012087   \n 279601     1067          1  0.001911  0.021976   0.009044    0.002017   \n 279602     1067          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 289286     1067       5656  0.018053  0.006614   0.009698  685.994000   \n 289287     1067       5845  0.131354  0.010094   0.003000    0.439140   \n 289288     1067       5865  0.003243  0.000045   0.340472   18.326600   \n 289289     1067       5887  0.010789  0.002366  21.107800    0.076290   \n 289290     1067       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 279598   0.202192          2         16.668800          0.442926   \n 279599   0.186941          7          1.547140          0.644196   \n 279600   0.367161         12          1.027290          0.223957   \n 279601   0.876026         14          1.951150          0.355134   \n 279602   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 289286   0.563012          8          4.999610          0.891314   \n 289287   0.403850          1        233.686000          0.214519   \n 289288   0.394949          6          6.671510          0.051653   \n 289289   0.241146         12         61.870400          0.537502   \n 289290   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 279598           0.435029  0.500000   0.188167     3917  \n 279599           0.558417  0.687351   0.201000     3917  \n 279600           0.909821  0.633265   0.243500     3917  \n 279601           0.076143  0.793029   0.186500     3917  \n 279602           0.874333  0.767533   0.180400     3917  \n ...                   ...       ...        ...      ...  \n 289286           0.876172  0.500000   6.167000     3917  \n 289287           0.663293  0.500000  10.426375     3917  \n 289288           0.785170  0.801549  14.177400     3917  \n 289289           0.805439  0.500000   5.505800     3917  \n 289290           0.270201  0.800609  22.245667     3917  \n \n [9693 rows x 14 columns],\n 3918:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 289291     1068          1  0.000097  0.002120   1.074280  156.026000   \n 289292     1068          1  0.000200  0.014477   0.001188    0.971001   \n 289293     1068          1  0.001281  0.000142   0.000156    0.012087   \n 289294     1068          1  0.001911  0.021976   0.009044    0.002017   \n 289295     1068          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 298535     1068       5656  0.018053  0.006614   0.009698  685.994000   \n 298536     1068       5845  0.131354  0.010094   0.003000    0.439140   \n 298537     1068       5865  0.003243  0.000045   0.340472   18.326600   \n 298538     1068       5887  0.010789  0.002366  21.107800    0.076290   \n 298539     1068       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 289291   0.202192          2         16.668800          0.442926   \n 289292   0.186941          7          1.547140          0.644196   \n 289293   0.367161         12          1.027290          0.223957   \n 289294   0.876026         14          1.951150          0.355134   \n 289295   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 298535   0.563012          8          4.999610          0.891314   \n 298536   0.403850          1        233.686000          0.214519   \n 298537   0.394949          6          6.671510          0.051653   \n 298538   0.241146         12         61.870400          0.537502   \n 298539   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 289291           0.435029  0.500000  0.173375     3918  \n 289292           0.558417  0.631408  0.249333     3918  \n 289293           0.909821  0.588289  0.177000     3918  \n 289294           0.076143  0.372573  0.184000     3918  \n 289295           0.874333  0.568221  0.207250     3918  \n ...                   ...       ...       ...      ...  \n 298535           0.876172  0.500000  4.098333     3918  \n 298536           0.663293  0.500000  3.588500     3918  \n 298537           0.785170  0.727840  3.662273     3918  \n 298538           0.805439  0.500000  3.992500     3918  \n 298539           0.270201  0.834725  3.920500     3918  \n \n [9249 rows x 14 columns],\n 3945:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 298540     1111          1  0.000200  0.014477    0.001188    0.971001   \n 298541     1111          1  0.002022  0.000058    0.258957    1.867090   \n 298542     1111          1  0.005065  1.015660    0.086148    0.338362   \n 298543     1111          1  0.005310  0.009738    0.018482    0.008865   \n 298544     1111          1  0.006275  0.007421    0.047000    5.249080   \n ...         ...        ...       ...       ...         ...         ...   \n 303427     1111       3520  0.168119  0.005091    0.001530    4.427620   \n 303428     1111       4127  0.669168  0.002332   87.110400    0.000502   \n 303429     1111       4180  0.504788  0.000805  416.362000  690.390000   \n 303431     1111       4361  0.831121  0.000628    0.068076   28.107900   \n 303435     1111       4738  0.006621  0.000135    0.000112    0.013867   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 298540   0.186941          7           1.54714          0.644196   \n 298541   0.354213          4           2.44719          0.239795   \n 298542   0.667977         12          24.07030          0.907258   \n 298543   0.521716          9           3.37821          0.616922   \n 298544   0.959774         15           2.04807          0.790151   \n ...           ...        ...               ...               ...   \n 303427   0.755557          7          12.01180          0.256184   \n 303428   0.102350         15          51.71450          0.323692   \n 303429   0.584915          4           2.18529          0.118481   \n 303431   0.722156         11           1.19589          0.044284   \n 303435   0.414460          7           3.69541          0.031932   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 298540           0.558417  0.530032   17.761889     3945  \n 298541           0.874333  0.499919   12.268800     3945  \n 298542           0.637908  0.758712   14.649500     3945  \n 298543           0.434134  0.693902   21.770000     3945  \n 298544           0.896264  0.621794   11.867667     3945  \n ...                   ...       ...         ...      ...  \n 303427           0.046136  0.759464  471.625571     3945  \n 303428           0.122990  0.565457  469.657500     3945  \n 303429           0.510327  0.669359  548.887500     3945  \n 303431           0.472366  0.746251  528.984000     3945  \n 303435           0.166793  0.780329  594.261600     3945  \n \n [4620 rows x 14 columns],\n 10090:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 303440     1457          1  0.024234  0.000067    1.000500  768.603000   \n 303441     1457          1  0.042303  0.029853  158.783000    0.001131   \n 303442     1457          1  0.049104  0.353042    0.000065    0.027377   \n 303443     1457          1  0.094189  0.000669  183.615000    0.017106   \n 303444     1457          1  0.099818  0.028129    0.001607    0.001102   \n ...         ...        ...       ...       ...         ...         ...   \n 305244     1457        380  0.002776  4.231070    0.023549    2.040640   \n 305246     1457        385  0.012634  1.828760    0.010226    7.123260   \n 305247     1457        385  0.172272  0.017093    0.027665   20.573600   \n 305250     1457        391  0.358624  0.001855    0.017738    0.156887   \n 305265     1457        408  0.021462  0.004945    0.422638  239.028000   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 303440   0.641650          5           4.64789          0.561103   \n 303441   0.525185          1          17.03950          0.916042   \n 303442   0.959828          3           2.48812          0.180005   \n 303443   0.879396         10           3.90781          0.312301   \n 303444   0.821261         14           8.48476          0.245569   \n ...           ...        ...               ...               ...   \n 305244   0.174647          5          59.99580          0.096376   \n 305246   0.470994          2           2.35548          0.294581   \n 305247   0.311749         19           3.78129          0.115443   \n 305250   0.591047          9          25.74780          0.193782   \n 305265   0.354104          7          99.28090          0.452499   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 303440           0.167461  0.500000   18.572933    10090  \n 303441           0.085641  0.500000   17.278600    10090  \n 303442           0.161454  0.758136   20.713600    10090  \n 303443           0.710544  0.821472   26.939550    10090  \n 303444           0.451169  0.761832   23.275000    10090  \n ...                   ...       ...         ...      ...  \n 305244           0.334251  0.500000  569.065300    10090  \n 305246           0.022493  0.892100  554.336600    10090  \n 305247           0.226419  0.500000  586.978100    10090  \n 305250           0.031961  0.500000  557.823300    10090  \n 305265           0.022317  0.500000  595.749000    10090  \n \n [1553 rows x 14 columns],\n 14965:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 305469     1461          1  0.000200  0.014477  0.001188    0.971001   \n 305470     1461          1  0.001911  0.021976  0.009044    0.002017   \n 305471     1461          1  0.002022  0.000058  0.258957    1.867090   \n 305472     1461          1  0.005065  1.015660  0.086148    0.338362   \n 305473     1461          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 312432     1461       5646  0.711899  0.000108  2.355870    0.039822   \n 312433     1461       5656  0.018053  0.006614  0.009698  685.994000   \n 312434     1461       5845  0.131354  0.010094  0.003000    0.439140   \n 312435     1461       5865  0.003243  0.000045  0.340472   18.326600   \n 312436     1461       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 305469   0.186941          7          1.547140          0.644196   \n 305470   0.876026         14          1.951150          0.355134   \n 305471   0.354213          4          2.447190          0.239795   \n 305472   0.667977         12         24.070300          0.907258   \n 305473   0.521716          9          3.378210          0.616922   \n ...           ...        ...               ...               ...   \n 312432   0.218562         11         14.791100          0.133948   \n 312433   0.563012          8          4.999610          0.891314   \n 312434   0.403850          1        233.686000          0.214519   \n 312435   0.394949          6          6.671510          0.051653   \n 312436   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 305469           0.558417  0.729199    0.66500    14965  \n 305470           0.076143  0.594358    0.86880    14965  \n 305471           0.874333  0.735493    0.67100    14965  \n 305472           0.637908  0.901220    0.98350    14965  \n 305473           0.434134  0.861741    0.90200    14965  \n ...                   ...       ...        ...      ...  \n 312432           0.888355  0.869115  141.36420    14965  \n 312433           0.876172  0.884678  187.69100    14965  \n 312434           0.663293  0.883554  103.80300    14965  \n 312435           0.785170  0.894543   88.25375    14965  \n 312436           0.270201  0.914180   94.62200    14965  \n \n [6966 rows x 14 columns],\n 10093:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 312437     1462          1  0.000097  0.002120  1.074280  156.026000   \n 312438     1462          1  0.000200  0.014477  0.001188    0.971001   \n 312439     1462          1  0.001281  0.000142  0.000156    0.012087   \n 312440     1462          1  0.001911  0.021976  0.009044    0.002017   \n 312441     1462          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 321329     1462       5646  0.711899  0.000108  2.355870    0.039822   \n 321330     1462       5656  0.018053  0.006614  0.009698  685.994000   \n 321331     1462       5845  0.131354  0.010094  0.003000    0.439140   \n 321332     1462       5865  0.003243  0.000045  0.340472   18.326600   \n 321333     1462       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 312437   0.202192          2         16.668800          0.442926   \n 312438   0.186941          7          1.547140          0.644196   \n 312439   0.367161         12          1.027290          0.223957   \n 312440   0.876026         14          1.951150          0.355134   \n 312441   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 321329   0.218562         11         14.791100          0.133948   \n 321330   0.563012          8          4.999610          0.891314   \n 321331   0.403850          1        233.686000          0.214519   \n 321332   0.394949          6          6.671510          0.051653   \n 321333   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 312437           0.435029  0.500000  0.259750    10093  \n 312438           0.558417  0.873167  0.145500    10093  \n 312439           0.909821  0.619607  0.144000    10093  \n 312440           0.076143  0.892047  0.204500    10093  \n 312441           0.874333  0.600240  0.156714    10093  \n ...                   ...       ...       ...      ...  \n 321329           0.888355  0.971259  3.278333    10093  \n 321330           0.876172  0.500000  3.510000    10093  \n 321331           0.663293  0.500000  3.444000    10093  \n 321332           0.785170  0.966280  5.977308    10093  \n 321333           0.270201  0.997052  3.622667    10093  \n \n [8897 rows x 14 columns],\n 10101:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 321334     1464          1  0.000097  0.002120  1.074280  156.026000   \n 321335     1464          1  0.000200  0.014477  0.001188    0.971001   \n 321336     1464          1  0.001281  0.000142  0.000156    0.012087   \n 321337     1464          1  0.002022  0.000058  0.258957    1.867090   \n 321338     1464          1  0.005065  1.015660  0.086148    0.338362   \n ...         ...        ...       ...       ...       ...         ...   \n 329943     1464       5646  0.711899  0.000108  2.355870    0.039822   \n 329944     1464       5656  0.018053  0.006614  0.009698  685.994000   \n 329945     1464       5845  0.131354  0.010094  0.003000    0.439140   \n 329946     1464       5865  0.003243  0.000045  0.340472   18.326600   \n 329947     1464       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 321334   0.202192          2         16.668800          0.442926   \n 321335   0.186941          7          1.547140          0.644196   \n 321336   0.367161         12          1.027290          0.223957   \n 321337   0.354213          4          2.447190          0.239795   \n 321338   0.667977         12         24.070300          0.907258   \n ...           ...        ...               ...               ...   \n 329943   0.218562         11         14.791100          0.133948   \n 329944   0.563012          8          4.999610          0.891314   \n 329945   0.403850          1        233.686000          0.214519   \n 329946   0.394949          6          6.671510          0.051653   \n 329947   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 321334           0.435029  0.500000  0.176600    10101  \n 321335           0.558417  0.482456  0.184500    10101  \n 321336           0.909821  0.614035  0.146000    10101  \n 321337           0.874333  0.602994  0.160286    10101  \n 321338           0.637908  0.642057  0.137500    10101  \n ...                   ...       ...       ...      ...  \n 329943           0.888355  0.773535  2.687000    10101  \n 329944           0.876172  0.500000  2.611000    10101  \n 329945           0.663293  0.500000  2.646500    10101  \n 329946           0.785170  0.740706  2.892600    10101  \n 329947           0.270201  0.788255  2.893500    10101  \n \n [8614 rows x 14 columns],\n 9981:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 329948     1468          1  0.000200  0.014477  0.001188    0.971001   \n 329949     1468          1  0.001911  0.021976  0.009044    0.002017   \n 329950     1468          1  0.002022  0.000058  0.258957    1.867090   \n 329951     1468          1  0.005065  1.015660  0.086148    0.338362   \n 329952     1468          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 337041     1468       5646  0.711899  0.000108  2.355870    0.039822   \n 337042     1468       5656  0.018053  0.006614  0.009698  685.994000   \n 337043     1468       5845  0.131354  0.010094  0.003000    0.439140   \n 337044     1468       5865  0.003243  0.000045  0.340472   18.326600   \n 337045     1468       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 329948   0.186941          7          1.547140          0.644196   \n 329949   0.876026         14          1.951150          0.355134   \n 329950   0.354213          4          2.447190          0.239795   \n 329951   0.667977         12         24.070300          0.907258   \n 329952   0.521716          9          3.378210          0.616922   \n ...           ...        ...               ...               ...   \n 337041   0.218562         11         14.791100          0.133948   \n 337042   0.563012          8          4.999610          0.891314   \n 337043   0.403850          1        233.686000          0.214519   \n 337044   0.394949          6          6.671510          0.051653   \n 337045   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 329948           0.558417  0.875096    1.112167     9981  \n 329949           0.076143  0.804615    1.174167     9981  \n 329950           0.874333  0.783914    1.133500     9981  \n 329951           0.637908  0.664969    1.204200     9981  \n 329952           0.434134  0.914834    1.247500     9981  \n ...                   ...       ...         ...      ...  \n 337041           0.888355  0.673942  129.187286     9981  \n 337042           0.876172  0.500000  304.804143     9981  \n 337043           0.663293  0.500000  138.452750     9981  \n 337044           0.785170  0.930498  115.610000     9981  \n 337045           0.270201  0.987823  158.611500     9981  \n \n [7096 rows x 14 columns],\n 9985:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 337046     1475          1  0.000200  0.014477    0.001188    0.971001   \n 337047     1475          1  0.001911  0.021976    0.009044    0.002017   \n 337048     1475          1  0.002022  0.000058    0.258957    1.867090   \n 337049     1475          1  0.005065  1.015660    0.086148    0.338362   \n 337050     1475          1  0.005310  0.009738    0.018482    0.008865   \n ...         ...        ...       ...       ...         ...         ...   \n 343267     1475       5626  0.003521  0.008594  169.189000    0.181859   \n 343268     1475       5646  0.711899  0.000108    2.355870    0.039822   \n 343269     1475       5656  0.018053  0.006614    0.009698  685.994000   \n 343270     1475       5845  0.131354  0.010094    0.003000    0.439140   \n 343271     1475       5865  0.003243  0.000045    0.340472   18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 337046   0.186941          7           1.54714          0.644196   \n 337047   0.876026         14           1.95115          0.355134   \n 337048   0.354213          4           2.44719          0.239795   \n 337049   0.667977         12          24.07030          0.907258   \n 337050   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 343267   0.701476          5          57.84400          0.762448   \n 343268   0.218562         11          14.79110          0.133948   \n 343269   0.563012          8           4.99961          0.891314   \n 343270   0.403850          1         233.68600          0.214519   \n 343271   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 337046           0.558417  0.716444    0.443400     9985  \n 337047           0.076143  0.767513    0.380000     9985  \n 337048           0.874333  0.712730    0.369556     9985  \n 337049           0.637908  0.766601    0.480750     9985  \n 337050           0.434134  0.756697    0.433167     9985  \n ...                   ...       ...         ...      ...  \n 343267           0.675186  0.825457  357.181667     9985  \n 343268           0.888355  0.818968   92.254500     9985  \n 343269           0.876172  0.626898  154.737556     9985  \n 343270           0.663293  0.777965   71.922250     9985  \n 343271           0.785170  0.755366   81.203250     9985  \n \n [6226 rows x 14 columns],\n 9986:         data_id  num_round       eta     gamma        lambda       alpha  \\\n 343272     1476          1  0.000200  0.014477      0.001188    0.971001   \n 343273     1476          1  0.002022  0.000058      0.258957    1.867090   \n 343274     1476          1  0.006275  0.007421      0.047000    5.249080   \n 343275     1476          1  0.006948  0.000345  18183.000000    0.034853   \n 343276     1476          1  0.024234  0.000067      1.000500  768.603000   \n ...         ...        ...       ...       ...           ...         ...   \n 347254     1476       4658  0.075037  0.000673      0.007150  601.580000   \n 347255     1476       4680  0.003782  0.959678      0.003538    0.003371   \n 347256     1476       4734  0.003478  0.003172     45.664900  157.418000   \n 347257     1476       4927  0.027808  0.000358      0.225318    0.008919   \n 347258     1476       5538  0.046476  0.955996      0.001383    0.716637   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 343272   0.186941          7           1.54714          0.644196   \n 343273   0.354213          4           2.44719          0.239795   \n 343274   0.959774         15           2.04807          0.790151   \n 343275   0.194973          1         115.84000          0.506227   \n 343276   0.641650          5           4.64789          0.561103   \n ...           ...        ...               ...               ...   \n 347254   0.430539          5          10.52920          0.788015   \n 347255   0.229583          4           7.80861          0.685374   \n 347256   0.287994         12           6.58579          0.674964   \n 347257   0.164003          9           2.55505          0.709183   \n 347258   0.766008          3          75.79260          0.662224   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 343272           0.558417  0.985332    1.467154     9986  \n 343273           0.874333  0.971227    1.252231     9986  \n 343274           0.896264  0.995656    2.311333     9986  \n 343275           0.956662  0.850776    1.136000     9986  \n 343276           0.167461  0.777768    1.523727     9986  \n ...                   ...       ...         ...      ...  \n 347254           0.523482  0.865405  434.857842     9986  \n 347255           0.038369  0.999099  239.928625     9986  \n 347256           0.537067  0.976073  582.038000     9986  \n 347257           0.786188  0.999582  567.335083     9986  \n 347258           0.121894  0.998872  305.887000     9986  \n \n [3962 rows x 14 columns],\n 9970:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 347259     1479          1  0.000097  0.002120   1.074280  156.026000   \n 347260     1479          1  0.000200  0.014477   0.001188    0.971001   \n 347261     1479          1  0.001281  0.000142   0.000156    0.012087   \n 347262     1479          1  0.001911  0.021976   0.009044    0.002017   \n 347263     1479          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 357184     1479       5656  0.018053  0.006614   0.009698  685.994000   \n 357185     1479       5845  0.131354  0.010094   0.003000    0.439140   \n 357186     1479       5865  0.003243  0.000045   0.340472   18.326600   \n 357187     1479       5887  0.010789  0.002366  21.107800    0.076290   \n 357188     1479       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 347259   0.202192          2         16.668800          0.442926   \n 347260   0.186941          7          1.547140          0.644196   \n 347261   0.367161         12          1.027290          0.223957   \n 347262   0.876026         14          1.951150          0.355134   \n 347263   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 357184   0.563012          8          4.999610          0.891314   \n 357185   0.403850          1        233.686000          0.214519   \n 357186   0.394949          6          6.671510          0.051653   \n 357187   0.241146         12         61.870400          0.537502   \n 357188   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 347259           0.435029  0.500000   0.298571     9970  \n 347260           0.558417  0.450000   0.288000     9970  \n 347261           0.909821  0.532472   0.330750     9970  \n 347262           0.076143  0.482397   0.352000     9970  \n 347263           0.874333  0.523647   0.335125     9970  \n ...                   ...       ...        ...      ...  \n 357184           0.876172  0.500000  18.559500     9970  \n 357185           0.663293  0.500000   6.052800     9970  \n 357186           0.785170  0.541847   5.862000     9970  \n 357187           0.805439  0.500000   6.912000     9970  \n 357188           0.270201  0.532041   7.749400     9970  \n \n [9930 rows x 14 columns],\n 9976:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 357189     1485          1  0.000097  0.002120   1.074280  156.026000   \n 357190     1485          1  0.000200  0.014477   0.001188    0.971001   \n 357191     1485          1  0.001046  0.000033   0.001359  372.626000   \n 357192     1485          1  0.001281  0.000142   0.000156    0.012087   \n 357193     1485          1  0.001911  0.021976   0.009044    0.002017   \n ...         ...        ...       ...       ...        ...         ...   \n 367249     1485       5656  0.018053  0.006614   0.009698  685.994000   \n 367250     1485       5845  0.131354  0.010094   0.003000    0.439140   \n 367251     1485       5865  0.003243  0.000045   0.340472   18.326600   \n 367252     1485       5887  0.010789  0.002366  21.107800    0.076290   \n 367253     1485       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 357189   0.202192          2         16.668800          0.442926   \n 357190   0.186941          7          1.547140          0.644196   \n 357191   0.591609         15          1.099660          0.641624   \n 357192   0.367161         12          1.027290          0.223957   \n 357193   0.876026         14          1.951150          0.355134   \n ...           ...        ...               ...               ...   \n 367249   0.563012          8          4.999610          0.891314   \n 367250   0.403850          1        233.686000          0.214519   \n 367251   0.394949          6          6.671510          0.051653   \n 367252   0.241146         12         61.870400          0.537502   \n 367253   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 357189           0.435029  0.500000   1.070800     9976  \n 357190           0.558417  0.731065   1.131000     9976  \n 357191           0.207744  0.500000   1.076000     9976  \n 357192           0.909821  0.641612   1.063500     9976  \n 357193           0.076143  0.544630   1.103000     9976  \n ...                   ...       ...        ...      ...  \n 367249           0.876172  0.500000  76.518500     9976  \n 367250           0.663293  0.500000  29.676000     9976  \n 367251           0.785170  0.715489  40.076846     9976  \n 367252           0.805439  0.616272  67.381857     9976  \n 367253           0.270201  0.686686  34.267000     9976  \n \n [10064 rows x 14 columns],\n 9977:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 367254     1486          1  0.000200  0.014477  0.001188    0.971001   \n 367255     1486          1  0.001911  0.021976  0.009044    0.002017   \n 367256     1486          1  0.002022  0.000058  0.258957    1.867090   \n 367257     1486          1  0.005065  1.015660  0.086148    0.338362   \n 367258     1486          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 374123     1486       5581  0.081455  1.323940  3.207280  721.920000   \n 374125     1486       5646  0.711899  0.000108  2.355870    0.039822   \n 374126     1486       5656  0.018053  0.006614  0.009698  685.994000   \n 374127     1486       5845  0.131354  0.010094  0.003000    0.439140   \n 374128     1486       5865  0.003243  0.000045  0.340472   18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 367254   0.186941          7           1.54714          0.644196   \n 367255   0.876026         14           1.95115          0.355134   \n 367256   0.354213          4           2.44719          0.239795   \n 367257   0.667977         12          24.07030          0.907258   \n 367258   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 374123   0.764634         10         112.93100          0.033118   \n 374125   0.218562         11          14.79110          0.133948   \n 374126   0.563012          8           4.99961          0.891314   \n 374127   0.403850          1         233.68600          0.214519   \n 374128   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 367254           0.558417  0.969995    8.427700     9977  \n 367255           0.076143  0.956403    1.976833     9977  \n 367256           0.874333  0.946304    2.119857     9977  \n 367257           0.637908  0.979677    2.109000     9977  \n 367258           0.434134  0.978867    2.119500     9977  \n ...                   ...       ...         ...      ...  \n 374123           0.470175  0.974742  112.421600     9977  \n 374125           0.888355  0.990177  213.481333     9977  \n 374126           0.876172  0.978523  403.283500     9977  \n 374127           0.663293  0.984752  163.250800     9977  \n 374128           0.785170  0.989276  199.538000     9977  \n \n [6868 rows x 14 columns],\n 9978:         data_id  num_round       eta      gamma      lambda       alpha  \\\n 374129     1487          1  0.000097   0.002120    1.074280  156.026000   \n 374130     1487          1  0.000200   0.014477    0.001188    0.971001   \n 374131     1487          1  0.001046   0.000033    0.001359  372.626000   \n 374132     1487          1  0.001281   0.000142    0.000156    0.012087   \n 374133     1487          1  0.001513  29.504900  463.819000   22.707600   \n ...         ...        ...       ...        ...         ...         ...   \n 384725     1487       5749  0.028359   0.000032    0.018361    0.007791   \n 384726     1487       5845  0.131354   0.010094    0.003000    0.439140   \n 384727     1487       5865  0.003243   0.000045    0.340472   18.326600   \n 384728     1487       5887  0.010789   0.002366   21.107800    0.076290   \n 384729     1487       5917  0.081559   4.644440    0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 374129   0.202192          2         16.668800          0.442926   \n 374130   0.186941          7          1.547140          0.644196   \n 374131   0.591609         15          1.099660          0.641624   \n 374132   0.367161         12          1.027290          0.223957   \n 374133   0.634888          8         15.484400          0.670356   \n ...           ...        ...               ...               ...   \n 384725   0.569283         13          2.053570          0.332356   \n 384726   0.403850          1        233.686000          0.214519   \n 384727   0.394949          6          6.671510          0.051653   \n 384728   0.241146         12         61.870400          0.537502   \n 384729   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 374129           0.435029  0.500000   0.310000     9978  \n 374130           0.558417  0.773075   0.266000     9978  \n 374131           0.207744  0.551534   0.338250     9978  \n 374132           0.909821  0.758240   0.288000     9978  \n 374133           0.689271  0.500000   0.415125     9978  \n ...                   ...       ...        ...      ...  \n 384725           0.825170  0.874212  16.552000     9978  \n 384726           0.663293  0.500000   9.662333     9978  \n 384727           0.785170  0.853729   7.783625     9978  \n 384728           0.805439  0.500000  12.757500     9978  \n 384729           0.270201  0.890793   8.819500     9978  \n \n [10601 rows x 14 columns],\n 9956:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 384730     1493          1  0.024234  0.000067    1.000500  768.603000   \n 384731     1493          1  0.042303  0.029853  158.783000    0.001131   \n 384732     1493          1  0.049104  0.353042    0.000065    0.027377   \n 384733     1493          1  0.094189  0.000669  183.615000    0.017106   \n 384734     1493          1  0.099818  0.028129    0.001607    0.001102   \n ...         ...        ...       ...       ...         ...         ...   \n 387314     1493       4127  0.669168  0.002332   87.110400    0.000502   \n 387315     1493       4180  0.504788  0.000805  416.362000  690.390000   \n 387316     1493       4658  0.075037  0.000673    0.007150  601.580000   \n 387317     1493       4680  0.003782  0.959678    0.003538    0.003371   \n 387318     1493       4734  0.003478  0.003172   45.664900  157.418000   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 384730   0.641650          5           4.64789          0.561103   \n 384731   0.525185          1          17.03950          0.916042   \n 384732   0.959828          3           2.48812          0.180005   \n 384733   0.879396         10           3.90781          0.312301   \n 384734   0.821261         14           8.48476          0.245569   \n ...           ...        ...               ...               ...   \n 387314   0.102350         15          51.71450          0.323692   \n 387315   0.584915          4           2.18529          0.118481   \n 387316   0.430539          5          10.52920          0.788015   \n 387317   0.229583          4           7.80861          0.685374   \n 387318   0.287994         12           6.58579          0.674964   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 384730           0.167461  0.500000    0.360700     9956  \n 384731           0.085641  0.500000    0.488500     9956  \n 384732           0.161454  0.829050    0.361800     9956  \n 384733           0.710544  0.867018    0.364600     9956  \n 384734           0.451169  0.797582    0.350200     9956  \n ...                   ...       ...         ...      ...  \n 387314           0.122990  0.500000  223.466700     9956  \n 387315           0.510327  0.500000  236.904000     9956  \n 387316           0.523482  0.500000  412.800733     9956  \n 387317           0.038369  0.500000  261.705100     9956  \n 387318           0.537067  0.500000  369.641000     9956  \n \n [2589 rows x 14 columns],\n 9957:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 387319     1494          1  0.000097  0.002120  1.074280  156.026000   \n 387320     1494          1  0.000200  0.014477  0.001188    0.971001   \n 387321     1494          1  0.001281  0.000142  0.000156    0.012087   \n 387322     1494          1  0.001911  0.021976  0.009044    0.002017   \n 387323     1494          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 396418     1494       5646  0.711899  0.000108  2.355870    0.039822   \n 396419     1494       5656  0.018053  0.006614  0.009698  685.994000   \n 396420     1494       5845  0.131354  0.010094  0.003000    0.439140   \n 396421     1494       5865  0.003243  0.000045  0.340472   18.326600   \n 396422     1494       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 387319   0.202192          2         16.668800          0.442926   \n 387320   0.186941          7          1.547140          0.644196   \n 387321   0.367161         12          1.027290          0.223957   \n 387322   0.876026         14          1.951150          0.355134   \n 387323   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 396418   0.218562         11         14.791100          0.133948   \n 396419   0.563012          8          4.999610          0.891314   \n 396420   0.403850          1        233.686000          0.214519   \n 396421   0.394949          6          6.671510          0.051653   \n 396422   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 387319           0.435029  0.500000  0.248000     9957  \n 387320           0.558417  0.820859  0.206000     9957  \n 387321           0.909821  0.817443  0.212750     9957  \n 387322           0.076143  0.852858  0.204000     9957  \n 387323           0.874333  0.761046  0.231000     9957  \n ...                   ...       ...       ...      ...  \n 396418           0.888355  0.848163  9.115600     9957  \n 396419           0.876172  0.500000  5.611000     9957  \n 396420           0.663293  0.500000  3.849000     9957  \n 396421           0.785170  0.877067  4.072625     9957  \n 396422           0.270201  0.920998  5.778500     9957  \n \n [9104 rows x 14 columns],\n 9960:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 396423     1497          1  0.000097  0.002120  1.074280  156.026000   \n 396424     1497          1  0.000200  0.014477  0.001188    0.971001   \n 396425     1497          1  0.001281  0.000142  0.000156    0.012087   \n 396426     1497          1  0.001911  0.021976  0.009044    0.002017   \n 396427     1497          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 405307     1497       5646  0.711899  0.000108  2.355870    0.039822   \n 405308     1497       5656  0.018053  0.006614  0.009698  685.994000   \n 405309     1497       5845  0.131354  0.010094  0.003000    0.439140   \n 405310     1497       5865  0.003243  0.000045  0.340472   18.326600   \n 405311     1497       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 396423   0.202192          2         16.668800          0.442926   \n 396424   0.186941          7          1.547140          0.644196   \n 396425   0.367161         12          1.027290          0.223957   \n 396426   0.876026         14          1.951150          0.355134   \n 396427   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 405307   0.218562         11         14.791100          0.133948   \n 405308   0.563012          8          4.999610          0.891314   \n 405309   0.403850          1        233.686000          0.214519   \n 405310   0.394949          6          6.671510          0.051653   \n 405311   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 396423           0.435029  0.781924   0.255667     9960  \n 396424           0.558417  0.994102   0.353000     9960  \n 396425           0.909821  0.952655   0.283333     9960  \n 396426           0.076143  0.967635   0.257200     9960  \n 396427           0.874333  0.892537   0.234250     9960  \n ...                   ...       ...        ...      ...  \n 405307           0.888355  0.998940  28.761500     9960  \n 405308           0.876172  0.909561  56.005500     9960  \n 405309           0.663293  0.968200  39.309500     9960  \n 405310           0.785170  0.991572  42.321545     9960  \n 405311           0.270201  0.999620  35.393000     9960  \n \n [8889 rows x 14 columns],\n 9964:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 405312     1501          1  0.000200  0.014477  0.001188    0.971001   \n 405313     1501          1  0.001911  0.021976  0.009044    0.002017   \n 405314     1501          1  0.002022  0.000058  0.258957    1.867090   \n 405315     1501          1  0.005065  1.015660  0.086148    0.338362   \n 405316     1501          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 412554     1501       5646  0.711899  0.000108  2.355870    0.039822   \n 412555     1501       5656  0.018053  0.006614  0.009698  685.994000   \n 412556     1501       5845  0.131354  0.010094  0.003000    0.439140   \n 412557     1501       5865  0.003243  0.000045  0.340472   18.326600   \n 412558     1501       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 405312   0.186941          7          1.547140          0.644196   \n 405313   0.876026         14          1.951150          0.355134   \n 405314   0.354213          4          2.447190          0.239795   \n 405315   0.667977         12         24.070300          0.907258   \n 405316   0.521716          9          3.378210          0.616922   \n ...           ...        ...               ...               ...   \n 412554   0.218562         11         14.791100          0.133948   \n 412555   0.563012          8          4.999610          0.891314   \n 412556   0.403850          1        233.686000          0.214519   \n 412557   0.394949          6          6.671510          0.051653   \n 412558   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 405312           0.558417  0.890659    0.534250     9964  \n 405313           0.076143  0.920994    0.498667     9964  \n 405314           0.874333  0.892281    0.532222     9964  \n 405315           0.637908  0.876019    0.605250     9964  \n 405316           0.434134  0.928242    0.589444     9964  \n ...                   ...       ...         ...      ...  \n 412554           0.888355  0.979224   78.856417     9964  \n 412555           0.876172  0.500000  213.329000     9964  \n 412556           0.663293  0.500000   89.661250     9964  \n 412557           0.785170  0.979520   84.669750     9964  \n 412558           0.270201  0.988804   91.866000     9964  \n \n [7246 rows x 14 columns],\n 9946:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 412559     1510          1  0.000097  0.002120  1.074280  156.026000   \n 412560     1510          1  0.000200  0.014477  0.001188    0.971001   \n 412561     1510          1  0.001281  0.000142  0.000156    0.012087   \n 412562     1510          1  0.001911  0.021976  0.009044    0.002017   \n 412563     1510          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 421033     1510       5646  0.711899  0.000108  2.355870    0.039822   \n 421034     1510       5656  0.018053  0.006614  0.009698  685.994000   \n 421035     1510       5845  0.131354  0.010094  0.003000    0.439140   \n 421036     1510       5865  0.003243  0.000045  0.340472   18.326600   \n 421037     1510       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 412559   0.202192          2         16.668800          0.442926   \n 412560   0.186941          7          1.547140          0.644196   \n 412561   0.367161         12          1.027290          0.223957   \n 412562   0.876026         14          1.951150          0.355134   \n 412563   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 421033   0.218562         11         14.791100          0.133948   \n 421034   0.563012          8          4.999610          0.891314   \n 421035   0.403850          1        233.686000          0.214519   \n 421036   0.394949          6          6.671510          0.051653   \n 421037   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 412559           0.435029  0.500000  0.190000     9946  \n 412560           0.558417  0.959319  0.206750     9946  \n 412561           0.909821  0.948413  0.170000     9946  \n 412562           0.076143  0.902273  0.190000     9946  \n 412563           0.874333  0.933965  0.199875     9946  \n ...                   ...       ...       ...      ...  \n 421033           0.888355  0.826535  2.860500     9946  \n 421034           0.876172  0.500000  3.747000     9946  \n 421035           0.663293  0.500000  2.892000     9946  \n 421036           0.785170  0.983798  2.939700     9946  \n 421037           0.270201  0.987795  3.384250     9946  \n \n [8479 rows x 14 columns],\n 7592:         data_id  num_round       eta     gamma        lambda        alpha  \\\n 421038     1590          1  0.000200  0.014477      0.001188     0.971001   \n 421039     1590          1  0.002022  0.000058      0.258957     1.867090   \n 421040     1590          1  0.005065  1.015660      0.086148     0.338362   \n 421041     1590          1  0.006275  0.007421      0.047000     5.249080   \n 421042     1590          1  0.006948  0.000345  18183.000000     0.034853   \n ...         ...        ...       ...       ...           ...          ...   \n 425893     1590       5112  0.647139  0.000101      2.013690     0.132957   \n 425894     1590       5138  0.504453  0.000261      0.559442  4219.570000   \n 425895     1590       5538  0.046476  0.955996      0.001383     0.716637   \n 425896     1590       5845  0.131354  0.010094      0.003000     0.439140   \n 425897     1590       5865  0.003243  0.000045      0.340472    18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 421038   0.186941          7           1.54714          0.644196   \n 421039   0.354213          4           2.44719          0.239795   \n 421040   0.667977         12          24.07030          0.907258   \n 421041   0.959774         15           2.04807          0.790151   \n 421042   0.194973          1         115.84000          0.506227   \n ...           ...        ...               ...               ...   \n 425893   0.886760         10           5.68833          0.039330   \n 425894   0.981838          7          19.24390          0.824216   \n 425895   0.766008          3          75.79260          0.662224   \n 425896   0.403850          1         233.68600          0.214519   \n 425897   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 421038           0.558417  0.859325    1.544200     7592  \n 421039           0.874333  0.710008    1.007429     7592  \n 421040           0.637908  0.895545    1.388800     7592  \n 421041           0.896264  0.893015    1.449700     7592  \n 421042           0.956662  0.597122    1.025250     7592  \n ...                   ...       ...         ...      ...  \n 425893           0.805865  0.919623  120.357333     7592  \n 425894           0.596008  0.855184  161.579818     7592  \n 425895           0.121894  0.922579  165.549400     7592  \n 425896           0.663293  0.891528  147.493286     7592  \n 425897           0.785170  0.905720  149.692900     7592  \n \n [4857 rows x 14 columns],\n 9910:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 425898     4134          1  0.000097  0.002120  1.074280  156.026000   \n 425899     4134          1  0.000200  0.014477  0.001188    0.971001   \n 425900     4134          1  0.001911  0.021976  0.009044    0.002017   \n 425901     4134          1  0.002022  0.000058  0.258957    1.867090   \n 425902     4134          1  0.004626  0.931871  7.967300   22.020300   \n ...         ...        ...       ...       ...       ...         ...   \n 434564     4134       5646  0.711899  0.000108  2.355870    0.039822   \n 434565     4134       5656  0.018053  0.006614  0.009698  685.994000   \n 434566     4134       5845  0.131354  0.010094  0.003000    0.439140   \n 434567     4134       5865  0.003243  0.000045  0.340472   18.326600   \n 434568     4134       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 425898   0.202192          2         16.668800          0.442926   \n 425899   0.186941          7          1.547140          0.644196   \n 425900   0.876026         14          1.951150          0.355134   \n 425901   0.354213          4          2.447190          0.239795   \n 425902   0.101276         12          4.499670          0.679333   \n ...           ...        ...               ...               ...   \n 434564   0.218562         11         14.791100          0.133948   \n 434565   0.563012          8          4.999610          0.891314   \n 434566   0.403850          1        233.686000          0.214519   \n 434567   0.394949          6          6.671510          0.051653   \n 434568   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 425898           0.435029  0.500000    3.479167     9910  \n 425899           0.558417  0.749152    3.598833     9910  \n 425900           0.076143  0.741897    5.169333     9910  \n 425901           0.874333  0.707249    3.609500     9910  \n 425902           0.109048  0.609417    3.867000     9910  \n ...                   ...       ...         ...      ...  \n 434564           0.888355  0.776445  140.621125     9910  \n 434565           0.876172  0.500000  366.353250     9910  \n 434566           0.663293  0.500000  129.765000     9910  \n 434567           0.785170  0.841110  158.545833     9910  \n 434568           0.270201  0.831120  135.926667     9910  \n \n [8664 rows x 14 columns],\n 34539:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 434569     4135          1  0.002022  0.000058    0.258957    1.867090   \n 434570     4135          1  0.024234  0.000067    1.000500  768.603000   \n 434571     4135          1  0.042303  0.029853  158.783000    0.001131   \n 434572     4135          1  0.049104  0.353042    0.000065    0.027377   \n 434573     4135          1  0.094189  0.000669  183.615000    0.017106   \n ...         ...        ...       ...       ...         ...         ...   \n 437658     4135       4430  0.600513  0.000188    0.006966  935.943000   \n 437659     4135       4658  0.075037  0.000673    0.007150  601.580000   \n 437660     4135       4680  0.003782  0.959678    0.003538    0.003371   \n 437661     4135       4734  0.003478  0.003172   45.664900  157.418000   \n 437662     4135       5538  0.046476  0.955996    0.001383    0.716637   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 434569   0.354213          4           2.44719          0.239795   \n 434570   0.641650          5           4.64789          0.561103   \n 434571   0.525185          1          17.03950          0.916042   \n 434572   0.959828          3           2.48812          0.180005   \n 434573   0.879396         10           3.90781          0.312301   \n ...           ...        ...               ...               ...   \n 437658   0.434922          8           1.27593          0.240157   \n 437659   0.430539          5          10.52920          0.788015   \n 437660   0.229583          4           7.80861          0.685374   \n 437661   0.287994         12           6.58579          0.674964   \n 437662   0.766008          3          75.79260          0.662224   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 434569           0.874333  0.502121    1.467833    34539  \n 434570           0.167461  0.500090    3.598000    34539  \n 434571           0.085641  0.500000    1.528375    34539  \n 434572           0.161454  0.513926    8.904571    34539  \n 434573           0.710544  0.500249    2.588071    34539  \n ...                   ...       ...         ...      ...  \n 437658           0.395377  0.544477  144.917143    34539  \n 437659           0.523482  0.566870  266.057111    34539  \n 437660           0.038369  0.634015  130.361286    34539  \n 437661           0.537067  0.625681  364.734000    34539  \n 437662           0.121894  0.633005  205.347333    34539  \n \n [3089 rows x 14 columns],\n 14952:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 437663     4534          1  0.000097  0.002120   1.074280  156.026000   \n 437664     4534          1  0.000200  0.014477   0.001188    0.971001   \n 437665     4534          1  0.001281  0.000142   0.000156    0.012087   \n 437666     4534          1  0.001911  0.021976   0.009044    0.002017   \n 437667     4534          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 447114     4534       5656  0.018053  0.006614   0.009698  685.994000   \n 447115     4534       5845  0.131354  0.010094   0.003000    0.439140   \n 447116     4534       5865  0.003243  0.000045   0.340472   18.326600   \n 447117     4534       5887  0.010789  0.002366  21.107800    0.076290   \n 447118     4534       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 437663   0.202192          2         16.668800          0.442926   \n 437664   0.186941          7          1.547140          0.644196   \n 437665   0.367161         12          1.027290          0.223957   \n 437666   0.876026         14          1.951150          0.355134   \n 437667   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 447114   0.563012          8          4.999610          0.891314   \n 447115   0.403850          1        233.686000          0.214519   \n 447116   0.394949          6          6.671510          0.051653   \n 447117   0.241146         12         61.870400          0.537502   \n 447118   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 437663           0.435029  0.824915   0.402833    14952  \n 437664           0.558417  0.958576   0.356000    14952  \n 437665           0.909821  0.795459   0.367500    14952  \n 437666           0.076143  0.539058   0.370667    14952  \n 437667           0.874333  0.848221   0.391800    14952  \n ...                   ...       ...        ...      ...  \n 447114           0.876172  0.932740  84.974250    14952  \n 447115           0.663293  0.947712  33.772500    14952  \n 447116           0.785170  0.978752  28.575200    14952  \n 447117           0.805439  0.971092  59.361333    14952  \n 447118           0.270201  0.985479  35.729667    14952  \n \n [9456 rows x 14 columns],\n 14969:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 447119     4538          1  0.000200  0.014477  0.001188    0.971001   \n 447120     4538          1  0.001911  0.021976  0.009044    0.002017   \n 447121     4538          1  0.002022  0.000058  0.258957    1.867090   \n 447122     4538          1  0.005065  1.015660  0.086148    0.338362   \n 447123     4538          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 452673     4538       5538  0.046476  0.955996  0.001383    0.716637   \n 452674     4538       5646  0.711899  0.000108  2.355870    0.039822   \n 452675     4538       5656  0.018053  0.006614  0.009698  685.994000   \n 452676     4538       5845  0.131354  0.010094  0.003000    0.439140   \n 452677     4538       5865  0.003243  0.000045  0.340472   18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 447119   0.186941          7           1.54714          0.644196   \n 447120   0.876026         14           1.95115          0.355134   \n 447121   0.354213          4           2.44719          0.239795   \n 447122   0.667977         12          24.07030          0.907258   \n 447123   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 452673   0.766008          3          75.79260          0.662224   \n 452674   0.218562         11          14.79110          0.133948   \n 452675   0.563012          8           4.99961          0.891314   \n 452676   0.403850          1         233.68600          0.214519   \n 452677   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 447119           0.558417  0.728537    0.420750    14969  \n 447120           0.076143  0.721726    0.385750    14969  \n 447121           0.874333  0.730165    0.386000    14969  \n 447122           0.637908  0.781874    8.602000    14969  \n 447123           0.434134  0.751695    0.429571    14969  \n ...                   ...       ...         ...      ...  \n 452673           0.121894  0.866338  107.477000    14969  \n 452674           0.888355  0.815301  100.442364    14969  \n 452675           0.876172  0.709813  156.236000    14969  \n 452676           0.663293  0.796236   80.849000    14969  \n 452677           0.785170  0.781884   99.345250    14969  \n \n [5559 rows x 14 columns],\n 168339:         data_id  num_round       eta     gamma       lambda      alpha  \\\n 452678     4541          1  0.000200  0.014477     0.001188   0.971001   \n 452679     4541          1  0.001911  0.021976     0.009044   0.002017   \n 452680     4541          1  0.002022  0.000058     0.258957   1.867090   \n 452681     4541          1  0.005065  1.015660     0.086148   0.338362   \n 452682     4541          1  0.005310  0.009738     0.018482   0.008865   \n ...         ...        ...       ...       ...          ...        ...   \n 457644     4541       2042  0.010077  5.448790    17.985100   0.003473   \n 457647     4541       2097  0.017460  0.087025     0.003724   0.320840   \n 457648     4541       2107  0.167591  0.001877   551.975000   8.572160   \n 457653     4541       2629  0.001007  0.345503  3383.740000   2.488230   \n 457654     4541       2655  0.009224  0.000164     0.000008  19.739900   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 452678   0.186941          7           1.54714          0.644196   \n 452679   0.876026         14           1.95115          0.355134   \n 452680   0.354213          4           2.44719          0.239795   \n 452681   0.667977         12          24.07030          0.907258   \n 452682   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 457644   0.516276         14        4776.42000          0.611708   \n 457647   0.773870         12          51.37710          0.106106   \n 457648   0.130110         16          66.11220          0.113656   \n 457653   0.863821         14           2.51265          0.522335   \n 457654   0.589483          1           2.15243          0.027240   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 452678           0.558417  0.653158   15.069875   168339  \n 452679           0.076143  0.599621    4.999250   168339  \n 452680           0.874333  0.611339    5.376667   168339  \n 452681           0.637908  0.679363   10.008000   168339  \n 452682           0.434134  0.660995    8.179750   168339  \n ...                   ...       ...         ...      ...  \n 457644           0.029486  0.641552  447.370600   168339  \n 457647           0.053770  0.662419  436.875000   168339  \n 457648           0.420408  0.708652  595.931714   168339  \n 457653           0.015418  0.651039  563.603750   168339  \n 457654           0.720329  0.665196  583.946600   168339  \n \n [4380 rows x 14 columns],\n 125920:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 457670    23381          1  0.000097  0.002120  1.074280  156.026000   \n 457671    23381          1  0.000200  0.014477  0.001188    0.971001   \n 457672    23381          1  0.001281  0.000142  0.000156    0.012087   \n 457673    23381          1  0.001911  0.021976  0.009044    0.002017   \n 457674    23381          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 466385    23381       5646  0.711899  0.000108  2.355870    0.039822   \n 466386    23381       5656  0.018053  0.006614  0.009698  685.994000   \n 466387    23381       5845  0.131354  0.010094  0.003000    0.439140   \n 466388    23381       5865  0.003243  0.000045  0.340472   18.326600   \n 466389    23381       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 457670   0.202192          2         16.668800          0.442926   \n 457671   0.186941          7          1.547140          0.644196   \n 457672   0.367161         12          1.027290          0.223957   \n 457673   0.876026         14          1.951150          0.355134   \n 457674   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 466385   0.218562         11         14.791100          0.133948   \n 466386   0.563012          8          4.999610          0.891314   \n 466387   0.403850          1        233.686000          0.214519   \n 466388   0.394949          6          6.671510          0.051653   \n 466389   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 457670           0.435029  0.500000  0.182500   125920  \n 457671           0.558417  0.575534  0.300000   125920  \n 457672           0.909821  0.581554  0.212333   125920  \n 457673           0.076143  0.500000  0.264000   125920  \n 457674           0.874333  0.537767  0.228000   125920  \n ...                   ...       ...       ...      ...  \n 466385           0.888355  0.523946  5.797833   125920  \n 466386           0.876172  0.500000  5.849000   125920  \n 466387           0.663293  0.500000  3.984000   125920  \n 466388           0.785170  0.685413  3.556500   125920  \n 466389           0.270201  0.594828  4.159000   125920  \n \n [8720 rows x 14 columns],\n 146606:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 466390    23512          1  0.000200  0.014477  0.001188    0.971001   \n 466391    23512          1  0.001911  0.021976  0.009044    0.002017   \n 466392    23512          1  0.005065  1.015660  0.086148    0.338362   \n 466393    23512          1  0.005310  0.009738  0.018482    0.008865   \n 466394    23512          1  0.006275  0.007421  0.047000    5.249080   \n ...         ...        ...       ...       ...       ...         ...   \n 470899    23512       5538  0.046476  0.955996  0.001383    0.716637   \n 470900    23512       5581  0.081455  1.323940  3.207280  721.920000   \n 470901    23512       5646  0.711899  0.000108  2.355870    0.039822   \n 470902    23512       5656  0.018053  0.006614  0.009698  685.994000   \n 470903    23512       5845  0.131354  0.010094  0.003000    0.439140   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 466390   0.186941          7           1.54714          0.644196   \n 466391   0.876026         14           1.95115          0.355134   \n 466392   0.667977         12          24.07030          0.907258   \n 466393   0.521716          9           3.37821          0.616922   \n 466394   0.959774         15           2.04807          0.790151   \n ...           ...        ...               ...               ...   \n 470899   0.766008          3          75.79260          0.662224   \n 470900   0.764634         10         112.93100          0.033118   \n 470901   0.218562         11          14.79110          0.133948   \n 470902   0.563012          8           4.99961          0.891314   \n 470903   0.403850          1         233.68600          0.214519   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 466390           0.558417  0.667512    2.016667   146606  \n 466391           0.076143  0.532337    2.034800   146606  \n 466392           0.637908  0.743301    2.555867   146606  \n 466393           0.434134  0.659601    3.779400   146606  \n 466394           0.896264  0.723379    2.996444   146606  \n ...                   ...       ...         ...      ...  \n 470899           0.121894  0.795466  220.680364   146606  \n 470900           0.470175  0.733572  152.203000   146606  \n 470901           0.888355  0.669826  398.943824   146606  \n 470902           0.876172  0.761814  448.902375   146606  \n 470903           0.663293  0.767367  200.933875   146606  \n \n [4474 rows x 14 columns],\n 167120:         data_id  num_round       eta     gamma    lambda        alpha  \\\n 470904    23517          1  0.000200  0.014477  0.001188     0.971001   \n 470905    23517          1  0.002022  0.000058  0.258957     1.867090   \n 470906    23517          1  0.005065  1.015660  0.086148     0.338362   \n 470907    23517          1  0.005310  0.009738  0.018482     0.008865   \n 470908    23517          1  0.006275  0.007421  0.047000     5.249080   \n ...         ...        ...       ...       ...       ...          ...   \n 476269    23517       5138  0.504453  0.000261  0.559442  4219.570000   \n 476270    23517       5538  0.046476  0.955996  0.001383     0.716637   \n 476271    23517       5656  0.018053  0.006614  0.009698   685.994000   \n 476272    23517       5845  0.131354  0.010094  0.003000     0.439140   \n 476273    23517       5865  0.003243  0.000045  0.340472    18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 470904   0.186941          7           1.54714          0.644196   \n 470905   0.354213          4           2.44719          0.239795   \n 470906   0.667977         12          24.07030          0.907258   \n 470907   0.521716          9           3.37821          0.616922   \n 470908   0.959774         15           2.04807          0.790151   \n ...           ...        ...               ...               ...   \n 476269   0.981838          7          19.24390          0.824216   \n 476270   0.766008          3          75.79260          0.662224   \n 476271   0.563012          8           4.99961          0.891314   \n 476272   0.403850          1         233.68600          0.214519   \n 476273   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 470904           0.558417  0.508524    1.433429   167120  \n 470905           0.874333  0.512374    1.428500   167120  \n 470906           0.637908  0.510505    1.688937   167120  \n 470907           0.434134  0.511510    1.521500   167120  \n 470908           0.896264  0.508197    1.952444   167120  \n ...                   ...       ...         ...      ...  \n 476269           0.596008  0.500000  136.678231   167120  \n 476270           0.121894  0.520964  185.839500   167120  \n 476271           0.876172  0.522996  221.913571   167120  \n 476272           0.663293  0.518607  162.060714   167120  \n 476273           0.785170  0.522713  260.298000   167120  \n \n [5362 rows x 14 columns],\n 125921:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 476274    40496          1  0.000200  0.014477    0.001188    0.971001   \n 476275    40496          1  0.001911  0.021976    0.009044    0.002017   \n 476276    40496          1  0.002022  0.000058    0.258957    1.867090   \n 476277    40496          1  0.005065  1.015660    0.086148    0.338362   \n 476278    40496          1  0.005310  0.009738    0.018482    0.008865   \n ...         ...        ...       ...       ...         ...         ...   \n 484120    40496       5626  0.003521  0.008594  169.189000    0.181859   \n 484121    40496       5646  0.711899  0.000108    2.355870    0.039822   \n 484122    40496       5656  0.018053  0.006614    0.009698  685.994000   \n 484123    40496       5865  0.003243  0.000045    0.340472   18.326600   \n 484124    40496       5917  0.081559  4.644440    0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 476274   0.186941          7          1.547140          0.644196   \n 476275   0.876026         14          1.951150          0.355134   \n 476276   0.354213          4          2.447190          0.239795   \n 476277   0.667977         12         24.070300          0.907258   \n 476278   0.521716          9          3.378210          0.616922   \n ...           ...        ...               ...               ...   \n 484120   0.701476          5         57.844000          0.762448   \n 484121   0.218562         11         14.791100          0.133948   \n 484122   0.563012          8          4.999610          0.891314   \n 484123   0.394949          6          6.671510          0.051653   \n 484124   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 476274           0.558417  0.854252  0.154500   125921  \n 476275           0.076143  0.806301  0.187000   125921  \n 476276           0.874333  0.747668  0.210125   125921  \n 476277           0.637908  0.727216  0.146000   125921  \n 476278           0.434134  0.794512  0.157667   125921  \n ...                   ...       ...       ...      ...  \n 484120           0.675186  0.500000  8.018667   125921  \n 484121           0.888355  0.518935  8.587667   125921  \n 484122           0.876172  0.500000  7.802000   125921  \n 484123           0.785170  0.827751  9.272500   125921  \n 484124           0.270201  0.954061  7.983000   125921  \n \n [7851 rows x 14 columns],\n 145681:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 484125    40498          1  0.000097  0.002120  1.074280  156.026000   \n 484126    40498          1  0.000200  0.014477  0.001188    0.971001   \n 484127    40498          1  0.001911  0.021976  0.009044    0.002017   \n 484128    40498          1  0.002022  0.000058  0.258957    1.867090   \n 484129    40498          1  0.005065  1.015660  0.086148    0.338362   \n ...         ...        ...       ...       ...       ...         ...   \n 491584    40498       5646  0.711899  0.000108  2.355870    0.039822   \n 491585    40498       5656  0.018053  0.006614  0.009698  685.994000   \n 491586    40498       5845  0.131354  0.010094  0.003000    0.439140   \n 491587    40498       5865  0.003243  0.000045  0.340472   18.326600   \n 491588    40498       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 484125   0.202192          2         16.668800          0.442926   \n 484126   0.186941          7          1.547140          0.644196   \n 484127   0.876026         14          1.951150          0.355134   \n 484128   0.354213          4          2.447190          0.239795   \n 484129   0.667977         12         24.070300          0.907258   \n ...           ...        ...               ...               ...   \n 491584   0.218562         11         14.791100          0.133948   \n 491585   0.563012          8          4.999610          0.891314   \n 491586   0.403850          1        233.686000          0.214519   \n 491587   0.394949          6          6.671510          0.051653   \n 491588   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 484125           0.435029  0.500000   0.201000   145681  \n 484126           0.558417  0.671823   0.216000   145681  \n 484127           0.076143  0.684135   0.214667   145681  \n 484128           0.874333  0.611918   0.189000   145681  \n 484129           0.637908  0.688973   0.204500   145681  \n ...                   ...       ...        ...      ...  \n 491584           0.888355  0.719951  52.858000   145681  \n 491585           0.876172  0.540883  61.633250   145681  \n 491586           0.663293  0.694210  42.276000   145681  \n 491587           0.785170  0.707176  58.018833   145681  \n 491588           0.270201  0.715237  39.558000   145681  \n \n [7464 rows x 14 columns],\n 125922:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 491589    40499          1  0.000200  0.014477    0.001188    0.971001   \n 491590    40499          1  0.001911  0.021976    0.009044    0.002017   \n 491591    40499          1  0.002022  0.000058    0.258957    1.867090   \n 491592    40499          1  0.005065  1.015660    0.086148    0.338362   \n 491593    40499          1  0.005310  0.009738    0.018482    0.008865   \n ...         ...        ...       ...       ...         ...         ...   \n 497810    40499       5626  0.003521  0.008594  169.189000    0.181859   \n 497811    40499       5646  0.711899  0.000108    2.355870    0.039822   \n 497812    40499       5656  0.018053  0.006614    0.009698  685.994000   \n 497813    40499       5845  0.131354  0.010094    0.003000    0.439140   \n 497814    40499       5865  0.003243  0.000045    0.340472   18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 491589   0.186941          7           1.54714          0.644196   \n 491590   0.876026         14           1.95115          0.355134   \n 491591   0.354213          4           2.44719          0.239795   \n 491592   0.667977         12          24.07030          0.907258   \n 491593   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 497810   0.701476          5          57.84400          0.762448   \n 497811   0.218562         11          14.79110          0.133948   \n 497812   0.563012          8           4.99961          0.891314   \n 497813   0.403850          1         233.68600          0.214519   \n 497814   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 491589           0.558417  0.975048    0.335714   125922  \n 491590           0.076143  0.962759    0.328333   125922  \n 491591           0.874333  0.966112    0.349250   125922  \n 491592           0.637908  0.973663    0.389250   125922  \n 491593           0.434134  0.983035    0.462125   125922  \n ...                   ...       ...         ...      ...  \n 497810           0.675186  0.996060  319.942500   125922  \n 497811           0.888355  0.996807  115.179667   125922  \n 497812           0.876172  0.500000  200.650286   125922  \n 497813           0.663293  0.500000  108.793400   125922  \n 497814           0.785170  0.987266  113.573364   125922  \n \n [6226 rows x 14 columns],\n 146607:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 497815    40536          1  0.000097  0.002120  1.074280  156.026000   \n 497816    40536          1  0.000200  0.014477  0.001188    0.971001   \n 497817    40536          1  0.001281  0.000142  0.000156    0.012087   \n 497818    40536          1  0.001911  0.021976  0.009044    0.002017   \n 497819    40536          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 506880    40536       5646  0.711899  0.000108  2.355870    0.039822   \n 506881    40536       5656  0.018053  0.006614  0.009698  685.994000   \n 506882    40536       5845  0.131354  0.010094  0.003000    0.439140   \n 506883    40536       5865  0.003243  0.000045  0.340472   18.326600   \n 506884    40536       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 497815   0.202192          2         16.668800          0.442926   \n 497816   0.186941          7          1.547140          0.644196   \n 497817   0.367161         12          1.027290          0.223957   \n 497818   0.876026         14          1.951150          0.355134   \n 497819   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 506880   0.218562         11         14.791100          0.133948   \n 506881   0.563012          8          4.999610          0.891314   \n 506882   0.403850          1        233.686000          0.214519   \n 506883   0.394949          6          6.671510          0.051653   \n 506884   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 497815           0.435029  0.569665    0.994714   146607  \n 497816           0.558417  0.739309   23.004000   146607  \n 497817           0.909821  0.729995    0.995000   146607  \n 497818           0.076143  0.733023    1.107667   146607  \n 497819           0.874333  0.763585   12.531857   146607  \n ...                   ...       ...         ...      ...  \n 506880           0.888355  0.767094   71.688222   146607  \n 506881           0.876172  0.718581  190.472000   146607  \n 506882           0.663293  0.794432   91.352000   146607  \n 506883           0.785170  0.853985   75.769750   146607  \n 506884           0.270201  0.848650   82.532000   146607  \n \n [9069 rows x 14 columns],\n 146195:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 506885    40668          1  0.024234  0.000067    1.000500  768.603000   \n 506886    40668          1  0.094189  0.000669  183.615000    0.017106   \n 506887    40668          1  0.099818  0.028129    0.001607    0.001102   \n 506888    40668          1  0.101048  0.000067    3.301350    0.119235   \n 506889    40668          1  0.139589  2.918780    0.326829  196.354000   \n ...         ...        ...       ...       ...         ...         ...   \n 509233    40668       3520  0.168119  0.005091    0.001530    4.427620   \n 509234    40668       4127  0.669168  0.002332   87.110400    0.000502   \n 509235    40668       4180  0.504788  0.000805  416.362000  690.390000   \n 509236    40668       4361  0.831121  0.000628    0.068076   28.107900   \n 509238    40668       4680  0.003782  0.959678    0.003538    0.003371   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 506885   0.641650          5           4.64789          0.561103   \n 506886   0.879396         10           3.90781          0.312301   \n 506887   0.821261         14           8.48476          0.245569   \n 506888   0.618137          3         106.53300          0.702793   \n 506889   0.786081          4          40.70170          0.516169   \n ...           ...        ...               ...               ...   \n 509233   0.755557          7          12.01180          0.256184   \n 509234   0.102350         15          51.71450          0.323692   \n 509235   0.584915          4           2.18529          0.118481   \n 509236   0.722156         11           1.19589          0.044284   \n 509238   0.229583          4           7.80861          0.685374   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 506885           0.167461  0.570964    2.761353   146195  \n 506886           0.710544  0.696913    2.301056   146195  \n 506887           0.451169  0.715944    2.606000   146195  \n 506888           0.850884  0.692801    2.259187   146195  \n 506889           0.952272  0.675877    2.215000   146195  \n ...                   ...       ...         ...      ...  \n 509233           0.046136  0.885646  324.991667   146195  \n 509234           0.122990  0.915998  426.455158   146195  \n 509235           0.510327  0.773669  413.072444   146195  \n 509236           0.472366  0.894713  402.746333   146195  \n 509238           0.038369  0.855250  525.693000   146195  \n \n [2242 rows x 14 columns],\n 167140:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 509239    40670          1  0.000097  0.002120  1.074280  156.026000   \n 509240    40670          1  0.000200  0.014477  0.001188    0.971001   \n 509241    40670          1  0.001911  0.021976  0.009044    0.002017   \n 509242    40670          1  0.002022  0.000058  0.258957    1.867090   \n 509243    40670          1  0.005065  1.015660  0.086148    0.338362   \n ...         ...        ...       ...       ...       ...         ...   \n 517521    40670       5646  0.711899  0.000108  2.355870    0.039822   \n 517522    40670       5656  0.018053  0.006614  0.009698  685.994000   \n 517523    40670       5845  0.131354  0.010094  0.003000    0.439140   \n 517524    40670       5865  0.003243  0.000045  0.340472   18.326600   \n 517525    40670       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 509239   0.202192          2         16.668800          0.442926   \n 509240   0.186941          7          1.547140          0.644196   \n 509241   0.876026         14          1.951150          0.355134   \n 509242   0.354213          4          2.447190          0.239795   \n 509243   0.667977         12         24.070300          0.907258   \n ...           ...        ...               ...               ...   \n 517521   0.218562         11         14.791100          0.133948   \n 517522   0.563012          8          4.999610          0.891314   \n 517523   0.403850          1        233.686000          0.214519   \n 517524   0.394949          6          6.671510          0.051653   \n 517525   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 509239           0.435029  0.500000    0.571500   167140  \n 509240           0.558417  0.928054    0.710500   167140  \n 509241           0.076143  0.885804    0.797000   167140  \n 509242           0.874333  0.864137    0.612500   167140  \n 509243           0.637908  0.965899    0.750667   167140  \n ...                   ...       ...         ...      ...  \n 517521           0.888355  0.985529   71.693667   167140  \n 517522           0.876172  0.500000  165.894500   167140  \n 517523           0.663293  0.870043   78.402667   167140  \n 517524           0.785170  0.988777   72.110750   167140  \n 517525           0.270201  0.992059   79.851500   167140  \n \n [8287 rows x 14 columns],\n 167141:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 517526    40701          1  0.000097  0.002120   1.074280  156.026000   \n 517527    40701          1  0.000200  0.014477   0.001188    0.971001   \n 517528    40701          1  0.002022  0.000058   0.258957    1.867090   \n 517529    40701          1  0.004626  0.931871   7.967300   22.020300   \n 517530    40701          1  0.005065  1.015660   0.086148    0.338362   \n ...         ...        ...       ...       ...        ...         ...   \n 527127    40701       5656  0.018053  0.006614   0.009698  685.994000   \n 527128    40701       5845  0.131354  0.010094   0.003000    0.439140   \n 527129    40701       5865  0.003243  0.000045   0.340472   18.326600   \n 527130    40701       5887  0.010789  0.002366  21.107800    0.076290   \n 527131    40701       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 517526   0.202192          2         16.668800          0.442926   \n 517527   0.186941          7          1.547140          0.644196   \n 517528   0.354213          4          2.447190          0.239795   \n 517529   0.101276         12          4.499670          0.679333   \n 517530   0.667977         12         24.070300          0.907258   \n ...           ...        ...               ...               ...   \n 527127   0.563012          8          4.999610          0.891314   \n 527128   0.403850          1        233.686000          0.214519   \n 527129   0.394949          6          6.671510          0.051653   \n 527130   0.241146         12         61.870400          0.537502   \n 527131   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 517526           0.435029  0.515566   0.257750   167141  \n 517527           0.558417  0.808867   0.749667   167141  \n 517528           0.874333  0.640486   0.245750   167141  \n 517529           0.109048  0.500000   0.317000   167141  \n 517530           0.637908  0.848747   0.230000   167141  \n ...                   ...       ...        ...      ...  \n 527127           0.876172  0.824435  17.689000   167141  \n 527128           0.663293  0.500000  10.853500   167141  \n 527129           0.785170  0.816818  14.365231   167141  \n 527130           0.805439  0.615017  15.400000   167141  \n 527131           0.270201  0.883370  19.137833   167141  \n \n [9606 rows x 14 columns],\n 168759:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 527132    40900          1  0.000097  0.002120   1.074280  156.026000   \n 527133    40900          1  0.000200  0.014477   0.001188    0.971001   \n 527134    40900          1  0.001046  0.000033   0.001359  372.626000   \n 527135    40900          1  0.001281  0.000142   0.000156    0.012087   \n 527136    40900          1  0.001911  0.021976   0.009044    0.002017   \n ...         ...        ...       ...       ...        ...         ...   \n 537598    40900       5749  0.028359  0.000032   0.018361    0.007791   \n 537599    40900       5845  0.131354  0.010094   0.003000    0.439140   \n 537600    40900       5865  0.003243  0.000045   0.340472   18.326600   \n 537601    40900       5887  0.010789  0.002366  21.107800    0.076290   \n 537602    40900       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 527132   0.202192          2         16.668800          0.442926   \n 527133   0.186941          7          1.547140          0.644196   \n 527134   0.591609         15          1.099660          0.641624   \n 527135   0.367161         12          1.027290          0.223957   \n 527136   0.876026         14          1.951150          0.355134   \n ...           ...        ...               ...               ...   \n 537598   0.569283         13          2.053570          0.332356   \n 537599   0.403850          1        233.686000          0.214519   \n 537600   0.394949          6          6.671510          0.051653   \n 537601   0.241146         12         61.870400          0.537502   \n 537602   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 527132           0.435029  0.500000   0.26400   168759  \n 527133           0.558417  0.850741   0.26925   168759  \n 527134           0.207744  0.871016   0.25550   168759  \n 527135           0.909821  0.926299   0.28200   168759  \n 527136           0.076143  0.860504   0.26900   168759  \n ...                   ...       ...       ...      ...  \n 537598           0.825170  0.986803  15.89400   168759  \n 537599           0.663293  0.500000  11.00600   168759  \n 537600           0.785170  0.950354  11.71960   168759  \n 537601           0.805439  0.500000  12.25700   168759  \n 537602           0.270201  0.981053  34.35800   168759  \n \n [10471 rows x 14 columns],\n 167121:         data_id  num_round       eta     gamma       lambda       alpha  \\\n 537603    40923          1  0.000200  0.014477     0.001188    0.971001   \n 537604    40923          1  0.001911  0.021976     0.009044    0.002017   \n 537605    40923          1  0.002022  0.000058     0.258957    1.867090   \n 537606    40923          1  0.005065  1.015660     0.086148    0.338362   \n 537607    40923          1  0.005310  0.009738     0.018482    0.008865   \n ...         ...        ...       ...       ...          ...         ...   \n 539486    40923         72  0.001016  0.134359  5921.330000  644.737000   \n 539487    40923         72  0.001843  1.077390     0.111182    0.910184   \n 539488    40923         72  0.009540  0.000186     0.006581   93.031500   \n 539505    40923         75  0.019111  0.001537     7.010870  655.442000   \n 539538    40923         80  0.042324  0.002156     0.040307  863.835000   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 537603   0.186941          7           1.54714          0.644196   \n 537604   0.876026         14           1.95115          0.355134   \n 537605   0.354213          4           2.44719          0.239795   \n 537606   0.667977         12          24.07030          0.907258   \n 537607   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 539486   0.900979          9          27.25610          0.137734   \n 539487   0.131049          1         113.18900          0.246489   \n 539488   0.407589          1           2.00344          0.032953   \n 539505   0.824704          4          45.52140          0.021656   \n 539538   0.294752          1           2.94871          0.031694   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 537603           0.558417  0.892398   88.0588   167121  \n 537604           0.076143  0.918809   53.7164   167121  \n 537605           0.874333  0.888658   62.0405   167121  \n 537606           0.637908  0.915490  178.4724   167121  \n 537607           0.434134  0.919417  119.7099   167121  \n ...                   ...       ...       ...      ...  \n 539486           0.064706  0.824675  537.9143   167121  \n 539487           0.070376  0.904151  529.9143   167121  \n 539488           0.025353  0.897130  519.1935   167121  \n 539505           0.765474  0.821243  576.4244   167121  \n 539538           0.692541  0.500000  584.2824   167121  \n \n [1155 rows x 14 columns],\n 167124:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 539753    40927          1  0.000200  0.014477    0.001188    0.971001   \n 539754    40927          1  0.001911  0.021976    0.009044    0.002017   \n 539755    40927          1  0.002022  0.000058    0.258957    1.867090   \n 539756    40927          1  0.005065  1.015660    0.086148    0.338362   \n 539757    40927          1  0.005310  0.009738    0.018482    0.008865   \n ...         ...        ...       ...       ...         ...         ...   \n 541897    40927        134  0.028712  0.000220   21.266300    0.551205   \n 541899    40927        136  0.004105  0.470673   18.379500   73.787800   \n 541905    40927        137  0.179879  0.000108  493.256000  776.856000   \n 541916    40927        141  0.003436  0.028307   17.231800  156.298000   \n 541949    40927        155  0.002597  0.003534    0.024856  529.370000   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 539753   0.186941          7          1.547140          0.644196   \n 539754   0.876026         14          1.951150          0.355134   \n 539755   0.354213          4          2.447190          0.239795   \n 539756   0.667977         12         24.070300          0.907258   \n 539757   0.521716          9          3.378210          0.616922   \n ...           ...        ...               ...               ...   \n 541897   0.348730          4          6.817740          0.055577   \n 541899   0.267067          6          7.244280          0.013751   \n 541905   0.761299         11         48.508300          0.161433   \n 541916   0.381739          7         71.090300          0.003364   \n 541949   0.110175          8          0.299799          0.056991   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 539753           0.558417  0.711978  108.7431   167124  \n 539754           0.076143  0.705799   86.9774   167124  \n 539755           0.874333  0.732454   95.2709   167124  \n 539756           0.637908  0.758303  201.6407   167124  \n 539757           0.434134  0.728689  128.2185   167124  \n ...                   ...       ...       ...      ...  \n 541897           0.111366  0.829232  583.8291   167124  \n 541899           0.084593  0.770904  551.9231   167124  \n 541905           0.028640  0.743090  555.2964   167124  \n 541916           0.863200  0.766532  591.8501   167124  \n 541949           0.107242  0.500000  591.6633   167124  \n \n [1392 rows x 14 columns],\n 146800:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 542129    40966          1  0.000097  0.002120  1.074280  156.026000   \n 542130    40966          1  0.000200  0.014477  0.001188    0.971001   \n 542131    40966          1  0.001281  0.000142  0.000156    0.012087   \n 542132    40966          1  0.001911  0.021976  0.009044    0.002017   \n 542133    40966          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 550501    40966       5646  0.711899  0.000108  2.355870    0.039822   \n 550502    40966       5656  0.018053  0.006614  0.009698  685.994000   \n 550503    40966       5845  0.131354  0.010094  0.003000    0.439140   \n 550504    40966       5865  0.003243  0.000045  0.340472   18.326600   \n 550505    40966       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 542129   0.202192          2         16.668800          0.442926   \n 542130   0.186941          7          1.547140          0.644196   \n 542131   0.367161         12          1.027290          0.223957   \n 542132   0.876026         14          1.951150          0.355134   \n 542133   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 550501   0.218562         11         14.791100          0.133948   \n 550502   0.563012          8          4.999610          0.891314   \n 550503   0.403850          1        233.686000          0.214519   \n 550504   0.394949          6          6.671510          0.051653   \n 550505   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 542129           0.435029  0.500000   0.303600   146800  \n 542130           0.558417  0.880629   0.312500   146800  \n 542131           0.909821  0.934837   0.311000   146800  \n 542132           0.076143  0.860380   0.504000   146800  \n 542133           0.874333  0.878548   0.316286   146800  \n ...                   ...       ...        ...      ...  \n 550501           0.888355  0.982212  30.798833   146800  \n 550502           0.876172  0.500000  74.720250   146800  \n 550503           0.663293  0.500000  32.248000   146800  \n 550504           0.785170  0.973181  31.459625   146800  \n 550505           0.270201  0.994370  40.659333   146800  \n \n [8377 rows x 14 columns],\n 146821:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 550506    40975          1  0.000097  0.002120   1.074280  156.026000   \n 550507    40975          1  0.000200  0.014477   0.001188    0.971001   \n 550508    40975          1  0.001281  0.000142   0.000156    0.012087   \n 550509    40975          1  0.001911  0.021976   0.009044    0.002017   \n 550510    40975          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 560156    40975       5656  0.018053  0.006614   0.009698  685.994000   \n 560157    40975       5845  0.131354  0.010094   0.003000    0.439140   \n 560158    40975       5865  0.003243  0.000045   0.340472   18.326600   \n 560159    40975       5887  0.010789  0.002366  21.107800    0.076290   \n 560160    40975       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 550506   0.202192          2         16.668800          0.442926   \n 550507   0.186941          7          1.547140          0.644196   \n 550508   0.367161         12          1.027290          0.223957   \n 550509   0.876026         14          1.951150          0.355134   \n 550510   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 560156   0.563012          8          4.999610          0.891314   \n 560157   0.403850          1        233.686000          0.214519   \n 560158   0.394949          6          6.671510          0.051653   \n 560159   0.241146         12         61.870400          0.537502   \n 560160   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 550506           0.435029  0.500000   0.206571   146821  \n 550507           0.558417  0.796805   0.212750   146821  \n 550508           0.909821  0.680176   0.157600   146821  \n 550509           0.076143  0.626615   0.210500   146821  \n 550510           0.874333  0.659335   0.176000   146821  \n ...                   ...       ...        ...      ...  \n 560156           0.876172  0.500000  13.683333   146821  \n 560157           0.663293  0.500000  10.794500   146821  \n 560158           0.785170  0.890880  18.339333   146821  \n 560159           0.805439  0.829620  12.094500   146821  \n 560160           0.270201  0.951843  11.043500   146821  \n \n [9655 rows x 14 columns],\n 146824:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 560161    40979          1  0.000200  0.014477  0.001188    0.971001   \n 560162    40979          1  0.001911  0.021976  0.009044    0.002017   \n 560163    40979          1  0.002022  0.000058  0.258957    1.867090   \n 560164    40979          1  0.005065  1.015660  0.086148    0.338362   \n 560165    40979          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 567227    40979       5646  0.711899  0.000108  2.355870    0.039822   \n 567228    40979       5656  0.018053  0.006614  0.009698  685.994000   \n 567229    40979       5845  0.131354  0.010094  0.003000    0.439140   \n 567230    40979       5865  0.003243  0.000045  0.340472   18.326600   \n 567231    40979       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 560161   0.186941          7          1.547140          0.644196   \n 560162   0.876026         14          1.951150          0.355134   \n 560163   0.354213          4          2.447190          0.239795   \n 560164   0.667977         12         24.070300          0.907258   \n 560165   0.521716          9          3.378210          0.616922   \n ...           ...        ...               ...               ...   \n 567227   0.218562         11         14.791100          0.133948   \n 567228   0.563012          8          4.999610          0.891314   \n 567229   0.403850          1        233.686000          0.214519   \n 567230   0.394949          6          6.671510          0.051653   \n 567231   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 560161           0.558417  0.958906    0.695000   146824  \n 560162           0.076143  0.966694    0.549000   146824  \n 560163           0.874333  0.955931    0.558250   146824  \n 560164           0.637908  0.947222    0.599429   146824  \n 560165           0.434134  0.978351    0.661250   146824  \n ...                   ...       ...         ...      ...  \n 567227           0.888355  0.994072   93.821000   146824  \n 567228           0.876172  0.500000  248.730250   146824  \n 567229           0.663293  0.500000  113.993167   146824  \n 567230           0.785170  0.996717   95.339200   146824  \n 567231           0.270201  0.995176  110.728111   146824  \n \n [7070 rows x 14 columns],\n 146818:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 567232    40981          1  0.000097  0.002120  1.074280  156.026000   \n 567233    40981          1  0.000200  0.014477  0.001188    0.971001   \n 567234    40981          1  0.001281  0.000142  0.000156    0.012087   \n 567235    40981          1  0.001911  0.021976  0.009044    0.002017   \n 567236    40981          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 576081    40981       5646  0.711899  0.000108  2.355870    0.039822   \n 576082    40981       5656  0.018053  0.006614  0.009698  685.994000   \n 576083    40981       5845  0.131354  0.010094  0.003000    0.439140   \n 576084    40981       5865  0.003243  0.000045  0.340472   18.326600   \n 576085    40981       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 567232   0.202192          2         16.668800          0.442926   \n 567233   0.186941          7          1.547140          0.644196   \n 567234   0.367161         12          1.027290          0.223957   \n 567235   0.876026         14          1.951150          0.355134   \n 567236   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 576081   0.218562         11         14.791100          0.133948   \n 576082   0.563012          8          4.999610          0.891314   \n 576083   0.403850          1        233.686000          0.214519   \n 576084   0.394949          6          6.671510          0.051653   \n 576085   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 567232           0.435029  0.500000  0.163400   146818  \n 567233           0.558417  0.892004  0.205000   146818  \n 567234           0.909821  0.771081  0.210667   146818  \n 567235           0.076143  0.638240  0.190200   146818  \n 567236           0.874333  0.692495  4.605500   146818  \n ...                   ...       ...       ...      ...  \n 576081           0.888355  0.860256  3.209000   146818  \n 576082           0.876172  0.500000  4.236000   146818  \n 576083           0.663293  0.500000  3.234500   146818  \n 576084           0.785170  0.930777  3.130462   146818  \n 576085           0.270201  0.914261  3.218000   146818  \n \n [8854 rows x 14 columns],\n 146817:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 576086    40982          1  0.000097  0.002120  1.074280  156.026000   \n 576087    40982          1  0.000200  0.014477  0.001188    0.971001   \n 576088    40982          1  0.001281  0.000142  0.000156    0.012087   \n 576089    40982          1  0.001911  0.021976  0.009044    0.002017   \n 576090    40982          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 584848    40982       5646  0.711899  0.000108  2.355870    0.039822   \n 584849    40982       5656  0.018053  0.006614  0.009698  685.994000   \n 584850    40982       5845  0.131354  0.010094  0.003000    0.439140   \n 584851    40982       5865  0.003243  0.000045  0.340472   18.326600   \n 584852    40982       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 576086   0.202192          2         16.668800          0.442926   \n 576087   0.186941          7          1.547140          0.644196   \n 576088   0.367161         12          1.027290          0.223957   \n 576089   0.876026         14          1.951150          0.355134   \n 576090   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 584848   0.218562         11         14.791100          0.133948   \n 584849   0.563012          8          4.999610          0.891314   \n 584850   0.403850          1        233.686000          0.214519   \n 584851   0.394949          6          6.671510          0.051653   \n 584852   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 576086           0.435029  0.500000   0.188000   146817  \n 576087           0.558417  0.825357   0.265333   146817  \n 576088           0.909821  0.801718   0.221875   146817  \n 576089           0.076143  0.812514   0.239500   146817  \n 576090           0.874333  0.814180   0.269400   146817  \n ...                   ...       ...        ...      ...  \n 584848           0.888355  0.881006  24.097286   146817  \n 584849           0.876172  0.500000  36.498600   146817  \n 584850           0.663293  0.500000  36.659500   146817  \n 584851           0.785170  0.867782  21.990571   146817  \n 584852           0.270201  0.921289  23.482000   146817  \n \n [8767 rows x 14 columns],\n 146820:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 584853    40983          1  0.000097  0.002120   1.074280  156.026000   \n 584854    40983          1  0.000200  0.014477   0.001188    0.971001   \n 584855    40983          1  0.001281  0.000142   0.000156    0.012087   \n 584856    40983          1  0.001911  0.021976   0.009044    0.002017   \n 584857    40983          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 594878    40983       5656  0.018053  0.006614   0.009698  685.994000   \n 594879    40983       5845  0.131354  0.010094   0.003000    0.439140   \n 594880    40983       5865  0.003243  0.000045   0.340472   18.326600   \n 594881    40983       5887  0.010789  0.002366  21.107800    0.076290   \n 594882    40983       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 584853   0.202192          2         16.668800          0.442926   \n 584854   0.186941          7          1.547140          0.644196   \n 584855   0.367161         12          1.027290          0.223957   \n 584856   0.876026         14          1.951150          0.355134   \n 584857   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 594878   0.563012          8          4.999610          0.891314   \n 594879   0.403850          1        233.686000          0.214519   \n 594880   0.394949          6          6.671510          0.051653   \n 594881   0.241146         12         61.870400          0.537502   \n 594882   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 584853           0.435029  0.500000   0.180500   146820  \n 584854           0.558417  0.586812   0.211000   146820  \n 584855           0.909821  0.496179   0.293500   146820  \n 584856           0.076143  0.543788   0.239667   146820  \n 584857           0.874333  0.523818   0.178875   146820  \n ...                   ...       ...        ...      ...  \n 594878           0.876172  0.500000   8.537000   146820  \n 594879           0.663293  0.500000   8.004000   146820  \n 594880           0.785170  0.946842  11.242778   146820  \n 594881           0.805439  0.860178  13.650333   146820  \n 594882           0.270201  0.986396   8.655000   146820  \n \n [10030 rows x 14 columns],\n 146822:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 594883    40984          1  0.000097  0.002120  1.074280  156.026000   \n 594884    40984          1  0.000200  0.014477  0.001188    0.971001   \n 594885    40984          1  0.001281  0.000142  0.000156    0.012087   \n 594886    40984          1  0.001911  0.021976  0.009044    0.002017   \n 594887    40984          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 603848    40984       5646  0.711899  0.000108  2.355870    0.039822   \n 603849    40984       5656  0.018053  0.006614  0.009698  685.994000   \n 603850    40984       5845  0.131354  0.010094  0.003000    0.439140   \n 603851    40984       5865  0.003243  0.000045  0.340472   18.326600   \n 603852    40984       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 594883   0.202192          2         16.668800          0.442926   \n 594884   0.186941          7          1.547140          0.644196   \n 594885   0.367161         12          1.027290          0.223957   \n 594886   0.876026         14          1.951150          0.355134   \n 594887   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 603848   0.218562         11         14.791100          0.133948   \n 603849   0.563012          8          4.999610          0.891314   \n 603850   0.403850          1        233.686000          0.214519   \n 603851   0.394949          6          6.671510          0.051653   \n 603852   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 594883           0.435029  0.500000   0.212400   146822  \n 594884           0.558417  0.963748   0.214286   146822  \n 594885           0.909821  0.917880   0.200000   146822  \n 594886           0.076143  0.956120   0.220000   146822  \n 594887           0.874333  0.959142   0.192625   146822  \n ...                   ...       ...        ...      ...  \n 603848           0.888355  0.992275  20.871667   146822  \n 603849           0.876172  0.500000  32.298000   146822  \n 603850           0.663293  0.500000  20.516000   146822  \n 603851           0.785170  0.984592  25.039500   146822  \n 603852           0.270201  0.995598  23.341333   146822  \n \n [8970 rows x 14 columns],\n 146819:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 603853    40994          1  0.000097  0.002120  1.074280  156.026000   \n 603854    40994          1  0.000200  0.014477  0.001188    0.971001   \n 603855    40994          1  0.001281  0.000142  0.000156    0.012087   \n 603856    40994          1  0.001911  0.021976  0.009044    0.002017   \n 603857    40994          1  0.002022  0.000058  0.258957    1.867090   \n ...         ...        ...       ...       ...       ...         ...   \n 612267    40994       5646  0.711899  0.000108  2.355870    0.039822   \n 612268    40994       5656  0.018053  0.006614  0.009698  685.994000   \n 612269    40994       5845  0.131354  0.010094  0.003000    0.439140   \n 612270    40994       5865  0.003243  0.000045  0.340472   18.326600   \n 612271    40994       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 603853   0.202192          2         16.668800          0.442926   \n 603854   0.186941          7          1.547140          0.644196   \n 603855   0.367161         12          1.027290          0.223957   \n 603856   0.876026         14          1.951150          0.355134   \n 603857   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 612267   0.218562         11         14.791100          0.133948   \n 612268   0.563012          8          4.999610          0.891314   \n 612269   0.403850          1        233.686000          0.214519   \n 612270   0.394949          6          6.671510          0.051653   \n 612271   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 603853           0.435029  0.500000  0.173200   146819  \n 603854           0.558417  0.613469  0.208000   146819  \n 603855           0.909821  0.679681  0.176500   146819  \n 603856           0.076143  0.479592  0.169000   146819  \n 603857           0.874333  0.560673  0.188600   146819  \n ...                   ...       ...       ...      ...  \n 612267           0.888355  0.500000  2.487000   146819  \n 612268           0.876172  0.500000  2.994400   146819  \n 612269           0.663293  0.500000  2.947000   146819  \n 612270           0.785170  0.878010  5.067667   146819  \n 612271           0.270201  0.985000  2.822000   146819  \n \n [8419 rows x 14 columns],\n 168765:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 612272    41142          1  0.000200  0.014477  0.001188    0.971001   \n 612273    41142          1  0.001911  0.021976  0.009044    0.002017   \n 612274    41142          1  0.002022  0.000058  0.258957    1.867090   \n 612275    41142          1  0.005065  1.015660  0.086148    0.338362   \n 612276    41142          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 619238    41142       5581  0.081455  1.323940  3.207280  721.920000   \n 619240    41142       5646  0.711899  0.000108  2.355870    0.039822   \n 619241    41142       5656  0.018053  0.006614  0.009698  685.994000   \n 619242    41142       5845  0.131354  0.010094  0.003000    0.439140   \n 619243    41142       5865  0.003243  0.000045  0.340472   18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 612272   0.186941          7           1.54714          0.644196   \n 612273   0.876026         14           1.95115          0.355134   \n 612274   0.354213          4           2.44719          0.239795   \n 612275   0.667977         12          24.07030          0.907258   \n 612276   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 619238   0.764634         10         112.93100          0.033118   \n 619240   0.218562         11          14.79110          0.133948   \n 619241   0.563012          8           4.99961          0.891314   \n 619242   0.403850          1         233.68600          0.214519   \n 619243   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 612272           0.558417  0.690484    7.455750   168765  \n 612273           0.076143  0.675999    4.575800   168765  \n 612274           0.874333  0.710343    4.518000   168765  \n 612275           0.637908  0.755180    4.747000   168765  \n 612276           0.434134  0.678469   13.199333   168765  \n ...                   ...       ...         ...      ...  \n 619238           0.470175  0.500000  111.508500   168765  \n 619240           0.888355  0.692311  158.368000   168765  \n 619241           0.876172  0.500000  503.488600   168765  \n 619242           0.663293  0.757778  192.343200   168765  \n 619243           0.785170  0.806630  248.511667   168765  \n \n [6922 rows x 14 columns],\n 168764:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 619244    41143          1  0.000097  0.002120   1.074280  156.026000   \n 619245    41143          1  0.000200  0.014477   0.001188    0.971001   \n 619246    41143          1  0.001046  0.000033   0.001359  372.626000   \n 619247    41143          1  0.001281  0.000142   0.000156    0.012087   \n 619248    41143          1  0.001911  0.021976   0.009044    0.002017   \n ...         ...        ...       ...       ...        ...         ...   \n 629415    41143       5646  0.711899  0.000108   2.355870    0.039822   \n 629416    41143       5656  0.018053  0.006614   0.009698  685.994000   \n 629417    41143       5865  0.003243  0.000045   0.340472   18.326600   \n 629418    41143       5887  0.010789  0.002366  21.107800    0.076290   \n 629419    41143       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 619244   0.202192          2         16.668800          0.442926   \n 619245   0.186941          7          1.547140          0.644196   \n 619246   0.591609         15          1.099660          0.641624   \n 619247   0.367161         12          1.027290          0.223957   \n 619248   0.876026         14          1.951150          0.355134   \n ...           ...        ...               ...               ...   \n 629415   0.218562         11         14.791100          0.133948   \n 629416   0.563012          8          4.999610          0.891314   \n 629417   0.394949          6          6.671510          0.051653   \n 629418   0.241146         12         61.870400          0.537502   \n 629419   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 619244           0.435029  0.500000   1.108750   168764  \n 619245           0.558417  0.806868   0.519000   168764  \n 619246           0.207744  0.500000   0.460167   168764  \n 619247           0.909821  0.782228   0.448500   168764  \n 619248           0.076143  0.765014   0.495000   168764  \n ...                   ...       ...        ...      ...  \n 629415           0.888355  0.809974  26.367167   168764  \n 629416           0.876172  0.500000  42.969000   168764  \n 629417           0.785170  0.839095  19.356600   168764  \n 629418           0.805439  0.836600  42.117000   168764  \n 629419           0.270201  0.832125  21.568000   168764  \n \n [10176 rows x 14 columns],\n 168761:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 629420    41146          1  0.000097  0.002120   1.074280  156.026000   \n 629421    41146          1  0.000200  0.014477   0.001188    0.971001   \n 629422    41146          1  0.001281  0.000142   0.000156    0.012087   \n 629423    41146          1  0.001911  0.021976   0.009044    0.002017   \n 629424    41146          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 638884    41146       5656  0.018053  0.006614   0.009698  685.994000   \n 638885    41146       5845  0.131354  0.010094   0.003000    0.439140   \n 638886    41146       5865  0.003243  0.000045   0.340472   18.326600   \n 638887    41146       5887  0.010789  0.002366  21.107800    0.076290   \n 638888    41146       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 629420   0.202192          2         16.668800          0.442926   \n 629421   0.186941          7          1.547140          0.644196   \n 629422   0.367161         12          1.027290          0.223957   \n 629423   0.876026         14          1.951150          0.355134   \n 629424   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 638884   0.563012          8          4.999610          0.891314   \n 638885   0.403850          1        233.686000          0.214519   \n 638886   0.394949          6          6.671510          0.051653   \n 638887   0.241146         12         61.870400          0.537502   \n 638888   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 629420           0.435029  0.560575   0.216000   168761  \n 629421           0.558417  0.700534   0.303000   168761  \n 629422           0.909821  0.927910   0.218000   168761  \n 629423           0.076143  0.609321   0.270333   168761  \n 629424           0.874333  0.798420   2.262667   168761  \n ...                   ...       ...        ...      ...  \n 638884           0.876172  0.500000  12.383000   168761  \n 638885           0.663293  0.913915  13.336000   168761  \n 638886           0.785170  0.957301  11.212000   168761  \n 638887           0.805439  0.960182  14.115500   168761  \n 638888           0.270201  0.971975  10.302667   168761  \n \n [9469 rows x 14 columns],\n 168335:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 638889    41150          1  0.000200  0.014477  0.001188    0.971001   \n 638890    41150          1  0.001911  0.021976  0.009044    0.002017   \n 638891    41150          1  0.002022  0.000058  0.258957    1.867090   \n 638892    41150          1  0.005065  1.015660  0.086148    0.338362   \n 638893    41150          1  0.005310  0.009738  0.018482    0.008865   \n ...         ...        ...       ...       ...       ...         ...   \n 644836    41150       5538  0.046476  0.955996  0.001383    0.716637   \n 644837    41150       5581  0.081455  1.323940  3.207280  721.920000   \n 644838    41150       5646  0.711899  0.000108  2.355870    0.039822   \n 644840    41150       5845  0.131354  0.010094  0.003000    0.439140   \n 644841    41150       5865  0.003243  0.000045  0.340472   18.326600   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 638889   0.186941          7           1.54714          0.644196   \n 638890   0.876026         14           1.95115          0.355134   \n 638891   0.354213          4           2.44719          0.239795   \n 638892   0.667977         12          24.07030          0.907258   \n 638893   0.521716          9           3.37821          0.616922   \n ...           ...        ...               ...               ...   \n 644836   0.766008          3          75.79260          0.662224   \n 644837   0.764634         10         112.93100          0.033118   \n 644838   0.218562         11          14.79110          0.133948   \n 644840   0.403850          1         233.68600          0.214519   \n 644841   0.394949          6           6.67151          0.051653   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 638889           0.558417  0.935604    3.874833   168335  \n 638890           0.076143  0.891749    4.377333   168335  \n 638891           0.874333  0.880971    6.133370   168335  \n 638892           0.637908  0.958286    4.697875   168335  \n 638893           0.434134  0.944817    3.887500   168335  \n ...                   ...       ...         ...      ...  \n 644836           0.121894  0.985654  444.503857   168335  \n 644837           0.470175  0.960481  234.842750   168335  \n 644838           0.888355  0.953638  545.022667   168335  \n 644840           0.663293  0.973429  303.080500   168335  \n 644841           0.785170  0.969310  429.285571   168335  \n \n [5786 rows x 14 columns],\n 168767:         data_id  num_round       eta     gamma     lambda       alpha  \\\n 644842    41156          1  0.000097  0.002120   1.074280  156.026000   \n 644843    41156          1  0.000200  0.014477   0.001188    0.971001   \n 644844    41156          1  0.001281  0.000142   0.000156    0.012087   \n 644845    41156          1  0.001911  0.021976   0.009044    0.002017   \n 644846    41156          1  0.002022  0.000058   0.258957    1.867090   \n ...         ...        ...       ...       ...        ...         ...   \n 654708    41156       5656  0.018053  0.006614   0.009698  685.994000   \n 654709    41156       5845  0.131354  0.010094   0.003000    0.439140   \n 654710    41156       5865  0.003243  0.000045   0.340472   18.326600   \n 654711    41156       5887  0.010789  0.002366  21.107800    0.076290   \n 654712    41156       5917  0.081559  4.644440   0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 644842   0.202192          2         16.668800          0.442926   \n 644843   0.186941          7          1.547140          0.644196   \n 644844   0.367161         12          1.027290          0.223957   \n 644845   0.876026         14          1.951150          0.355134   \n 644846   0.354213          4          2.447190          0.239795   \n ...           ...        ...               ...               ...   \n 654708   0.563012          8          4.999610          0.891314   \n 654709   0.403850          1        233.686000          0.214519   \n 654710   0.394949          6          6.671510          0.051653   \n 654711   0.241146         12         61.870400          0.537502   \n 654712   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc   avg_time  task_id  \n 644842           0.435029  0.500000   0.294000   168767  \n 644843           0.558417  0.808641   0.266000   168767  \n 644844           0.909821  0.793751   0.280000   168767  \n 644845           0.076143  0.676659   0.281667   168767  \n 644846           0.874333  0.798425   0.298333   168767  \n ...                   ...       ...        ...      ...  \n 654708           0.876172  0.500000  14.612500   168767  \n 654709           0.663293  0.500000   9.964000   168767  \n 654710           0.785170  0.890322   9.691222   168767  \n 654711           0.805439  0.834057  25.370500   168767  \n 654712           0.270201  0.919304   9.937500   168767  \n \n [9871 rows x 14 columns],\n 168768:         data_id  num_round       eta     gamma    lambda       alpha  \\\n 654713    41157          1  0.000097  0.002120  1.074280  156.026000   \n 654714    41157          1  0.001281  0.000142  0.000156    0.012087   \n 654715    41157          1  0.001911  0.021976  0.009044    0.002017   \n 654716    41157          1  0.002022  0.000058  0.258957    1.867090   \n 654717    41157          1  0.004626  0.931871  7.967300   22.020300   \n ...         ...        ...       ...       ...       ...         ...   \n 663177    41157       5646  0.711899  0.000108  2.355870    0.039822   \n 663178    41157       5656  0.018053  0.006614  0.009698  685.994000   \n 663179    41157       5845  0.131354  0.010094  0.003000    0.439140   \n 663180    41157       5865  0.003243  0.000045  0.340472   18.326600   \n 663181    41157       5917  0.081559  4.644440  0.001400    0.253718   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 654713   0.202192          2         16.668800          0.442926   \n 654714   0.367161         12          1.027290          0.223957   \n 654715   0.876026         14          1.951150          0.355134   \n 654716   0.354213          4          2.447190          0.239795   \n 654717   0.101276         12          4.499670          0.679333   \n ...           ...        ...               ...               ...   \n 663177   0.218562         11         14.791100          0.133948   \n 663178   0.563012          8          4.999610          0.891314   \n 663179   0.403850          1        233.686000          0.214519   \n 663180   0.394949          6          6.671510          0.051653   \n 663181   0.998722          1          0.592597          0.802198   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 654713           0.435029  0.500000    9.618750   168768  \n 654714           0.909821  0.626528   20.940167   168768  \n 654715           0.076143  0.624167    9.804667   168768  \n 654716           0.874333  0.617857   23.987714   168768  \n 654717           0.109048  0.500000    9.295000   168768  \n ...                   ...       ...         ...      ...  \n 663177           0.888355  0.500000   39.901667   168768  \n 663178           0.876172  0.500000  101.093000   168768  \n 663179           0.663293  0.500000   37.901000   168768  \n 663180           0.785170  0.500000   32.809455   168768  \n 663181           0.270201  0.916667   62.648000   168768  \n \n [8469 rows x 14 columns],\n 168337:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 663182    41159          1  0.000200  0.014477    0.001188    0.971001   \n 663183    41159          1  0.002022  0.000058    0.258957    1.867090   \n 663184    41159          1  0.005065  1.015660    0.086148    0.338362   \n 663185    41159          1  0.005310  0.009738    0.018482    0.008865   \n 663186    41159          1  0.006275  0.007421    0.047000    5.249080   \n ...         ...        ...       ...       ...         ...         ...   \n 668295    41159       2097  0.017460  0.087025    0.003724    0.320840   \n 668296    41159       2107  0.167591  0.001877  551.975000    8.572160   \n 668297    41159       2117  0.480287  1.980150  303.541000   98.122900   \n 668299    41159       2200  0.037232  0.002664   21.585800  194.988000   \n 668304    41159       2655  0.009224  0.000164    0.000008   19.739900   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 663182   0.186941          7           1.54714          0.644196   \n 663183   0.354213          4           2.44719          0.239795   \n 663184   0.667977         12          24.07030          0.907258   \n 663185   0.521716          9           3.37821          0.616922   \n 663186   0.959774         15           2.04807          0.790151   \n ...           ...        ...               ...               ...   \n 668295   0.773870         12          51.37710          0.106106   \n 668296   0.130110         16          66.11220          0.113656   \n 668297   0.159755         14           4.53173          0.948839   \n 668299   0.270176          4           9.87309          0.880697   \n 668304   0.589483          1           2.15243          0.027240   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 663182           0.558417  0.726639   33.107333   168337  \n 663183           0.874333  0.678523   54.200000   168337  \n 663184           0.637908  0.783298   39.548000   168337  \n 663185           0.434134  0.772882   36.403600   168337  \n 663186           0.896264  0.806272   50.270333   168337  \n ...                   ...       ...         ...      ...  \n 668295           0.053770  0.895832  499.837500   168337  \n 668296           0.420408  0.814092  574.722222   168337  \n 668297           0.117871  0.774917  533.701067   168337  \n 668299           0.123730  0.737227  589.533000   168337  \n 668304           0.720329  0.775241  557.691250   168337  \n \n [4423 rows x 14 columns],\n 168338:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 668324    41161          1  0.000200  0.014477    0.001188    0.971001   \n 668325    41161          1  0.002022  0.000058    0.258957    1.867090   \n 668326    41161          1  0.005065  1.015660    0.086148    0.338362   \n 668327    41161          1  0.005310  0.009738    0.018482    0.008865   \n 668328    41161          1  0.006275  0.007421    0.047000    5.249080   \n ...         ...        ...       ...       ...         ...         ...   \n 673315    41161       2107  0.167591  0.001877  551.975000    8.572160   \n 673316    41161       2117  0.480287  1.980150  303.541000   98.122900   \n 673318    41161       2200  0.037232  0.002664   21.585800  194.988000   \n 673324    41161       2655  0.009224  0.000164    0.000008   19.739900   \n 673328    41161       2825  0.023328  0.000729    0.029029    0.136759   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 668324   0.186941          7           1.54714          0.644196   \n 668325   0.354213          4           2.44719          0.239795   \n 668326   0.667977         12          24.07030          0.907258   \n 668327   0.521716          9           3.37821          0.616922   \n 668328   0.959774         15           2.04807          0.790151   \n ...           ...        ...               ...               ...   \n 673315   0.130110         16          66.11220          0.113656   \n 673316   0.159755         14           4.53173          0.948839   \n 673318   0.270176          4           9.87309          0.880697   \n 673324   0.589483          1           2.15243          0.027240   \n 673328   0.013699          6          46.69330          0.212363   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 668324           0.558417  0.766228   33.204857   168338  \n 668325           0.874333  0.716591   46.168125   168338  \n 668326           0.637908  0.857224   51.012429   168338  \n 668327           0.434134  0.829120   39.323000   168338  \n 668328           0.896264  0.908044   41.741600   168338  \n ...                   ...       ...         ...      ...  \n 673315           0.420408  0.996032  531.490643   168338  \n 673316           0.117871  0.905588  511.301571   168338  \n 673318           0.123730  0.891232  580.464182   168338  \n 673324           0.720329  0.935861  545.441250   168338  \n 673328           0.795850  0.500000  598.881625   168338  \n \n [4348 rows x 14 columns],\n 168770:         data_id  num_round       eta     gamma        lambda       alpha  \\\n 673346    41163          1  0.002022  0.000058      0.258957    1.867090   \n 673347    41163          1  0.006275  0.007421      0.047000    5.249080   \n 673348    41163          1  0.006948  0.000345  18183.000000    0.034853   \n 673349    41163          1  0.024234  0.000067      1.000500  768.603000   \n 673350    41163          1  0.026128  0.002665      1.395920    2.984300   \n ...         ...        ...       ...       ...           ...         ...   \n 676474    41163       2042  0.010077  5.448790     17.985100    0.003473   \n 676475    41163       2042  0.024953  0.028946     69.766500    0.113284   \n 676477    41163       2097  0.017460  0.087025      0.003724    0.320840   \n 676478    41163       2107  0.167591  0.001877    551.975000    8.572160   \n 676485    41163       2655  0.009224  0.000164      0.000008   19.739900   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 673346   0.354213          4           2.44719          0.239795   \n 673347   0.959774         15           2.04807          0.790151   \n 673348   0.194973          1         115.84000          0.506227   \n 673349   0.641650          5           4.64789          0.561103   \n 673350   0.876623          8          13.50390          0.438203   \n ...           ...        ...               ...               ...   \n 676474   0.516276         14        4776.42000          0.611708   \n 676475   0.144710         11          42.44260          0.420743   \n 676477   0.773870         12          51.37710          0.106106   \n 676478   0.130110         16          66.11220          0.113656   \n 676485   0.589483          1           2.15243          0.027240   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 673346           0.874333  0.900831   10.521857   168770  \n 673347           0.896264  0.965120   36.162857   168770  \n 673348           0.956662  0.766397    9.777333   168770  \n 673349           0.167461  0.500000   15.592364   168770  \n 673350           0.644152  0.950184   17.645625   168770  \n ...                   ...       ...         ...      ...  \n 676474           0.029486  0.500000  427.391667   168770  \n 676475           0.086915  0.993434  542.967429   168770  \n 676477           0.053770  0.999434  497.625067   168770  \n 676478           0.420408  0.981341  516.598200   168770  \n 676485           0.720329  0.971668  585.520636   168770  \n \n [2714 rows x 14 columns],\n 168760:         data_id  num_round       eta     gamma      lambda       alpha  \\\n 676495    41164          1  0.002022  0.000058    0.258957    1.867090   \n 676496    41164          1  0.024234  0.000067    1.000500  768.603000   \n 676497    41164          1  0.042303  0.029853  158.783000    0.001131   \n 676498    41164          1  0.049104  0.353042    0.000065    0.027377   \n 676499    41164          1  0.094189  0.000669  183.615000    0.017106   \n ...         ...        ...       ...       ...         ...         ...   \n 679601    41164       4127  0.669168  0.002332   87.110400    0.000502   \n 679602    41164       4180  0.504788  0.000805  416.362000  690.390000   \n 679603    41164       4361  0.831121  0.000628    0.068076   28.107900   \n 679604    41164       4430  0.600513  0.000188    0.006966  935.943000   \n 679606    41164       4680  0.003782  0.959678    0.003538    0.003371   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 676495   0.354213          4           2.44719          0.239795   \n 676496   0.641650          5           4.64789          0.561103   \n 676497   0.525185          1          17.03950          0.916042   \n 676498   0.959828          3           2.48812          0.180005   \n 676499   0.879396         10           3.90781          0.312301   \n ...           ...        ...               ...               ...   \n 679601   0.102350         15          51.71450          0.323692   \n 679602   0.584915          4           2.18529          0.118481   \n 679603   0.722156         11           1.19589          0.044284   \n 679604   0.434922          8           1.27593          0.240157   \n 679606   0.229583          4           7.80861          0.685374   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 676495           0.874333  0.626467    3.242000   168760  \n 676496           0.167461  0.500000    3.138857   168760  \n 676497           0.085641  0.541029    4.461000   168760  \n 676498           0.161454  0.579766    3.842867   168760  \n 676499           0.710544  0.676932    6.542562   168760  \n ...                   ...       ...         ...      ...  \n 679601           0.122990  0.500000  426.458833   168760  \n 679602           0.510327  0.500000  470.776769   168760  \n 679603           0.472366  0.887926  469.791000   168760  \n 679604           0.395377  0.500000  557.807429   168760  \n 679606           0.038369  0.800723  570.842833   168760  \n \n [2948 rows x 14 columns],\n 168331:         data_id  num_round       eta     gamma      lambda        alpha  \\\n 679608    41166          1  0.024234  0.000067    1.000500   768.603000   \n 679609    41166          1  0.042303  0.029853  158.783000     0.001131   \n 679610    41166          1  0.049104  0.353042    0.000065     0.027377   \n 679611    41166          1  0.094189  0.000669  183.615000     0.017106   \n 679612    41166          1  0.099818  0.028129    0.001607     0.001102   \n ...         ...        ...       ...       ...         ...          ...   \n 682070    41166       1758  0.115709  0.001155    0.414496    84.398700   \n 682072    41166       1775  0.374792  4.027000    7.330860   110.257000   \n 682085    41166       1862  0.001132  0.019673  357.180000  1017.430000   \n 682101    41166       2038  0.153044  0.004377    2.655400   450.561000   \n 682102    41166       2042  0.010077  5.448790   17.985100     0.003473   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 679608   0.641650          5           4.64789          0.561103   \n 679609   0.525185          1          17.03950          0.916042   \n 679610   0.959828          3           2.48812          0.180005   \n 679611   0.879396         10           3.90781          0.312301   \n 679612   0.821261         14           8.48476          0.245569   \n ...           ...        ...               ...               ...   \n 682070   0.651704         14          38.82860          0.256725   \n 682072   0.380522         11          80.17730          0.514888   \n 682085   0.440474          2           2.42984          0.114101   \n 682101   0.728295          4           4.40586          0.985228   \n 682102   0.516276         14        4776.42000          0.611708   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 679608           0.167461  0.791491    4.4995   168331  \n 679609           0.085641  0.735868    4.2491   168331  \n 679610           0.161454  0.781920    6.6360   168331  \n 679611           0.710544  0.843937    6.4637   168331  \n 679612           0.451169  0.854108    9.0855   168331  \n ...                   ...       ...       ...      ...  \n 682070           0.014657  0.897115  515.6894   168331  \n 682072           0.017968  0.875306  491.0609   168331  \n 682085           0.271424  0.788006  592.1710   168331  \n 682101           0.023599  0.861654  560.5207   168331  \n 682102           0.029486  0.500000  563.9864   168331  \n \n [2119 rows x 14 columns],\n 168330:         data_id  num_round       eta     gamma        lambda       alpha  \\\n 682112    41168          1  0.002022  0.000058      0.258957    1.867090   \n 682113    41168          1  0.006275  0.007421      0.047000    5.249080   \n 682114    41168          1  0.006948  0.000345  18183.000000    0.034853   \n 682115    41168          1  0.024234  0.000067      1.000500  768.603000   \n 682116    41168          1  0.026128  0.002665      1.395920    2.984300   \n ...         ...        ...       ...       ...           ...         ...   \n 685430    41168       2655  0.009224  0.000164      0.000008   19.739900   \n 685434    41168       2825  0.023328  0.000729      0.029029    0.136759   \n 685436    41168       3387  0.017974  0.002811     13.424800    0.002545   \n 685437    41168       3427  0.218403  1.106990      0.002202    0.486650   \n 685440    41168       4180  0.504788  0.000805    416.362000  690.390000   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 682112   0.354213          4           2.44719          0.239795   \n 682113   0.959774         15           2.04807          0.790151   \n 682114   0.194973          1         115.84000          0.506227   \n 682115   0.641650          5           4.64789          0.561103   \n 682116   0.876623          8          13.50390          0.438203   \n ...           ...        ...               ...               ...   \n 685430   0.589483          1           2.15243          0.027240   \n 685434   0.013699          6          46.69330          0.212363   \n 685436   0.560355          5           1.08976          0.131838   \n 685437   0.650195          1         122.55400          0.156254   \n 685440   0.584915          4           2.18529          0.118481   \n \n         colsample_bylevel   avg_auc  avg_time  task_id  \n 682112           0.874333  0.751699    2.7099   168330  \n 682113           0.896264  0.816148   12.4464   168330  \n 682114           0.956662  0.679283    2.6345   168330  \n 682115           0.167461  0.718015    6.6331   168330  \n 682116           0.644152  0.798050    4.0579   168330  \n ...                   ...       ...       ...      ...  \n 685430           0.720329  0.793710  264.7594   168330  \n 685434           0.795850  0.842203  424.8717   168330  \n 685436           0.121567  0.853295  565.8842   168330  \n 685437           0.868801  0.840770  434.5951   168330  \n 685440           0.510327  0.804929  450.9357   168330  \n \n [3092 rows x 14 columns],\n 168329:         data_id  num_round       eta     gamma         lambda         alpha  \\\n 685444    41169          1  0.024234  0.000067       1.000500  7.686030e+02   \n 685445    41169          1  0.094189  0.000669     183.615000  1.710590e-02   \n 685446    41169          1  0.099818  0.028129       0.001607  1.102340e-03   \n 685447    41169          1  0.101048  0.000067       3.301350  1.192350e-01   \n 685448    41169          1  0.139589  2.918780       0.326829  1.963540e+02   \n ...         ...        ...       ...       ...            ...           ...   \n 686918    41169        289  0.001786  0.000044       0.000661  1.300380e+06   \n 686920    41169        291  0.004754  0.036812       0.083498  4.374930e+02   \n 686922    41169        296  0.032758  0.013199       0.064903  6.894050e+02   \n 686925    41169        297  0.003336  0.002165  225522.000000  2.144040e+04   \n 686927    41169        299  0.053342  0.003202       0.003826  1.459330e+02   \n \n         subsample  max_depth  min_child_weight  colsample_bytree  \\\n 685444   0.641650          5           4.64789          0.561103   \n 685445   0.879396         10           3.90781          0.312301   \n 685446   0.821261         14           8.48476          0.245569   \n 685447   0.618137          3         106.53300          0.702793   \n 685448   0.786081          4          40.70170          0.516169   \n ...           ...        ...               ...               ...   \n 686918   0.423302         10          34.27490          0.280342   \n 686920   0.283655          2          22.37560          0.063498   \n 686922   0.139465          6          66.05600          0.731609   \n 686925   0.659421          2           1.67637          0.206902   \n 686927   0.143239          9          85.40560          0.384941   \n \n         colsample_bylevel   avg_auc    avg_time  task_id  \n 685444           0.167461  0.647285    3.871100   168329  \n 685445           0.710544  0.771419    8.119100   168329  \n 685446           0.451169  0.768174   13.896400   168329  \n 685447           0.850884  0.775310    7.700400   168329  \n 685448           0.952272  0.725790    6.307800   168329  \n ...                   ...       ...         ...      ...  \n 686918           0.045500  0.500000  475.443600   168329  \n 686920           0.838751  0.690066  529.362000   168329  \n 686922           0.158958  0.500000  474.058967   168329  \n 686925           0.611683  0.500000  565.682000   168329  \n 686927           0.376549  0.723821  548.418800   168329  \n \n [1275 rows x 14 columns]}"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Testing get_average_performance with per_task=True, so instead of a single dataframe we get a dictionary of dataframes\n",
    "    with the key being the task_id of a training dataset\n",
    "\"\"\"\n",
    "from python.project_utils import get_dataset_to_task,get_average_performance\n",
    "avg_perf = get_average_performance(per_task=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys([3, 6, 11, 12, 14, 15, 18, 23, 24, 28, 29, 32, 37, 3021, 41, 43, 45, 49, 53, 58, 219, 2073, 3481, 3022, 3485, 3510, 3512, 3549, 3560, 3561, 3573, 3893, 3902, 3903, 3904, 3917, 3918, 3945, 10090, 14965, 10093, 10101, 9981, 9985, 9986, 9970, 9976, 9977, 9978, 9956, 9957, 9960, 9964, 9946, 7592, 9910, 34539, 14952, 14969, 168339, 125920, 146606, 167120, 125921, 145681, 125922, 146607, 146195, 167140, 167141, 168759, 167121, 167124, 146800, 146821, 146824, 146818, 146817, 146820, 146822, 146819, 168765, 168764, 168761, 168335, 168767, 168768, 168337, 168338, 168770, 168760, 168331, 168330, 168329])"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(avg_perf.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "      data_id  num_round       eta     gamma     lambda       alpha  \\\n0           3          1  0.000097  0.002120   1.074280  156.026000   \n1           3          1  0.000200  0.014477   0.001188    0.971001   \n2           3          1  0.001281  0.000142   0.000156    0.012087   \n3           3          1  0.001911  0.021976   0.009044    0.002017   \n4           3          1  0.002022  0.000058   0.258957    1.867090   \n...       ...        ...       ...       ...        ...         ...   \n9945        3       5749  0.028359  0.000032   0.018361    0.007791   \n9946        3       5845  0.131354  0.010094   0.003000    0.439140   \n9947        3       5865  0.003243  0.000045   0.340472   18.326600   \n9948        3       5887  0.010789  0.002366  21.107800    0.076290   \n9949        3       5917  0.081559  4.644440   0.001400    0.253718   \n\n      subsample  max_depth  min_child_weight  colsample_bytree  \\\n0      0.202192          2         16.668800          0.442926   \n1      0.186941          7          1.547140          0.644196   \n2      0.367161         12          1.027290          0.223957   \n3      0.876026         14          1.951150          0.355134   \n4      0.354213          4          2.447190          0.239795   \n...         ...        ...               ...               ...   \n9945   0.569283         13          2.053570          0.332356   \n9946   0.403850          1        233.686000          0.214519   \n9947   0.394949          6          6.671510          0.051653   \n9948   0.241146         12         61.870400          0.537502   \n9949   0.998722          1          0.592597          0.802198   \n\n      colsample_bylevel   avg_auc   avg_time  task_id  \n0              0.435029  0.500000   0.264143        3  \n1              0.558417  0.884843   0.280833        3  \n2              0.909821  0.725330   0.240000        3  \n3              0.076143  0.500000   0.335000        3  \n4              0.874333  0.866785   0.246500        3  \n...                 ...       ...        ...      ...  \n9945           0.825170  0.999335  24.884750        3  \n9946           0.663293  0.500000   9.541000        3  \n9947           0.785170  0.978744  10.782600        3  \n9948           0.805439  0.757829  14.016500        3  \n9949           0.270201  0.990246   9.615667        3  \n\n[9950 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data_id</th>\n      <th>num_round</th>\n      <th>eta</th>\n      <th>gamma</th>\n      <th>lambda</th>\n      <th>alpha</th>\n      <th>subsample</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>colsample_bytree</th>\n      <th>colsample_bylevel</th>\n      <th>avg_auc</th>\n      <th>avg_time</th>\n      <th>task_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.000097</td>\n      <td>0.002120</td>\n      <td>1.074280</td>\n      <td>156.026000</td>\n      <td>0.202192</td>\n      <td>2</td>\n      <td>16.668800</td>\n      <td>0.442926</td>\n      <td>0.435029</td>\n      <td>0.500000</td>\n      <td>0.264143</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.000200</td>\n      <td>0.014477</td>\n      <td>0.001188</td>\n      <td>0.971001</td>\n      <td>0.186941</td>\n      <td>7</td>\n      <td>1.547140</td>\n      <td>0.644196</td>\n      <td>0.558417</td>\n      <td>0.884843</td>\n      <td>0.280833</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.001281</td>\n      <td>0.000142</td>\n      <td>0.000156</td>\n      <td>0.012087</td>\n      <td>0.367161</td>\n      <td>12</td>\n      <td>1.027290</td>\n      <td>0.223957</td>\n      <td>0.909821</td>\n      <td>0.725330</td>\n      <td>0.240000</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.001911</td>\n      <td>0.021976</td>\n      <td>0.009044</td>\n      <td>0.002017</td>\n      <td>0.876026</td>\n      <td>14</td>\n      <td>1.951150</td>\n      <td>0.355134</td>\n      <td>0.076143</td>\n      <td>0.500000</td>\n      <td>0.335000</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>3</td>\n      <td>1</td>\n      <td>0.002022</td>\n      <td>0.000058</td>\n      <td>0.258957</td>\n      <td>1.867090</td>\n      <td>0.354213</td>\n      <td>4</td>\n      <td>2.447190</td>\n      <td>0.239795</td>\n      <td>0.874333</td>\n      <td>0.866785</td>\n      <td>0.246500</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9945</th>\n      <td>3</td>\n      <td>5749</td>\n      <td>0.028359</td>\n      <td>0.000032</td>\n      <td>0.018361</td>\n      <td>0.007791</td>\n      <td>0.569283</td>\n      <td>13</td>\n      <td>2.053570</td>\n      <td>0.332356</td>\n      <td>0.825170</td>\n      <td>0.999335</td>\n      <td>24.884750</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9946</th>\n      <td>3</td>\n      <td>5845</td>\n      <td>0.131354</td>\n      <td>0.010094</td>\n      <td>0.003000</td>\n      <td>0.439140</td>\n      <td>0.403850</td>\n      <td>1</td>\n      <td>233.686000</td>\n      <td>0.214519</td>\n      <td>0.663293</td>\n      <td>0.500000</td>\n      <td>9.541000</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9947</th>\n      <td>3</td>\n      <td>5865</td>\n      <td>0.003243</td>\n      <td>0.000045</td>\n      <td>0.340472</td>\n      <td>18.326600</td>\n      <td>0.394949</td>\n      <td>6</td>\n      <td>6.671510</td>\n      <td>0.051653</td>\n      <td>0.785170</td>\n      <td>0.978744</td>\n      <td>10.782600</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9948</th>\n      <td>3</td>\n      <td>5887</td>\n      <td>0.010789</td>\n      <td>0.002366</td>\n      <td>21.107800</td>\n      <td>0.076290</td>\n      <td>0.241146</td>\n      <td>12</td>\n      <td>61.870400</td>\n      <td>0.537502</td>\n      <td>0.805439</td>\n      <td>0.757829</td>\n      <td>14.016500</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>9949</th>\n      <td>3</td>\n      <td>5917</td>\n      <td>0.081559</td>\n      <td>4.644440</td>\n      <td>0.001400</td>\n      <td>0.253718</td>\n      <td>0.998722</td>\n      <td>1</td>\n      <td>0.592597</td>\n      <td>0.802198</td>\n      <td>0.270201</td>\n      <td>0.990246</td>\n      <td>9.615667</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n<p>9950 rows Ã— 14 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Average performance for task_id = 3 - training data\n",
    "\"\"\"\n",
    "display(avg_perf[3])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8858118162868935,\n",
      "             enable_categorical=False, eta=0.8505635404044724, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.06234557918260022, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=143, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6534642143327124, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6887276700546747,\n",
      "             enable_categorical=False, eta=0.8828201915080831, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.06840612390585947, max_delta_step=0, max_depth=5,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=382, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9532994526427592, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8014679789648791,\n",
      "             enable_categorical=False, eta=0.5501225198483334, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.031061671241326964, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=282, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.3390255300155745, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8154327273437514,\n",
      "             enable_categorical=False, eta=0.0430120860622284, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04789568865414078, max_delta_step=0, max_depth=9,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=330, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5206354105746317, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.5932786978844793,\n",
      "             enable_categorical=False, eta=0.19417139400096156, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.08742894060313912, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=162, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8259962831347396, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8735577468039204,\n",
      "             enable_categorical=False, eta=0.8210877278777298, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.017404180095864936, max_delta_step=0, max_depth=7,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=342, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.41201344190215305, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8454515655099943,\n",
      "             enable_categorical=False, eta=0.9362086098390661, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.08770053604438102, max_delta_step=0, max_depth=5,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=196, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6926170608112185, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7957841784466676,\n",
      "             enable_categorical=False, eta=0.3946619366421986, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.0919388017471885, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=133, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.48659712342262496, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.853383771271522,\n",
      "             enable_categorical=False, eta=0.32259305076921924, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.008318745657933332, max_delta_step=0, max_depth=11,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=311, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.3961794529401583, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8141395188730012,\n",
      "             enable_categorical=False, eta=0.4653187980675485, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.07575269831425756, max_delta_step=0, max_depth=4,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=374, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9379032784613766, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.813912632886284,\n",
      "             enable_categorical=False, eta=0.1375754690624256, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.023461047080733682, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=442, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5129168130699895, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.608130120052597,\n",
      "             enable_categorical=False, eta=0.7911778333891804, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.019531320848787292, max_delta_step=0, max_depth=11,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=330, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8380521134511478, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8796450590687404,\n",
      "             enable_categorical=False, eta=0.9963576134305924, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.03645972754966614, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=209, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.7645987811130621, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7022212564928804,\n",
      "             enable_categorical=False, eta=0.948455172681644, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.0218422456896127, max_delta_step=0, max_depth=13,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=237, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8197253152346216, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6833460606044973,\n",
      "             enable_categorical=False, eta=0.854750612141861, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.015263758588542458, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=470, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9244987564168141, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8560808378112057,\n",
      "             enable_categorical=False, eta=0.8515211097483414, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.09853900084311135, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=269, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8114749440237148, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6075988607570628,\n",
      "             enable_categorical=False, eta=0.0027975065388995417, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.07662855950283232, max_delta_step=0, max_depth=13,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=325, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.47177122559234974, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.5296129205464234,\n",
      "             enable_categorical=False, eta=0.4956547444819728, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.09416080322704219, max_delta_step=0, max_depth=3,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=322, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6182738899149556, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8672427047702562,\n",
      "             enable_categorical=False, eta=0.22974788150508035, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.05693002485997525, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=371, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6627706901973114, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7627057312095892,\n",
      "             enable_categorical=False, eta=0.07858434946135286, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.006983767229849322, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=445, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.45244457286046946, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8353052494403015,\n",
      "             enable_categorical=False, eta=0.7811615934338979, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.02354112702213659, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=239, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.31281092710664044, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.889994803286067,\n",
      "             enable_categorical=False, eta=0.12058559885732781, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04571890910273066, max_delta_step=0, max_depth=13,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=261, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.7669131434076084, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.5672912884661421,\n",
      "             enable_categorical=False, eta=0.18864817157250352, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.09477460850706543, max_delta_step=0, max_depth=12,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=487, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5218052150147519, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7900199474923388,\n",
      "             enable_categorical=False, eta=0.4835626795198439, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.0821164407241869, max_delta_step=0, max_depth=11,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=221, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5810505924288045, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6555682547299533,\n",
      "             enable_categorical=False, eta=0.5310710872977682, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.03698350294230055, max_delta_step=0, max_depth=13,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=488, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6232066849955725, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.5085186260714039,\n",
      "             enable_categorical=False, eta=0.4453044795408011, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.019611222277425486, max_delta_step=0, max_depth=7,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=471, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.7387127135515603, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8442478892086487,\n",
      "             enable_categorical=False, eta=0.778676080163126, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.0249459075678802, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=396, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6023670234461839, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8451654385434875,\n",
      "             enable_categorical=False, eta=0.820050307506358, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.053110504438295694, max_delta_step=0, max_depth=9,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=261, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8516028889512572, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7316976734623108,\n",
      "             enable_categorical=False, eta=0.7966894591959413, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04910543630778614, max_delta_step=0, max_depth=9,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=488, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.809434310608907, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7115710953781311,\n",
      "             enable_categorical=False, eta=0.758081928108746, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.047393467423414125, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=156, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5026858185764734, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.843910601705391,\n",
      "             enable_categorical=False, eta=0.01957511738587148, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.06669373924169038, max_delta_step=0, max_depth=5,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=475, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5922155604757545, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8123870115711049,\n",
      "             enable_categorical=False, eta=0.4547137285413174, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.012475999188473126, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=486, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6090782144426798, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6719622390368035,\n",
      "             enable_categorical=False, eta=0.7985517588508135, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.01951887758646569, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=476, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9615828597409519, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8332420338449211,\n",
      "             enable_categorical=False, eta=0.19712483382908025, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.030161150300262955, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=211, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.365936396804437, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8598538811209198,\n",
      "             enable_categorical=False, eta=0.2179728003229881, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.022342316776115752, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=239, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8145412588072651, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8722605074322289,\n",
      "             enable_categorical=False, eta=0.6821124021500061, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04507511907799472, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=320, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9548989263552445, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8075062184451784,\n",
      "             enable_categorical=False, eta=0.5322710878280207, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.0346840126232074, max_delta_step=0, max_depth=12,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=292, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8907644048465697, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7712750238073323,\n",
      "             enable_categorical=False, eta=0.15777636523170116, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.03656706673587545, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=309, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.647730553040277, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6988122044123443,\n",
      "             enable_categorical=False, eta=0.6026182148552898, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.06361979034987797, max_delta_step=0, max_depth=5,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=463, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.853519232988625, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8918289176143719,\n",
      "             enable_categorical=False, eta=0.8414994913962643, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.06602064807483493, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=437, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8880985505126833, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8154342657084248,\n",
      "             enable_categorical=False, eta=0.5214746997795895, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.06467244862027967, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=472, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.4108317393856039, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8408060106999489,\n",
      "             enable_categorical=False, eta=0.34532231161793286, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.054298470560774054, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=452, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6484909897265978, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.797959291289682,\n",
      "             enable_categorical=False, eta=0.7126340615295044, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.01688522708576208, max_delta_step=0, max_depth=5,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=155, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.31448151760104703, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8634789064221597,\n",
      "             enable_categorical=False, eta=0.7608841777877132, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.048860705365957734, max_delta_step=0, max_depth=7,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=352, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.329344946616851, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7413667062912712,\n",
      "             enable_categorical=False, eta=0.22824599098720894, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04746872044982519, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=270, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.7676218806238497, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8036884638279027,\n",
      "             enable_categorical=False, eta=0.005540464950174241, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.027800671901571063, max_delta_step=0, max_depth=11,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=227, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8136128385315022, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.691364290961231,\n",
      "             enable_categorical=False, eta=0.11268219478256516, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.023241114352520974, max_delta_step=0, max_depth=11,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=472, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9225184747580812, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.873045334214833,\n",
      "             enable_categorical=False, eta=0.899588331018987, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.05839121103235037, max_delta_step=0, max_depth=11,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=372, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.677736194496593, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8750032951144286,\n",
      "             enable_categorical=False, eta=0.4511689981383653, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.033996522532593026, max_delta_step=0, max_depth=13,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=227, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.4050455007839566, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7710475959411145,\n",
      "             enable_categorical=False, eta=0.1556824543718124, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.01733833261907034, max_delta_step=0, max_depth=9,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=294, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.852090113777902, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6693092855248096,\n",
      "             enable_categorical=False, eta=0.8405359819942695, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.03207127410467577, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=414, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.488962234614269, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7755263787133964,\n",
      "             enable_categorical=False, eta=0.6213280105328756, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.016363252644375777, max_delta_step=0, max_depth=11,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=417, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9842919851471152, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8596187668735078,\n",
      "             enable_categorical=False, eta=0.4597170168988577, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.03154272437210707, max_delta_step=0, max_depth=4,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=192, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9210172993543277, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8088998432779857,\n",
      "             enable_categorical=False, eta=0.7563927127700818, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04895832208122192, max_delta_step=0, max_depth=11,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=445, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.3144462856733484, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8051578562238229,\n",
      "             enable_categorical=False, eta=0.002898258600648185, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.05113468861253456, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=181, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9098459970165473, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6014864054542646,\n",
      "             enable_categorical=False, eta=0.801057818304515, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.029895679881250643, max_delta_step=0, max_depth=4,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=184, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6737286452425255, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6165532114067132,\n",
      "             enable_categorical=False, eta=0.5783383592728494, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.019891794250474503, max_delta_step=0, max_depth=9,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=492, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6849904910734226, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8116505629021411,\n",
      "             enable_categorical=False, eta=0.8740389458619282, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04247532218761467, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=197, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6136481137602607, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8388301056841236,\n",
      "             enable_categorical=False, eta=0.9630997742493158, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.030757943914438056, max_delta_step=0, max_depth=9,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=435, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.44700535182355783, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8059492215207971,\n",
      "             enable_categorical=False, eta=0.8691520638560715, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.057445957676893075, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=426, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9150991966064135, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8622924423188967,\n",
      "             enable_categorical=False, eta=0.2748407568306706, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.03111715898631599, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=361, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.7636599202397096, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8878404099615782,\n",
      "             enable_categorical=False, eta=0.04000889416403653, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.018100591638020646, max_delta_step=0, max_depth=14,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=419, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.4082118454552876, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6562929987753481,\n",
      "             enable_categorical=False, eta=0.47193949877673047, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.038924163257161484, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=429, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.7465330601827884, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8339187091992417,\n",
      "             enable_categorical=False, eta=0.029162520541833666, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.022915330745680796, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=213, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5809408296384051, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8722215259083885,\n",
      "             enable_categorical=False, eta=0.2877801135463594, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.05726666315066346, max_delta_step=0, max_depth=3,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=249, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.4451578893924215, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8863316934136058,\n",
      "             enable_categorical=False, eta=0.13007434496430084, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04099358141025772, max_delta_step=0, max_depth=8,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=230, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6404389177853232, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.891978306230802,\n",
      "             enable_categorical=False, eta=0.2833259258317656, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.044886776700666105, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=326, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9820246316467602, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8494948199301529,\n",
      "             enable_categorical=False, eta=0.7765644593485365, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04538363148227697, max_delta_step=0, max_depth=9,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=109, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.4016811892034869, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8308592755843038,\n",
      "             enable_categorical=False, eta=0.024981542194339575, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.07401642461807785, max_delta_step=0, max_depth=9,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=452, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5386563846169587, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6805077041064649,\n",
      "             enable_categorical=False, eta=0.6717407365350996, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.06864385061529689, max_delta_step=0, max_depth=5,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=284, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.4893986617938906, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6122676360048698,\n",
      "             enable_categorical=False, eta=0.6632066493581217, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.024650762753632472, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=270, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5733281401947683, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7020963880327615,\n",
      "             enable_categorical=False, eta=0.7413540314606886, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.07438342363590765, max_delta_step=0, max_depth=12,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=100, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5369973358081802, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7791239524708462,\n",
      "             enable_categorical=False, eta=0.46868051073265626, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.037187850446548536, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=468, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.535466318721407, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7438702713591905,\n",
      "             enable_categorical=False, eta=0.3089006174426005, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04739857669514581, max_delta_step=0, max_depth=7,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=108, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.419357122499028, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6007284272473081,\n",
      "             enable_categorical=False, eta=0.5679849499095149, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.02941115539541297, max_delta_step=0, max_depth=13,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=445, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6204547845854734, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8601705514038991,\n",
      "             enable_categorical=False, eta=0.9828220507878662, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.051231053066465054, max_delta_step=0, max_depth=9,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=182, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.3910401380223111, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8290121131065047,\n",
      "             enable_categorical=False, eta=0.09796619855645197, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.05282184238886169, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=72, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.6509754958627108, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8245956662970448,\n",
      "             enable_categorical=False, eta=0.552788583319431, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.018542333637928905, max_delta_step=0, max_depth=13,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=373, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9738382533086669, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8947177623565639,\n",
      "             enable_categorical=False, eta=0.9073125415166053, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.015707929663597017, max_delta_step=0, max_depth=14,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=464, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5527878964828883, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7205094884955942,\n",
      "             enable_categorical=False, eta=0.7941705210168888, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.058897481493989594, max_delta_step=0, max_depth=5,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=62, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.4294731028102459, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8686426406994809,\n",
      "             enable_categorical=False, eta=0.41562529445466434, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.030695849040426357, max_delta_step=0, max_depth=5,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=469, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9412974387655504, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7512523981711166,\n",
      "             enable_categorical=False, eta=0.39022642739240465, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.09108348256868322, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=188, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8514171647965161, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7761906309658881,\n",
      "             enable_categorical=False, eta=0.42989699360738176, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.025220448789475595, max_delta_step=0, max_depth=13,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=194, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.621306251010473, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.87410744516961,\n",
      "             enable_categorical=False, eta=0.20590044448555916, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.016387681199665067, max_delta_step=0, max_depth=12,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=269, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.5970208832988935, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7391927228568891,\n",
      "             enable_categorical=False, eta=0.8603749281281089, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.09940059965847542, max_delta_step=0, max_depth=4,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=159, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.8278808287457171, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7009467649226864,\n",
      "             enable_categorical=False, eta=0.23258366859018814, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.069812832447618, max_delta_step=0, max_depth=14,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=336, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9040599360356261, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8944844028790222,\n",
      "             enable_categorical=False, eta=0.08563046536445819, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.047653269918005434, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=307, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.7143840154888403, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8280032714266912,\n",
      "             enable_categorical=False, eta=0.8150985975625274, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.04029736279499831, max_delta_step=0, max_depth=11,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=328, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.3506710156685936, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.6847681166256632,\n",
      "             enable_categorical=False, eta=0.41085590139557826, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.02917642821112624, max_delta_step=0, max_depth=10,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=321, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.9416994745131937, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.866244521654117,\n",
      "             enable_categorical=False, eta=0.44290314668876485, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.028012498297272127, max_delta_step=0, max_depth=13,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=299, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.47274617929190355, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.5641144040680093,\n",
      "             enable_categorical=False, eta=0.4934796831996203, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.037207522006542294, max_delta_step=0, max_depth=4,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=407, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.7410853650955143, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.4954021844378537,\n",
      "             enable_categorical=False, eta=0.3200698172384073, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.07541023329242501, max_delta_step=0, max_depth=3,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=475, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.7987156549898394, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.7545489971317509,\n",
      "             enable_categorical=False, eta=0.45757137177524676, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.030614808428983765, max_delta_step=0, max_depth=4,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=415, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.3730966812800356, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n",
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "             colsample_bynode=1, colsample_bytree=0.8807451005725946,\n",
      "             enable_categorical=False, eta=0.45033795866826676, gamma=0,\n",
      "             gpu_id=-1, importance_type=None, interaction_constraints='',\n",
      "             learning_rate=0.06851195597669454, max_delta_step=0, max_depth=6,\n",
      "             min_child_weight=1, missing=nan, monotone_constraints='()',\n",
      "             n_estimators=429, n_jobs=4, num_parallel_tree=1, predictor='auto',\n",
      "             random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "             subsample=0.4803728700682828, tree_method='exact',\n",
      "             validate_parameters=1, verbosity=None)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Sanity check on the trained models from taskRegression.py\n",
    "\"\"\"\n",
    "import pickle\n",
    "with open('taskwise_models.pkl', 'rb') as f:\n",
    "    models_new = pickle.load(f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8494831]\n"
     ]
    },
    {
     "data": {
      "text/plain": "       data_id  num_round       eta     gamma      lambda       alpha  \\\n9950         6          1  0.024234  0.000067    1.000500  768.603000   \n9951         6          1  0.099818  0.028129    0.001607    0.001102   \n9952         6          1  0.101048  0.000067    3.301350    0.119235   \n9953         6          1  0.139589  2.918780    0.326829  196.354000   \n9954         6          2  0.004823  0.002550    0.016822    5.293030   \n...        ...        ...       ...       ...         ...         ...   \n11828        6       3520  0.168119  0.005091    0.001530    4.427620   \n11829        6       4127  0.669168  0.002332   87.110400    0.000502   \n11830        6       4180  0.504788  0.000805  416.362000  690.390000   \n11831        6       4658  0.075037  0.000673    0.007150  601.580000   \n11833        6       4734  0.003478  0.003172   45.664900  157.418000   \n\n       subsample  max_depth  min_child_weight  colsample_bytree  \\\n9950    0.641650          5           4.64789          0.561103   \n9951    0.821261         14           8.48476          0.245569   \n9952    0.618137          3         106.53300          0.702793   \n9953    0.786081          4          40.70170          0.516169   \n9954    0.661372         15           5.38856          0.952915   \n...          ...        ...               ...               ...   \n11828   0.755557          7          12.01180          0.256184   \n11829   0.102350         15          51.71450          0.323692   \n11830   0.584915          4           2.18529          0.118481   \n11831   0.430539          5          10.52920          0.788015   \n11833   0.287994         12           6.58579          0.674964   \n\n       colsample_bylevel   avg_auc   avg_time  task_id  \n9950            0.167461  0.500000    0.57750        6  \n9951            0.451169  0.843565    4.34350        6  \n9952            0.850884  0.864983    0.74610        6  \n9953            0.952272  0.854778    0.56010        6  \n9954            0.840901  0.976389    1.54355        6  \n...                  ...       ...        ...      ...  \n11828           0.046136  0.998837  451.21480        6  \n11829           0.122990  0.946679  398.70570        6  \n11830           0.510327  0.500000  368.32790        6  \n11831           0.523482  0.500000  515.37045        6  \n11833           0.537067  0.782395  532.33990        6  \n\n[1876 rows x 14 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>data_id</th>\n      <th>num_round</th>\n      <th>eta</th>\n      <th>gamma</th>\n      <th>lambda</th>\n      <th>alpha</th>\n      <th>subsample</th>\n      <th>max_depth</th>\n      <th>min_child_weight</th>\n      <th>colsample_bytree</th>\n      <th>colsample_bylevel</th>\n      <th>avg_auc</th>\n      <th>avg_time</th>\n      <th>task_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>9950</th>\n      <td>6</td>\n      <td>1</td>\n      <td>0.024234</td>\n      <td>0.000067</td>\n      <td>1.000500</td>\n      <td>768.603000</td>\n      <td>0.641650</td>\n      <td>5</td>\n      <td>4.64789</td>\n      <td>0.561103</td>\n      <td>0.167461</td>\n      <td>0.500000</td>\n      <td>0.57750</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9951</th>\n      <td>6</td>\n      <td>1</td>\n      <td>0.099818</td>\n      <td>0.028129</td>\n      <td>0.001607</td>\n      <td>0.001102</td>\n      <td>0.821261</td>\n      <td>14</td>\n      <td>8.48476</td>\n      <td>0.245569</td>\n      <td>0.451169</td>\n      <td>0.843565</td>\n      <td>4.34350</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9952</th>\n      <td>6</td>\n      <td>1</td>\n      <td>0.101048</td>\n      <td>0.000067</td>\n      <td>3.301350</td>\n      <td>0.119235</td>\n      <td>0.618137</td>\n      <td>3</td>\n      <td>106.53300</td>\n      <td>0.702793</td>\n      <td>0.850884</td>\n      <td>0.864983</td>\n      <td>0.74610</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9953</th>\n      <td>6</td>\n      <td>1</td>\n      <td>0.139589</td>\n      <td>2.918780</td>\n      <td>0.326829</td>\n      <td>196.354000</td>\n      <td>0.786081</td>\n      <td>4</td>\n      <td>40.70170</td>\n      <td>0.516169</td>\n      <td>0.952272</td>\n      <td>0.854778</td>\n      <td>0.56010</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>9954</th>\n      <td>6</td>\n      <td>2</td>\n      <td>0.004823</td>\n      <td>0.002550</td>\n      <td>0.016822</td>\n      <td>5.293030</td>\n      <td>0.661372</td>\n      <td>15</td>\n      <td>5.38856</td>\n      <td>0.952915</td>\n      <td>0.840901</td>\n      <td>0.976389</td>\n      <td>1.54355</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11828</th>\n      <td>6</td>\n      <td>3520</td>\n      <td>0.168119</td>\n      <td>0.005091</td>\n      <td>0.001530</td>\n      <td>4.427620</td>\n      <td>0.755557</td>\n      <td>7</td>\n      <td>12.01180</td>\n      <td>0.256184</td>\n      <td>0.046136</td>\n      <td>0.998837</td>\n      <td>451.21480</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>11829</th>\n      <td>6</td>\n      <td>4127</td>\n      <td>0.669168</td>\n      <td>0.002332</td>\n      <td>87.110400</td>\n      <td>0.000502</td>\n      <td>0.102350</td>\n      <td>15</td>\n      <td>51.71450</td>\n      <td>0.323692</td>\n      <td>0.122990</td>\n      <td>0.946679</td>\n      <td>398.70570</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>11830</th>\n      <td>6</td>\n      <td>4180</td>\n      <td>0.504788</td>\n      <td>0.000805</td>\n      <td>416.362000</td>\n      <td>690.390000</td>\n      <td>0.584915</td>\n      <td>4</td>\n      <td>2.18529</td>\n      <td>0.118481</td>\n      <td>0.510327</td>\n      <td>0.500000</td>\n      <td>368.32790</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>11831</th>\n      <td>6</td>\n      <td>4658</td>\n      <td>0.075037</td>\n      <td>0.000673</td>\n      <td>0.007150</td>\n      <td>601.580000</td>\n      <td>0.430539</td>\n      <td>5</td>\n      <td>10.52920</td>\n      <td>0.788015</td>\n      <td>0.523482</td>\n      <td>0.500000</td>\n      <td>515.37045</td>\n      <td>6</td>\n    </tr>\n    <tr>\n      <th>11833</th>\n      <td>6</td>\n      <td>4734</td>\n      <td>0.003478</td>\n      <td>0.003172</td>\n      <td>45.664900</td>\n      <td>157.418000</td>\n      <td>0.287994</td>\n      <td>12</td>\n      <td>6.58579</td>\n      <td>0.674964</td>\n      <td>0.537067</td>\n      <td>0.782395</td>\n      <td>532.33990</td>\n      <td>6</td>\n    </tr>\n  </tbody>\n</table>\n<p>1876 rows Ã— 14 columns</p>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Regression model that was trained on dataset with task_id 6\n",
    "\"\"\"\n",
    "print(models_new[6].predict([[\t1,\t0.099818,\t0.028129,\t0.001607,\t0.001102,\t0.821261,\t14,\t8.48476,\t0.245569,\t0.451169]]))\n",
    "display(avg_perf[6])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 10 Axes>",
      "image/png": "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\n"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Looking at potential good values for a grid/random search - Examining values of HP for the best config of each task\n",
    "\"\"\"\n",
    "from python.project_utils import get_best_config_per_task, hyperparameters_data\n",
    "import matplotlib.pyplot as plt\n",
    "best_configs = get_best_config_per_task()\n",
    "best_configs[hyperparameters_data].plot(subplots=True, layout=(5,2))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "num_round            15.440000\neta                   0.001093\ngamma                 0.000018\nlambda                0.000029\nalpha                 0.000004\nsubsample             0.188223\nmax_depth             1.930000\nmin_child_weight      0.038333\ncolsample_bytree      0.042870\ncolsample_bylevel     0.040134\nName: 0.01, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "num_round            4934.070000\neta                     0.999958\ngamma                   4.097701\nlambda                383.455740\nalpha                  18.270835\nsubsample               0.998929\nmax_depth              18.000000\nmin_child_weight       53.032322\ncolsample_bytree        0.995774\ncolsample_bylevel       0.985291\nName: 0.99, dtype: float64"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(best_configs[hyperparameters_data].quantile(0.01), best_configs[hyperparameters_data].quantile(0.99))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from python.taskRegression import get_train_test_distances\n",
    "from python.taskRegression import get_taskwise_regressors\n",
    "\"\"\"\n",
    "    Testing distance function\n",
    "\"\"\"\n",
    "dist = get_train_test_distances()\n",
    "train_ids = sorted(dist[\"train_id\"].unique())\n",
    "train_ids\n",
    "dist\n",
    "\"\"\"\n",
    "    Testing taskwise regression function\n",
    "\"\"\"\n",
    "test_id = 16\n",
    "models = get_taskwise_regressors('taskwise_models.pkl')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Taskwise regression implementation - moved to taskRegression.py\n",
    "\"\"\"\n",
    "# from datetime import datetime\n",
    "# import pickle\n",
    "# from python.project_utils import training_meta_features, hyperparameters_data\n",
    "# from xgboost import XGBRegressor\n",
    "# from skopt import BayesSearchCV\n",
    "# from sklearn.model_selection import RepeatedKFold\n",
    "# from skopt.space import Real, Integer\n",
    "# \n",
    "# \n",
    "# #Defining search space for model\n",
    "# search_space = {}\n",
    "# search_space['eta'] = Real(0,1,'uniform')\n",
    "# search_space['n_estimators'] = Integer(50,500)\n",
    "# search_space['max_depth'] = Integer(3,15)\n",
    "# search_space['learning_rate'] = Real(10e-4,0.1,'log-uniform')\n",
    "# search_space['colsample_bytree'] = Real(0.3,0.9,'uniform')\n",
    "# search_space['subsample'] = Real(0.3,1,'uniform')\n",
    "# \n",
    "# #Defining CV method and Bayesian optimisation procedure\n",
    "# CV_folds = RepeatedKFold(n_splits=10, n_repeats=1, random_state=1)\n",
    "# search = BayesSearchCV(estimator= XGBRegressor(), search_spaces=search_space, n_jobs=-1, cv=CV_folds, n_iter=10)\n",
    "# \n",
    "# models = {}\n",
    "# for key in avg_perf.keys():\n",
    "#     print(\"Evaluating regressor for dataset with task_id: \",key)\n",
    "#     print(\"Start time: \", datetime.now().strftime(\"%H:%M:%S\"))\n",
    "#     #Defining search space for model\n",
    "#     current_df = avg_perf[key]\n",
    "# \n",
    "#     #Selecting relevant information for regressor\n",
    "#     features_train = current_df[hyperparameters_data]\n",
    "#     target_train = current_df['avg_auc']\n",
    "# \n",
    "#     #Performing the search\n",
    "#     search.fit(features_train.to_numpy(), target_train.to_numpy())\n",
    "#     # report the best result\n",
    "#     print(search.best_score_)\n",
    "#     print(search.best_params_)\n",
    "#     # train the model on the best parameters\n",
    "#     model = XGBRegressor(**search.best_params_)\n",
    "# \n",
    "#     # add the trained model to our collection\n",
    "#     model.fit(features_train.to_numpy(), target_train.to_numpy())\n",
    "#     models[key] = model\n",
    "# \n",
    "#     # save all models (done in every loop to not lose progress)\n",
    "#     with open('taskwise_models.pkl', 'wb') as f:\n",
    "#         pickle.dump(models, f)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}